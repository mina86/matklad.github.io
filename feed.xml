<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
<link href="https://matklad.github.io/feed.xml" rel="self" type="application/atom+xml"/>
<link href="https://matklad.github.io" rel="alternate" type="text/html"/>
<updated>2024-09-25T00:34:41.182Z</updated>
<id>https://matklad.github.io/feed.xml</id>
<title type="html">matklad</title>
<subtitle>Yet another programming blog by Alex Kladov aka matklad.</subtitle>
<author><name>Alex Kladov</name></author>

<entry>
<title type="text">The Watermelon Operator</title>
<link href="https://matklad.github.io/2024/09/24/watermelon-operator.html" rel="alternate" type="text/html" title="The Watermelon Operator" />
<published>2024-09-24T00:00:00+00:00</published>
<updated>2024-09-24T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/09/24/watermelon-operator</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[In these two most excellent articles,
https://without.boats/blog/let-futures-be-futures
and
https://without.boats/blog/futures-unordered, 
withoutboats introduces the concepts of multi-task and intra-task concurrency.
I want to revisit this distinction --- while I agree that there are different classes
of patterns of concurrency here, I am not quite satisfied with this specific partitioning of the
design space. I will use Rust-like syntax for most of the examples, but I am more interested in the
language-agnostic patterns, rather than in Rust's specific implementation of async.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/09/24/watermelon-operator.html"><![CDATA[
<h1><span>The Watermelon Operator</span> <time class="meta" datetime="2024-09-24">Sep 24, 2024</time></h1>
<p><span>In these two most excellent articles,</span>
<a href="https://without.boats/blog/let-futures-be-futures" class="display url">https://without.boats/blog/let-futures-be-futures</a>
<span>and</span>
<span class="display"><a href="https://without.boats/blog/futures-unordered" class="url">https://without.boats/blog/futures-unordered</a><span>, </span></span>
<span>withoutboats introduces the concepts of </span>&ldquo;<span>multi-task</span>&rdquo;<span> and </span>&ldquo;<span>intra-task</span>&rdquo;<span> concurrency.</span>
<span>I want to revisit this distinction </span>&mdash;<span> while I agree that there are different classes</span>
<span>of patterns of concurrency here, I am not quite satisfied with this specific partitioning of the</span>
<span>design space. I will use Rust-like syntax for most of the examples, but I am more interested in the</span>
<span>language-agnostic patterns, rather than in Rust</span>&rsquo;<span>s specific implementation of async.</span></p>
<section id="The-Two-Examples">

    <h2>
    <a href="#The-Two-Examples"><span>The Two Examples</span> </a>
    </h2>
<p><span>Let</span>&rsquo;<span>s introduce the two kinds of concurrency using a somewhat abstract example. We want to handle a</span>
<code>Request</code><span> by doing some computation and then persisting the results in the database and in the cache.</span>
<span>Notably, writes to the cache and to the database can proceed concurrently. So, something like this:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">process</span>(</span>
<span class="line">  db: DataBase,</span>
<span class="line">  cache: Cache,</span>
<span class="line">  request: Request,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> Response {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">response</span> = <span class="hl-title function_ invoke__">compute_response</span>(db, cache, request).<span class="hl-keyword">await</span>;</span>
<span class="line">  <span class="hl-title function_ invoke__">spawn</span>(<span class="hl-title function_ invoke__">update_db</span>(db, response));</span>
<span class="line">  <span class="hl-title function_ invoke__">spawn</span>(<span class="hl-title function_ invoke__">update_cache</span>(cache, response));</span>
<span class="line">  response</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">update_db</span>(db: DataBase, response: Response);</span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">update_cache</span>(cache: Cache, response: Response);</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">spawn</span>&lt;T&gt;(f: <span class="hl-keyword">impl</span> <span class="hl-title class_">Future</span>&lt;Output = T&gt;) <span class="hl-punctuation">-&gt;</span> JoinHandle&lt;T&gt;;</span></code></pre>

</figure>
<p><span>This is multi-task concurrency style </span>&mdash;<span> we fire off two tasks four updating the database and the</span>
<span>cache. Here</span>&rsquo;<span>s the same snippet in intra-task style, where we use join function on futures:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">process</span>(</span>
<span class="line">  db: DataBase,</span>
<span class="line">  cache: Cache,</span>
<span class="line">  request: Request,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> Response {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">response</span> = <span class="hl-title function_ invoke__">compute_response</span>(db, cache, request).<span class="hl-keyword">await</span>;</span>
<span class="line">  <span class="hl-title function_ invoke__">join</span>(</span>
<span class="line">    <span class="hl-title function_ invoke__">update_db</span>(db, response),</span>
<span class="line">    <span class="hl-title function_ invoke__">update_cache</span>(cache, response),</span>
<span class="line">  ).<span class="hl-keyword">await</span>;</span>
<span class="line">  response</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">update_db</span>(db: DataBase, response: Response) { ... }</span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">update_cache</span>(cache: Cache, response: Response) { ... }</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">join</span>&lt;U, V&gt;(</span>
<span class="line">  f: <span class="hl-keyword">impl</span> <span class="hl-title class_">Future</span>&lt;Output = U&gt;,</span>
<span class="line">  g: <span class="hl-keyword">impl</span> <span class="hl-title class_">Future</span>&lt;Output = V&gt;,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> (U, V);</span></code></pre>

</figure>
<p><span>In other words:</span></p>
<p><span>Multi-task concurrency uses </span><code>spawn</code><span> </span>&mdash;<span> an operation that takes a future and starts a tasks that</span>
<span>executes independently of the parent task.</span></p>
<p><span>Intra-task concurrency uses </span><code>join</code><span> </span>&mdash;<span> an operation that takes a pair of futures and executes them</span>
<span>concurrently as a part of the current task.</span></p>
<p><span>But what is the </span><em><span>actual</span></em><span> difference between the two?</span></p>
</section>
<section id="Parallelism-is-not">

    <h2>
    <a href="#Parallelism-is-not"><span>Parallelism is not</span> </a>
    </h2>
<p><span>One candidate is parallelism </span>&mdash;<span> with </span><code>spawn</code><span>, the tasks can run not only concurrently, but actually</span>
<span>in parallel, on different CPU cores. </span><code>join</code><span> restricts them to the same thread that runs the main</span>
<span>task. But I think this is not quite right, abstractly, and is more of a product of specific Rust</span>
<span>APIs. There </span><em><span>are</span></em><span> executors which spawn on the current thread only. And, while in Rust it</span>&rsquo;<span>s not</span>
<span>really possible to make </span><code>join</code><span> poll the futures in parallel, I </span><em><span>think</span></em><span> this is just an artifact of</span>
<span>Rust existing API design (futures can</span>&rsquo;<span>t opt-out of synchronous cancellation). In other words, I</span>
<span>think it is possible in theory to implement an async runtime which provides all of the following</span>
<span>functions at the same time:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">spawn</span>&lt;F&gt;(fut: F) <span class="hl-punctuation">-&gt;</span> JoinHandle&lt;Output = F::Output&gt;</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F: Future;</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">pspawn</span>&lt;F&gt;(fut: F) <span class="hl-punctuation">-&gt;</span> PJoinHandle&lt;Output = F&gt;</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F: Future + <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;static</span>,</span>
<span class="line">  F::Output: <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;static</span>;</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">join</span>&lt;F1, F2&gt;(</span>
<span class="line">  fut1: F1,</span>
<span class="line">  fut2: F2,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> (F1::Output, F2::Output)</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F1: Future,</span>
<span class="line">  F2: Future;</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">pjoin</span>&lt;F1, F2&gt;(</span>
<span class="line">  fut1: F1,</span>
<span class="line">  fut2: F2,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> (F1::Output, F2::Output)</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F1: Future + <span class="hl-built_in">Send</span>, <span class="hl-comment">// NB: only Send, no &#x27;static</span></span>
<span class="line">  F1::Output:  <span class="hl-built_in">Send</span>,</span>
<span class="line">  F2: Future + <span class="hl-built_in">Send</span>,</span>
<span class="line">  F2::Output:  <span class="hl-built_in">Send</span>;</span></code></pre>

</figure>
<p><span>To confuse matters further, let</span>&rsquo;<span>s rewrite our example in TypeScript:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">function</span> <span class="hl-title function_">process</span>(<span class="hl-params"></span></span>
<span class="line"><span class="hl-params">  db: DataBase,</span></span>
<span class="line"><span class="hl-params">  cache: Cache,</span></span>
<span class="line"><span class="hl-params">  request: Request,</span></span>
<span class="line"><span class="hl-params"></span>): <span class="hl-title class_">Response</span> {</span>
<span class="line">  <span class="hl-keyword">const</span> response = <span class="hl-keyword">await</span> <span class="hl-title function_">compute_response</span>(db, cache, request);</span>
<span class="line">  <span class="hl-keyword">const</span> db_update = <span class="hl-title function_">update_db</span>(db, response);</span>
<span class="line">  <span class="hl-keyword">const</span> cache_update = <span class="hl-title function_">update_cache</span>(cache, response);</span>
<span class="line">  <span class="hl-keyword">await</span> <span class="hl-title class_">Promise</span>.<span class="hl-title function_">all</span>([db_update, cache_update]);</span>
<span class="line">  <span class="hl-keyword">return</span> response</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>and using Rust</span>&rsquo;<span>s rayon library:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">process</span>(</span>
<span class="line">  db: DataBase,</span>
<span class="line">  cache: Cache,</span>
<span class="line">  request: Request,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> Response {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">response</span> = <span class="hl-title function_ invoke__">compute_response</span>(db, cache, request).<span class="hl-keyword">await</span>;</span>
<span class="line">  rayon::<span class="hl-title function_ invoke__">join</span>(</span>
<span class="line">    || <span class="hl-title function_ invoke__">update_db</span>(db, response),</span>
<span class="line">    || <span class="hl-title function_ invoke__">update_cache</span>(cache, response),</span>
<span class="line">  );</span>
<span class="line">  response</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Are these examples multi-task or intra-task? To me, the TypeScript one feels multi-task </span>&mdash;<span> although</span>
<span>it is syntactically close to </span><code>join().async</code><span>, the two update promises are running independently from</span>
<span>the parent task. If we forget the call to </span><code>Promise.all</code><span>, the cache and the database would still get</span>
<span>updated (but likely </span><em><span>after</span></em><span> we would have returned the response to the user)! In contrast, rayon</span>
<span>feels intra-task </span>&mdash;<span> although the closures could get stolen and be run by a different thread, they</span>
<span>won</span>&rsquo;<span>t </span>&ldquo;<span>escape</span>&rdquo;<span> dynamic extent of the encompassing </span><code>process</code><span> call.</span></p>
</section>
<section id="To-await-or-await-to">

    <h2>
    <a href="#To-await-or-await-to"><span>To await or await to?</span> </a>
    </h2>
<p><span>Let</span>&rsquo;<span>s zoom in onto the JS and the join examples:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">function</span> <span class="hl-title function_">process</span>(<span class="hl-params"></span></span>
<span class="line"><span class="hl-params">  db: DataBase,</span></span>
<span class="line"><span class="hl-params">  cache: Cache,</span></span>
<span class="line"><span class="hl-params">  request: Request</span></span>
<span class="line"><span class="hl-params"></span>): <span class="hl-title class_">Response</span> {</span>
<span class="line">  <span class="hl-keyword">const</span> response = <span class="hl-keyword">await</span> <span class="hl-title function_">compute_response</span>(db, cache, request);</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">await</span> <span class="hl-title class_">Promise</span>.<span class="hl-title function_">all</span>([</span>
<span class="line">    <span class="hl-title function_">update_db</span>(db, response),</span>
<span class="line">    <span class="hl-title function_">update_cache</span>(cache, response),</span>
<span class="line">  ]);</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">return</span> response;</span>
<span class="line">}</span></code></pre>

</figure>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">process</span>(</span>
<span class="line">  db: DataBase,</span>
<span class="line">  cache: Cache,</span>
<span class="line">  request: Request,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> Response {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">response</span> = <span class="hl-title function_ invoke__">compute_response</span>(db, cache, request).<span class="hl-keyword">await</span>;</span>
<span class="line"></span>
<span class="line">  <span class="hl-title function_ invoke__">join</span>(</span>
<span class="line">    <span class="hl-title function_ invoke__">update_db</span>(db, response),</span>
<span class="line">    <span class="hl-title function_ invoke__">update_cache</span>(cache, response),</span>
<span class="line">  ).<span class="hl-keyword">await</span>;</span>
<span class="line"></span>
<span class="line">  response</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>I</span>&rsquo;<span>ve re-written the JavaScript version to be syntactically isomorphic to the Rust one. The</span>
<span>difference is on the semantic level: JavaScript promises are eager, they start executing as soon as</span>
<span>a promise is created. In contrast, Rust futures are lazy </span>&mdash;<span> they do nothing until polled. And</span>
<em><span>this</span></em><span> I think is the fundamental difference, it is lazy vs. eager </span>&ldquo;<span>futures</span>&rdquo;<span> (</span><code>thread::spawn</code><span> is an</span>
<span>eager </span>&ldquo;<span>future</span>&rdquo;<span> while </span><code>rayon::join</code><span> a lazy one).</span></p>
<p><span>And it seems that lazy semantics is quiet a bit more elegant! The beauty of</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-title function_ invoke__">join</span>(</span>
<span class="line">  <span class="hl-title function_ invoke__">update_db</span>(db, response),</span>
<span class="line">  <span class="hl-title function_ invoke__">update_cache</span>(cache, response),</span>
<span class="line">).<span class="hl-keyword">await</span>;</span></code></pre>

</figure>
<p><span>is that it</span>&rsquo;<span>s Molière</span>&rsquo;<span>s prose </span>&mdash;<span> this is structured concurrency, but without bundles, nurseries,</span>
<span>scopes, and other weird APIs.</span></p>
<p><span>It makes runtime semantics nicer even in dynamically typed languages. In JavaScript, forgetting an</span>
<span>await is a common, and very hard to spot problem </span>&mdash;<span> without await, code still works, but is</span>
<em><span>sometimes</span></em><span> wrong (if the async operation doesn</span>&rsquo;<span>t finish quite as fast as usual). Imagine JS with</span>
<span>lazy promises </span>&mdash;<span> there, forgetting an </span><code>await</code><span> would </span><em><span>always</span></em><span> consistently break. So, the need to</span>
<span>statically lint missing awaits will be less pressing.</span></p>
<p><span>Compare this with Erlang</span>&rsquo;<span>s take on nulls: while in typical dynamically typed languages partial</span>
<span>functions can return a value </span><code>T</code><span> or a </span><code>None</code><span>, in Erlang the convention is to return either </span><code>{ok, T}</code>
<span>or </span><code>none</code><span>. That is, even if the value is non-null, the call-site is </span><em><span>forced</span></em><span> to unpack it, you can</span>&rsquo;<span>t</span>
<span>write code that happens to work as long as </span><code>T</code><span> is non-null.</span></p>
<p><span>And of course, in Rust, the killer feature of lazy futures is that you can just borrow data from the</span>
<span>enclosing scope.</span></p>
<p><span>But it seems like there is one difference between multi-task and intra-task concurrency.</span></p>
</section>
<section id="One-Two-N-and-More">

    <h2>
    <a href="#One-Two-N-and-More"><span>One, Two, N, and More</span> </a>
    </h2>
<p><span>In the words of withoutboats:</span></p>

<figure class="blockquote">
<blockquote><p><span>The first limitation is that it is only possible to achieve a static arity of concurrency with</span>
<span>intra-task concurrency. That is, you cannot join (or select, etc) an arbitrary number of futures</span>
<span>with intra-task concurrency: the number must be fixed at compile time.</span></p>
</blockquote>

</figure>
<p><span>That is, you can do</span>
<span class="display"><code>join(a, b).await</code><span>,</span></span>
<span>and</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-title function_ invoke__">join</span>(</span>
<span class="line">  <span class="hl-title function_ invoke__">join</span>(a, b)</span>
<span class="line">  c,</span>
<span class="line">).<span class="hl-keyword">await</span></span></code></pre>

</figure>
<p><span>and, with some macros, even</span></p>

<figure class="code-block">


<pre><code><span class="line">join!(a, b, c, d, e, f).<span class="hl-keyword">await</span>;</span></code></pre>

</figure>
<p><span>but you can</span>&rsquo;<span>t do </span><span class="display"><code>join(xs...).await</code><span>.</span></span></p>
<p><span>I think this is incorrect, in a trivial and in an interesting way.</span></p>
<p><span>The trivial incorrectness is that there</span>&rsquo;<span>s </span><code>join_all</code><span>, that takes a slice of futures and is a direct</span>
<span>generalization of </span><code>join</code><span> to a runtime-variable number of futures.</span></p>
<p><span>But </span><code>join_all</code><span> still can</span>&rsquo;<span>t express the case where you don</span>&rsquo;<span>t know the number of futures up-front,</span>
<span>where you spawn some work, and only later realize that you need to spawn some more.</span></p>
<p><span>This is sort-of possible to express with </span><code>FuturesUnordered</code><span>, but that</span>&rsquo;<span>s a yuck API. I mean, even</span>
<span>its name screams </span>&ldquo;<span>DO NOT USE ME!</span>&rdquo;<span>.</span></p>
<p><span>But I do think that this is just an unfortunate API, and that the pattern actually can be expressed</span>
<span>in intra-task concurrency style nicely.</span></p>
<p><span>Let</span>&rsquo;<span>s take a closer look at the base case, </span><code>join</code><span>!</span></p>
</section>
<section id="Asynchronous-Semicolon">

    <h2>
    <a href="#Asynchronous-Semicolon"><span>Asynchronous Semicolon</span> </a>
    </h2>
<p><span>Section title is a bit of a giveaway. The </span><code>join</code><span> operator </span><em><span>is</span></em><span> </span><code>async ;</code><span>. The semicolon is an</span>
<span>operator of sequential composition:</span>
<code class="display">A; B</code></p>
<p><span>runs </span><code>A</code><span> first and then </span><code>B</code><span>.</span></p>
<p><span>In contrast, </span><code>join</code><span> is concurrent composition:</span>
<code class="display">join(A, B)</code></p>
<p><span>runs </span><code>A</code><span> and </span><code>B</code><span> concurrently.</span></p>
<p><span>And both </span><code>join</code><span> and </span><code>;</code><span> share the same problem </span>&mdash;<span> they can compose only a finite number of things.</span></p>
<p><span>But that</span>&rsquo;<span>s why we have other operators for sequential composition! If we know how many things we</span>
<span>need to run, we can use a counted </span><code>for</code><span> loop. And </span><code>join_all</code><span> is an analogue of a counted for loop!</span></p>
<p><span>In case where we </span><em><span>don</span>&rsquo;<span>t</span></em><span> know up-front when to stop, we use a </span><code>while</code><span>. And this is exactly what we</span>
<span>miss </span>&mdash;<span> there</span>&rsquo;<span>s no concurrently-flavored </span><code>while</code><span> operator.</span></p>
<p><span>Importantly, what we are looking for is </span><em><span>not</span></em><span> an async for:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">for</span> <span class="hl-variable">x</span> <span class="hl-keyword">in</span> iter {</span>
<span class="line">  <span class="hl-title function_ invoke__">process</span>(x).<span class="hl-keyword">await</span>;</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Here, although there could be some concurrency inside a single loop iteration, the iterations</span>
<span>themselves are run sequentially. The second iteration starts only when the first one finished.</span>
<span>Pictorially, this looks like a spiral, or a loop if we look from the side:</span></p>

<figure>

<img alt="" src="https://github.com/user-attachments/assets/92a9c1dd-1f94-4fac-9e11-5f0c54e6e10e">
</figure>
<p><span>What we rather want is to run </span><em><span>many</span></em><span> copies of the body concurrently, something like this:</span></p>

<figure>

<img alt="" src="https://github.com/user-attachments/assets/7149b6e8-2d99-4837-911b-36fbee80134a">
</figure>
<p><span>A spindle-like shape with many concurrent strands, which looks like wheel</span>&rsquo;<span>s spokes from the side.</span>
<span>Or, if you are </span><em><span>really</span></em><span> short on fitting metaphors:</span></p>
</section>
<section id="The-Watermelon-Operator-1">

    <h2>
    <a href="#The-Watermelon-Operator-1"><span>The Watermelon Operator</span> </a>
    </h2>
<p><span>Now, I understand that I</span>&rsquo;<span>ve already poked fun at unfortunate </span><code>FuturesUnordered</code><span> name, but I can</span>&rsquo;<span>t</span>
<span>really find a fitting name for the construct we want here. So I am going to boringly use</span>
<code>concurrently</code><span> keyword, which is way too long, but I</span>&rsquo;<span>ll refer to it as </span>&ldquo;<span>the watermelon operator</span>&rdquo;
<span>The stripes on the watermelon resemble the independent strands of execution this operator creates:</span></p>

<figure>

<img alt="wikipedia watermelons" src="https://github.com/user-attachments/assets/f0760a4a-03ba-45a5-bc26-e9156e28c5c9">
</figure>
<p><span>So, if you are writing a TCP server, your accept loop could look like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">concurrently <span class="hl-keyword">let</span> <span class="hl-variable">Some</span>(socket) = listener.<span class="hl-title function_ invoke__">accept</span>().<span class="hl-keyword">await</span> <span class="hl-keyword">in</span> {</span>
<span class="line">  <span class="hl-title function_ invoke__">handle_connection</span>(socket).<span class="hl-keyword">await</span>;</span>
<span class="line">}.<span class="hl-keyword">await</span></span></code></pre>

</figure>
<p><span>This runs accept in a loop, and, for each accepted socket, runs </span><code>handle_connection</code><span> concurrently.</span>
<span>There are as many concurrent </span><code>handle_connection</code><span>s calls as there are ready sockets in our listener!</span></p>
<p><span>Let</span>&rsquo;<span>s limit the maximum number of concurrent connections, to provide back pressure:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">semaphore</span> = Semaphore::<span class="hl-title function_ invoke__">new</span>(<span class="hl-number">16</span>);</span>
<span class="line"></span>
<span class="line">concurrently</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">Some</span>((socket, permit)) = <span class="hl-keyword">try</span> {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">permit</span> = semaphore.<span class="hl-title function_ invoke__">acquire</span>().<span class="hl-keyword">await</span>;</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">socket</span> = listener.<span class="hl-title function_ invoke__">accept</span>().<span class="hl-keyword">await</span>?;</span>
<span class="line">    (socket, permit)</span>
<span class="line">  }</span>
<span class="line"><span class="hl-keyword">in</span> {</span>
<span class="line">  <span class="hl-title function_ invoke__">handle_connection</span>(socket).<span class="hl-keyword">await</span>;</span>
<span class="line">  <span class="hl-title function_ invoke__">drop</span>(permit);</span>
<span class="line">}.<span class="hl-keyword">await</span></span></code></pre>

</figure>
<p><span>You get the idea (hopefully):</span></p>
<ul>
<li>
<span>In the </span>&ldquo;<span>head</span>&rdquo;<span> of our concurrent loop (cooloop?) construct, we first aquire a semaphore permit</span>
<span>and then fetch a socket.</span>
</li>
<li>
<span>Both the socket and the permit are passed to the body.</span>
</li>
<li>
<span>The body releases the permit at the end.</span>
</li>
<li>
<span>While the </span>&ldquo;<span>head</span>&rdquo;<span> construct runs in a loop concurrently to bodies, it is throttled by the minimum</span>
<span>of the available permits and ready connections.</span>
</li>
</ul>
<p><span>To make this more concrete, let</span>&rsquo;<span>s spell this out as a library</span>
<span>function:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">join</span>&lt;F1, F2&gt;(</span>
<span class="line">  fut1: F1,</span>
<span class="line">  fut2: F2,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> (F1::Output, F2::Output)</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F1: Future,</span>
<span class="line">  F2: Future;</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">join_all</span>&lt;F&gt;(futs: <span class="hl-type">Vec</span>&lt;F&gt;) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">Vec</span>&lt;F::Output&gt;</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F: Future;</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">concurrently</span>&lt;C, FC, B, FB, T&gt;(condition: C, body: B)</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  C: <span class="hl-title function_ invoke__">FnMut</span>() <span class="hl-punctuation">-&gt;</span> FC,</span>
<span class="line">  FC: Future&lt;Output = <span class="hl-type">Option</span>&lt;T&gt;&gt;,</span>
<span class="line">  B: <span class="hl-title function_ invoke__">FnMut</span>(T) <span class="hl-punctuation">-&gt;</span> FB,</span>
<span class="line">  FB: Future&lt;Output = ()&gt;;</span></code></pre>

</figure>
<p><span>I claim that this is the full set of </span>&ldquo;<span>primitive</span>&rdquo;<span> operations needed to express</span>
<span>more-or-less everything in intra-task concurrency style.</span></p>
<p><span>In particular, we can implement multi-task concurrency this way! To do so, we</span>&rsquo;<span>ll write a universal</span>
<span>watermelon operator, where the </span><code>T</code><span> which is passed to the body is an</span>
<span class="display"><code>Box&lt;dyn Future&lt;Output=()&gt;&gt;</code><span>,</span></span>
<span>and where the body just runs this future:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">multi_task_concurrency_main</span>(</span>
<span class="line">  spawn: <span class="hl-keyword">impl</span> <span class="hl-title class_">Fn</span>(<span class="hl-keyword">impl</span> <span class="hl-title class_">Future</span>&lt;Output = ()&gt; + <span class="hl-symbol">&#x27;static</span>),</span>
<span class="line">) {</span>
<span class="line">    ...</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">type</span> <span class="hl-title class_">AnyFuture</span> = <span class="hl-type">Box</span>&lt;<span class="hl-keyword">dyn</span> Future&lt;Output = ()&gt; + <span class="hl-symbol">&#x27;static</span>&gt;;</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">universal_watermelon</span>() {</span>
<span class="line">  <span class="hl-keyword">let</span> (sender, receiver) = channel::&lt;AnyFuture&gt;();</span>
<span class="line">  <span class="hl-title function_ invoke__">join</span>(</span>
<span class="line">    <span class="hl-title function_ invoke__">multi_task_concurrency_main</span>(<span class="hl-keyword">move</span> |fut| {</span>
<span class="line">      sender.<span class="hl-title function_ invoke__">send</span>(<span class="hl-type">Box</span>::<span class="hl-title function_ invoke__">new</span>(fut))</span>
<span class="line">    }),</span>
<span class="line">    <span class="hl-title function_ invoke__">concurrently</span>(</span>
<span class="line">      || <span class="hl-keyword">async</span> {</span>
<span class="line">        receiver.<span class="hl-title function_ invoke__">recv</span>().<span class="hl-keyword">await</span>;</span>
<span class="line">      },</span>
<span class="line">      |fut| <span class="hl-keyword">async</span> {</span>
<span class="line">        fut.<span class="hl-keyword">await</span>;</span>
<span class="line">      },</span>
<span class="line">    ),</span>
<span class="line">  )</span>
<span class="line">  .<span class="hl-keyword">await</span>;</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Note that the conversion in the opposite direction is not possible! With intra-task concurrency, we</span>
<span>can borrow from the parent stack frame. So it is not a problem to </span><em><span>restrict</span></em><span> that to only allow</span>
<code>'static</code><span> futures into the channel. In a sense, in the above example we return the future up the</span>
<span>stack, which explains why it can</span>&rsquo;<span>t borrow locals from our stack frame.</span></p>
<p><span>With multi-task concurrency though, we </span><em><span>start</span></em><span> we static futures. To let them borrow any stack data</span>
<span>requires unsafe.</span></p>
<p><span>Note also that the above set of operators, </span><code>join</code><span>, </span><code>join_all</code><span>, </span><code>concurrently</code><span> is </span><em><span>orthogonal</span></em><span> to</span>
<span>parallelism. Alongside those operators, there could exist </span><code>pjoin</code><span>, </span><code>pjoin_all</code><span> and </span><code>pconcurrently</code>
<span>with the </span><code>Send</code><span> bounds, such that you could mix and match parallel and single-core concurrency.</span></p>
</section>
<section id="If-a-Stack-is-a-Tree-Does-it-Make-Any-Difference">

    <h2>
    <a href="#If-a-Stack-is-a-Tree-Does-it-Make-Any-Difference"><span>If a Stack is a Tree, Does it Make Any Difference?</span> </a>
    </h2>
<p><span>One possible objection to the above framing of watermelon as a language-level operator is that it</span>
<span>seemingly doesn</span>&rsquo;<span>t pass zero-cost abstraction test. It </span><em><span>can</span></em><span> start an unbounded number of futures,</span>
<span>and those futures have to be stored </span><em><span>somewhere</span></em><span>. So we have a language operator which requires</span>
<span>dynamic memory allocation, which is a big no-no for any systems programming language.</span></p>
<p><span>I think there </span><em><span>is</span></em><span> some truth to it, and not an insignificant amount of it, but I think I can maybe</span>
<span>weasel out of it.</span></p>
<p><span>Consider recursion. Recursion </span><em><span>also</span></em><span> can allocate arbitrary amount of memory (on the stack), but</span>
<span>that is considered fine (I would </span><em><span>also</span></em><span> agree that it is not in fact fine that unbounded recursion</span>
<span>is considered fine, but, for the scope of this discussion, I will be a hypocrite and will ignore</span>
<span>that opinion of mine).</span></p>
<p><span>And here, we have essentially the same situation </span>&mdash;<span> we want to allocate arbitrary many (async)</span>
<span>stack frames, arranged in a tree. Doing it </span>&ldquo;<span>on the heap</span>&rdquo;<span> is easy, but we don</span>&rsquo;<span>t like the heap here.</span>
<span>Luckily, I believe there</span>&rsquo;<span>s a compilation scheme (hat tip to </span><a href="https://www.abubalay.com"><span>@rpjohnst</span></a>
<span>for patiently explaining it to me five times in different words) that implements this more-or-less</span>
<span>as efficiently as the normal call stack.</span></p>
<p><span>The idea is that we will have </span><em><span>two</span></em><span> stacks </span>&mdash;<span> a sync one and an async one. Specifically:</span></p>
<ul>
<li>
<span>Every sync function we compile normally, with a single stack.</span>
</li>
<li>
<span>Async functions get </span><em><span>two</span></em><span> stack pointers. So, we burn </span><code>sp</code><span> and one other register</span>
<span>(let</span>&rsquo;<span>s call it </span><code>asp</code><span>).</span>
</li>
<li>
<span>If an async function calls a sync function, the callee</span>&rsquo;<span>s frame is pushed onto </span><code>sp</code><span>.</span>
<span>Crucially, because sync functions can only call other </span><code>sync</code><span> functions, the callee doesn</span>&rsquo;<span>t need</span>
<span>to know the value of </span><code>asp</code><span>.</span>
</li>
<li>
<span>If an async function calls another async function, the frame (specifically, the </span>&ldquo;<span>variables live</span>
<span>across await point</span>&rdquo;<span> part of it) is pushed onto </span><code>asp</code><span>.</span>
</li>
<li>
<span>This async stack is segmented. So, for async function calls, we also do a check for </span>&ldquo;<span>do we have</span>
<span>enough stack?</span>&rdquo;<span> and, if not, allocate a new segment, linking them via a frame pointer.</span>
</li>
<li>
&ldquo;<span>Allocating a new segment</span>&rdquo;<span> doesn</span>&rsquo;<span>t mean that we actually go and call malloc. Rather, there</span>&rsquo;<span>s a</span>
<span>fixed-sized contiguous slab of say, 8 megs, out of which all async frames are allocated.</span>
</li>
<li>
<span>If we are out of async-stack, we crash in pretty much the same way as for the boring sync stack</span>
<span>overflow.</span>
</li>
</ul>
<p><span>While this </span><em><span>looks</span></em><span> just like Go-style segmented stacks, I think this scheme is quite a bit more</span>
<span>efficient (warning: I in general have a tendency to confidently talk about things I know little</span>
<span>about, and this one is the extreme case of that. If some Go compiler engineer disagrees with me, I</span>
<span>am probably in the wrong!).</span></p>
<p><span>The main difference is that the distinction between sync and async functions is maintained in the</span>
<span>type system. There are </span><em><span>no</span></em><span> changes for sync functions at all, so the principle of don</span>&rsquo;<span>t pay for</span>
<span>what you don</span>&rsquo;<span>t use is observed. This is in contrast to Go </span>&mdash;<span> I believe that Go, in general, can</span>&rsquo;<span>t</span>
<span>know whether a particular function can yield (that  is, if any function it (indirectly) calls can</span>
<span>yield), so it has to conservatively insert stack checks everywhere.</span></p>
<p><span>Then, even the async stack frames don</span>&rsquo;<span>t have to store </span><em><span>everything</span></em><span>, but just the stuff live across</span>
<span>await. Everything that happens between two awaits can go to the normal stack.</span></p>
<p><span>On top of that, async functions can still do aggressive inlining. So, the async call (and the stack</span>
<span>growth check) has to happen only for dynamically dispatched async calls!</span></p>
<p><span>Furthermore, the future trait could have some kind of </span><code>size_hint</code><span> method, which returns the lower</span>
<span>and the upper bound on the size of the stack. Fully concrete futures type-erased to </span><code>dyn Future</code>
<span>would return the exact amount </span><code>(a, Some(a))</code><span>. The caller would be </span><em><span>required</span></em><span> to allocate at least</span>
<code>a</code><span> bytes of the async stack. The callee uses that contract to elide stack checks. Unknown bound,</span>
<code>(a, None)</code><span> would only be returned if type-erased concrete future </span><em><span>itself</span></em><span> calls something</span>
<span>dynamically dispatched. So only dynamically dispatched calls would have to do stack grow checks, and</span>
<span>that cost seems negligible in comparison to the cost of missing optimizations due to inability to</span>
<span>inline.</span></p>
<p><span>Altogether, it feels like this adds up to something sufficiently cheap to just call it </span>&ldquo;<span>async stack</span>
<span>allocation</span>&rdquo;<span>.</span></p>
<p><span>I guess that</span>&rsquo;<span>s all for today? Summarizing:</span></p>
<ul>
<li>
<span>Inter-task vs intra-task distinction is mostly orthogonal to the question of parallelism.</span>
</li>
<li>
<span>I claim that this is the same distinction as between </span><em><span>eager</span></em><span> and </span><em><span>lazy</span></em><span> futures.</span>
</li>
<li>
<span>In particular, there</span>&rsquo;<span>s no principled obstacles for runtime-bounded intra-task concurrency.</span>
</li>
<li>
<span>But we do miss </span><code>FuturesUnordered</code><span>, but nice. The </span><code>concurrently</code><span> operator/function feels like a</span>
<span>sufficiently low-hanging watermelon here.</span>
</li>
<li>
<span>One wrinkle is that watermelon requires dynamic allocation, but it looks like we could just</span>
<span>completely upend the compilation strategy we use for futures, implement async segmented stacks</span>
<span>which should be pretty fast, and </span><em><span>also</span></em><span> gain nice dynamically dispatched (and recursive) async</span>
<span>functions for free?</span>
</li>
</ul>
<hr>
<p><span>Haha, just kidding! Bonus content! This really should be a separate blog post, but it is</span>
<span>tangentially related, so here we go:</span></p>
</section>
<section id="Applied-Duality">

    <h2>
    <a href="#Applied-Duality"><span>Applied Duality</span> </a>
    </h2>
<p><span>So far, we</span>&rsquo;<span>ve focused on </span><code>join</code><span>, the operator that takes two futures, and </span>&ldquo;<span>runs</span>&rdquo;<span> them concurrently,</span>
<span>returning both results as a pair. But there</span>&rsquo;<span>s a second, dual operator:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">race</span>&lt;F1, F2&gt;(</span>
<span class="line">  fut1: F1,</span>
<span class="line">  fut2: F2,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> Either&lt;F1::Output, F2::Output&gt;</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F1: Future,</span>
<span class="line">  F2: Future,</span></code></pre>

</figure>
<p><span>Like </span><code>join</code><span>, </span><code>race</code><span> runs two futures concurrently. Unlike </span><code>join</code><span>,  it returns only one result </span>&mdash;
<span>that which came first. This operator is the basis for a more general </span><code>select</code><span> facility.</span></p>
<p><span>Although </span><code>race</code><span> is dual to </span><code>join</code><span>, I don</span>&rsquo;<span>t think it is as fundamental. It is possible to have two</span>
<span>dual things, where one of them is in the basis and the other is derived. For example, it is an axiom</span>
<span>of the set theory that the union of two sets, </span><code>A ∪ B</code><span>, is a set. Although the intersection of sets,</span>
<code>A ∩ B</code><span> is a dual for union, existence of intersection is </span><em><span>not</span></em><span> an axiom. Rather, the intersection</span>
<span>is defined using axiom of specification:</span></p>

<figure class="code-block">


<pre><code><span class="line">A ∩ B := {x ∈ A : x ∈ B}</span></code></pre>

</figure>
<p><span class="display"><span>Proposition 131.7.1: </span><code>race</code><span> can be defined in terms of </span><code>join</code></span></p>
<p><span>The </span><code>race</code><span> operator is trickier than it seems. Yes, it returns the result of the future that</span>
<span>finished first, but what happens with the other one? It gets cancelled. Rust implements this</span>
<span>cancellation </span>&ldquo;<span>for free</span>&rdquo;<span>, by just dropping the future, but this is restrictive. This is precisely the</span>
<span>issue that prevents </span><code>pjoin</code><span> from working.</span></p>
<p><span>I postulate that fully general cancellation is an asynchronous protocol:</span></p>
<ol>
<li>
<span>A </span><em><span>requests</span></em><span> that B is cancelled.</span>
</li>
<li>
<span>B receives this cancellation request and starts winding down.</span>
</li>
<li>
<span>A </span><em><span>waits</span></em><span> until B is cancelled.</span>
</li>
</ol>
<p><span>That is, cancellation is not </span>&ldquo;<span>I cancel thou</span>&rdquo;<span>. Rather it is </span>&ldquo;<span>I ask you to stop, and then I</span>
<span>cooperatively wait until you do so</span>&rdquo;<span>. This is very abstract, but the following three examples should</span>
<span>help make this concrete.</span></p>
<ol>
<li>
<p><span>A is some generic asynchronous task, which offloads some computation-heavy work to a CPU pool.</span>
<span>That work (B) </span><em><span>doesn</span>&rsquo;<span>t</span></em><span> have checks for cancelled flags. So, if A is canceled, it can</span>&rsquo;<span>t really</span>
<span>stop B, which means we are violating structured concurrency.</span></p>
</li>
<li>
<p><span>A is doing async IO. Specifically, A uses </span><code>io_uring</code><span> to read data from a socket. A owns a buffer,</span>
<span>and passes a pointer to it to the kernel via </span><code>io_uring</code><span> as the target buffer for a </span><code>read</code>
<span>syscall. While A is being cancelled, the kernel writes data to this buffer. If A doesn</span>&rsquo;<span>t wait</span>
<span>until the kernel is done, buffer</span>&rsquo;<span>s memory might get reused, and the kernel would corrupt some</span>
<span>unrelated data.</span></p>
</li>
</ol>
<p><span>These examples are somewhat unsatisfactory </span>&mdash;<span> A is philosophical (who needs structured</span>
<span>concurrency?), while B is esoteric (who uses </span><code>io_uring</code><span> in 2024?). But the two can be combined into</span>
<span>something rather pedestrianly bad:</span></p>
<p><span>Like in the case A, an async task submits some work to a CPU pool. But this time the work is very</span>
<span>specific </span>&mdash;<span> computing a cryptographic checksum of a message owned by A. </span><em><span>Because</span></em><span> this is</span>
<span>cryptography, this is going to be some hyper-optimized SIMD loop which definitely won</span>&rsquo;<span>t have any</span>
<span>affordance for checking some sort of a </span><code>cancelled</code><span> flag. The loop would have to run to completion,</span>
<span>or at least to a safe point. And, because the loop checksums data owned by A, we can</span>&rsquo;<span>t destroy A</span>
<span>before the loop exits, otherwise it</span>&rsquo;<span>ll be reading garbage memory!</span></p>
<p><span>And </span><em><span>this</span></em><span> example is the reason why</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">pjoin</span>&lt;F1, F2&gt;(</span>
<span class="line">  fut1: F1,</span>
<span class="line">  fut2: F2,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> (F1::Output, F2::Output)</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F1: Future + <span class="hl-built_in">Send</span>,</span>
<span class="line">  F1::Output:  <span class="hl-built_in">Send</span>,</span>
<span class="line">  F2: Future + <span class="hl-built_in">Send</span>,</span>
<span class="line">  F2::Output:  <span class="hl-built_in">Send</span>,</span></code></pre>

</figure>
<p><span>can</span>&rsquo;<span>t be a thing in Rust </span>&mdash;<span> if </span><code>fut1</code><span> runs on a thread separate from the </span><code>pjon</code><span> future, then, if</span>
<code>pjoin</code><span> ends up being cancelled, </span><code>fut1</code><span> would be pointing at garbage. You could have</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">pjoin</span>&lt;F1, F2&gt;(</span>
<span class="line">  fut1: F1,</span>
<span class="line">  fut2: F2,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> (F1::Output, F2::Output)</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F1: Future + <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;static</span>,</span>
<span class="line">  F1::Output:  <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;static</span>,</span>
<span class="line">  F2: Future + <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;static</span>,</span>
<span class="line">  F2::Output:  <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;static</span>,</span></code></pre>

</figure>
<p><span>but that removes one of the major benefits of intra-task style API </span>&mdash;<span> ability to just borrow data.</span></p>
<p><span>So the fully general cancellation should be cooperative. Let</span>&rsquo;<span>s assume that it is driven by some sort</span>
<span>of cancellation token API:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">CancellationSource</span> {</span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">request_cancellation</span>(&amp;<span class="hl-keyword">self</span>) { ... }</span>
<span class="line">  <span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">await_cancellation</span>(<span class="hl-keyword">self</span>) { ...  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">cancel</span>(<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">request_cancellation</span>();</span>
<span class="line">    <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">await_cancellation</span>().<span class="hl-keyword">await</span>;</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">new_token</span>(&amp;<span class="hl-keyword">self</span>) <span class="hl-punctuation">-&gt;</span> CancellationToken { ... }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">CancellationToken</span> {</span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">is_cancelled</span>(&amp;<span class="hl-keyword">self</span>) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">bool</span> { ... }</span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">on_cancelled</span>(&amp;<span class="hl-keyword">self</span>, callback: <span class="hl-keyword">impl</span> <span class="hl-title class_">FnOnce</span>()) { ... }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Note that the question of cancellation being cooperative is </span><em><span>orthogonal</span></em><span> to the question of explicit</span>
<span>threading of cancellation tokens! They can be threaded implicitly (cooperative, implicit</span>
<span>cancellation is how Python</span>&rsquo;<span>s trio does this, though they don</span>&rsquo;<span>t really document the cooperative part</span>
<span>(the shields stuff)).</span></p>
<p><span>With this, we can write our own </span><code>race</code><span> </span>&mdash;<span> we</span>&rsquo;<span>ll create a cancellation scope and then </span><em><span>join</span></em>
<span>modified futures, each of which would cancel the other upon completion:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">race</span>&lt;U, V&gt;(</span>
<span class="line">  fut1: <span class="hl-keyword">impl</span> <span class="hl-title class_">async</span> <span class="hl-title function_ invoke__">FnOnce</span>(&amp;CancellationToken) <span class="hl-punctuation">-&gt;</span> U,</span>
<span class="line">  fut2: <span class="hl-keyword">impl</span> <span class="hl-title class_">async</span> <span class="hl-title function_ invoke__">FnOnce</span>(&amp;CancellationToken) <span class="hl-punctuation">-&gt;</span> V,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> Either&lt;U, V&gt; {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">source</span> = CancellationSource::<span class="hl-title function_ invoke__">new</span>();</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">token</span> = source.<span class="hl-title function_ invoke__">new_token</span>();</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">u_or_v</span> = <span class="hl-title function_ invoke__">join</span>(</span>
<span class="line">    <span class="hl-keyword">async</span> {</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">u</span> = <span class="hl-title function_ invoke__">fut1</span>(&amp;token).<span class="hl-keyword">await</span>;</span>
<span class="line">      <span class="hl-keyword">if</span> token.<span class="hl-title function_ invoke__">is_cancelled</span>() {</span>
<span class="line">        <span class="hl-keyword">return</span> <span class="hl-literal">None</span>;</span>
<span class="line">      }</span>
<span class="line">      source.<span class="hl-title function_ invoke__">cancel</span>();</span>
<span class="line">      <span class="hl-title function_ invoke__">Some</span>(u)</span>
<span class="line">    },</span>
<span class="line">    <span class="hl-keyword">async</span> {</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">v</span> = <span class="hl-title function_ invoke__">fut2</span>(&amp;token).<span class="hl-keyword">await</span>;</span>
<span class="line">      <span class="hl-keyword">if</span> token.<span class="hl-title function_ invoke__">is_cancelled</span>() {</span>
<span class="line">        <span class="hl-keyword">return</span> <span class="hl-literal">None</span>;</span>
<span class="line">      }</span>
<span class="line">      source.<span class="hl-title function_ invoke__">cancel</span>();</span>
<span class="line">      <span class="hl-title function_ invoke__">Some</span>(v)</span>
<span class="line">    },</span>
<span class="line">  )</span>
<span class="line">  .<span class="hl-keyword">await</span>;</span>
<span class="line">  <span class="hl-keyword">match</span> u_or_v {</span>
<span class="line">    (<span class="hl-title function_ invoke__">Some</span>(u), <span class="hl-literal">None</span>) =&gt; <span class="hl-title function_ invoke__">Left</span>(u),</span>
<span class="line">    (<span class="hl-literal">None</span>, <span class="hl-title function_ invoke__">Some</span>(v)) =&gt; <span class="hl-title function_ invoke__">Right</span>(v),</span>
<span class="line">    _ =&gt; <span class="hl-built_in">unreachable!</span>(),</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>In other words, </span><code>race</code><span> is but a cooperatively-cancelled </span><code>join</code><span>!</span></p>
<p><span>That</span>&rsquo;<span>s all for real for today, viva la vida!</span></p>
</section>
]]></content>
</entry>

<entry>
<title type="text">What is io_uring?</title>
<link href="https://matklad.github.io/2024/09/23/what-is-io-uring.html" rel="alternate" type="text/html" title="What is io_uring?" />
<published>2024-09-23T00:00:00+00:00</published>
<updated>2024-09-23T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/09/23/what-is-io-uring</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[An attempt at concise explanation of what io_uring is.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/09/23/what-is-io-uring.html"><![CDATA[
<h1><span>What is io_uring?</span> <time class="meta" datetime="2024-09-23">Sep 23, 2024</time></h1>
<p><span>An attempt at concise explanation of what io_uring is.</span></p>
<p><code>io_uring</code><span> is a new Linux kernel interface for making system calls.</span>
<span>Traditionally, syscalls are submitted to the kernel </span><strong><span>individually</span></strong><span> and</span>
<strong><span>synchronously</span></strong><span>: a syscall CPU instruction transfers control from the</span>
<span>application to the kernel; control returns to the application only when the</span>
<span>syscall is completed. In contrast, </span><code>io_uring</code><span> is a </span><strong><span>batched</span></strong><span> and </span><strong><span>asynchronous</span></strong>
<span>interface. The application submits several syscalls by writing their codes &amp;</span>
<span>arguments to a lock-free shared-memory ring buffer. The kernel reads the</span>
<span>syscalls from this shared memory and executes them at its own pace. To</span>
<span>communicate results back to the application, the kernel writes the results to a</span>
<span>second lock-free shared-memory ring buffer, where they become available to the</span>
<span>application asynchronously.</span></p>
<p><span>You might want to use </span><code>io_uring</code><span> if:</span></p>
<ul>
<li>
<span>you need extra performance unlocked by amortizing userspace/kernelspace</span>
<span>context switching across entire batches of syscalls,</span>
</li>
<li>
<span>you want a unified asynchronous interface to the entire system.</span>
</li>
</ul>
<p><span>You might want to avoid </span><code>io_uring</code><span> if:</span></p>
<ul>
<li>
<span>you need to write portable software,</span>
</li>
<li>
<span>you want to use only old, proven features,</span>
</li>
<li>
<span>and in particular you want to use features with a good security track record.</span>
</li>
</ul>
]]></content>
</entry>

<entry>
<title type="text">Try to Fix It One Level Deeper</title>
<link href="https://matklad.github.io/2024/09/06/fix-one-level-deeper.html" rel="alternate" type="text/html" title="Try to Fix It One Level Deeper" />
<published>2024-09-06T00:00:00+00:00</published>
<updated>2024-09-06T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/09/06/fix-one-level-deeper</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[I had a productive day today! I did many different and unrelated things, but they all had the same
unifying theme:]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/09/06/fix-one-level-deeper.html"><![CDATA[
<h1><span>Try to Fix It One Level Deeper</span> <time class="meta" datetime="2024-09-06">Sep 6, 2024</time></h1>
<p><span>I had a productive day today! I did many different and unrelated things, but they all had the same</span>
<span>unifying theme:</span></p>
<p><span>There</span>&rsquo;<span>s a bug! And it is sort-of obvious how to fix it. But if you don</span>&rsquo;<span>t laser-focus on that, and</span>
<span>try to perceive the surrounding context, it turns out that the bug is valuable, and it is pointing</span>
<span>in the direction of a bigger related problem. So, instead of fixing the bug directly, a detour is</span>
<span>warranted to close off the avenue for a class of bugs.</span></p>
<p><span>Here are the examples!</span></p>
<p><span>In the morning, my colleague pointed out that we are giving substandard error message for a pretty</span>
<span>stressful situation when the database runs out of disk space. I went ahead and added appropriate log</span>
<span>messages to make it clearer. But then I stopped for a moment and noticed that the problem is bigger</span>
&mdash;<span> we are missing an infrastructure for fatal errors, and </span><code>NoSpaceLeft</code><span> is just one of a kind. So I</span>
<span>went ahead and added that along the way:</span>
<a href="https://github.com/tigerbeetle/tigerbeetle/pull/2289"><span>#2289</span></a><span>.</span></p>
<p><span>Then, I was reviewing a PR by </span><code>@martinconic</code><span> which was fixing some typos, and noticed that it was</span>
<span>also changing the formatting of our Go code. The latter is by far the biggest problem, as it is the</span>
<span>sign that we somehow are not running </span><code>gofmt</code><span> during our CI, which I fixed in</span>
<a href="https://github.com/tigerbeetle/tigerbeetle/pull/2287"><span>#2287</span></a><span>.</span></p>
<p><span>Then, there was a PR from yesterday, where we again had a not quite right log message. The cause was</span>
<span>a confusion between two compile-time configuration parameters, which were close, but not quite</span>
<span>identical. So, instead of fixing the error message I went ahead and made the two parameters</span>
<em><span>exactly</span></em><span> the same. But then my colleague noticed that I actually failed to fix it one level deeper</span>
<span>in this case! Turns out, it is possible to remove this compile-time parametrization altogether,</span>
<span>which I did in </span><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2292"><span>#2292</span></a><span>.</span></p>
<p><span>But these all were randomly-generated side quests. My intended story line for today was to refactor</span>
<span>the piece of code I had trouble explaining (and understanding!) on </span><a href="https://www.youtube.com/watch?v=C3XAteN_lYk&amp;list=PL9eL-xg48OM3pnVqFSRyBFleHtBBw-nmZ&amp;index=41"><span>yesterday</span>&rsquo;<span>s</span>
<span>episode</span></a>
<span>of Iron Beetle. To get into the groove, I decided to first refactor the code that </span><em><span>calls</span></em><span> the</span>
<span>problematic piece of logic, as I noticed a couple of minor stylistic problems there. Of course, when</span>
<span>doing that, I discovered that we have a bit of dead code, which luckily doesn</span>&rsquo;<span>t affect correctness,</span>
<span>but does obscure the logic. While fixing that, I used one of my favorite Zig patterns:</span>
<span class="display"><code>defer assert(postcondition);</code></span></p>
<p><span>It of course failed in the simulator in a way postcondition checks tend to fail </span>&mdash;<span> there was an</span>
<span>unintended reentrancy in the code. So I slacked my colleague something like</span></p>

<figure class="blockquote">
<blockquote><p><span>I thought myself to be so clever adding this assert, but now it fails and I have to fix it TT</span>
<span>I think I</span>&rsquo;<span>ll just go and </span><code>.next_tick</code><span> the prefetch path. It feels like there should be a more</span>
<span>elegant solution here, but I am not seeing it.</span></p>
</blockquote>

</figure>
<p><span>But of course I can</span>&rsquo;<span>t just </span>&ldquo;<span>go and </span><code>.next_tick</code><span> it</span>&rdquo;<span>, so here I am, trying to figure out how to</span>
<span>encode a </span><a href="https://en.wikipedia.org/wiki/Duff%27s_device"><span>Duff</span>&rsquo;<span>s device</span></a><span> in Zig</span>
<span>pre-</span><a href="https://github.com/ziglang/zig/issues/8220"><span>#8220</span></a><span>, so as to make this class of issues much</span>
<span>less likely.</span></p>
]]></content>
</entry>

<entry>
<title type="text">The Fundamental Law Of Software Dependencies</title>
<link href="https://matklad.github.io/2024/09/03/the-fundamental-law-of-dependencies.html" rel="alternate" type="text/html" title="The Fundamental Law Of Software Dependencies" />
<published>2024-09-03T00:00:00+00:00</published>
<updated>2024-09-03T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/09/03/the-fundamental-law-of-dependencies</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[Canonical source code for software should include checksums of the content of all its
dependencies.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/09/03/the-fundamental-law-of-dependencies.html"><![CDATA[
<h1><span>The Fundamental Law Of Software Dependencies</span> <time class="meta" datetime="2024-09-03">Sep 3, 2024</time></h1>

<figure class="blockquote">
<blockquote><p><span>Canonical source code for software should include checksums of the content of </span><em><span>all</span></em><span> its</span>
<span>dependencies.</span></p>
</blockquote>

</figure>
<p><span>Several examples of the law:</span></p>
<p><span>Software obviously depends on its source code. The law says that </span><em><span>something</span></em><span> should hold the hash of</span>
<span>the entire source, and thus mandates the use of a content-addressed version control system such as</span>
<span>git.</span></p>
<p><span>Software often depends on 3rd party libraries. These libraries could in turn depend on </span><em><span>other</span></em>
<span>libraries. It is imperative to include a lockfile that covers this entire set and comes with</span>
<span>checksums. Curiously, the lockfile itself is a part of source code, and gets mixed into the VCS</span>
<span>root hash.</span></p>
<p><span>Software needs a compiler. The </span><em><span>hash</span></em><span> of the required compiler should be included in the lockfile.</span>
<span>Typically, this is not done </span>&mdash;<span> only the version is specified. I think that is a mistake. Specifying</span>
<span>a version and a hash is not much more trouble than just the version, but that gives you a superpower</span>
&mdash;<span> you no longer need to trust the party that distributes your compiler. You could take a shady</span>
<span>blob of bytes you</span>&rsquo;<span>ve found laying on the street, as long as its checksum checks out.</span></p>
<p><span>Note that you can compress hashes by mixing them. For compiler use-case, there</span>&rsquo;<span>s a separate hash per</span>
<span>platform, because the Linux and the Windows versions of the compiler differ. This doesn</span>&rsquo;<span>t mean that</span>
<span>your project should include one compiler</span>&rsquo;<span>s hash per platform, one hash is enough. Compiler</span>
<span>distribution should include a manifest </span>&ndash;<span> a small text file which lists all platform and their</span>
<span>platform specific hashes. The single hash of </span><em><span>that</span></em><span> file is what is to be included by downstream</span>
<span>consumers. To verify a specific binary, the consumer first downloads a manifest, checks that it</span>
<span>has the correct hash, and then extracts the hash for the specific platform.</span></p>
<hr>
<p><span>The law is an instrumental goal. By itself, hashes are not </span><em><span>that</span></em><span> useful. But to get to the point</span>
<span>where you actually </span><em><span>know</span></em><span> the hashes requires:</span></p>
<ul>
<li>
<span>Actually learning </span><em><span>what</span></em><span> are your dependencies (this is </span><em><span>not</span></em><span> trivial! If you have a single</span>
<span>Makefile or an </span><code>.sh</code><span>, you most likely don</span>&rsquo;<span>t know the set of your dependencies).</span>
</li>
<li>
<span>Coming up with some automated way to download those dependencies.</span>
</li>
<li>
<span>Fixing dependencies</span>&rsquo;<span>s build process to become reproducible, so as to have a meaningful hash at</span>
<span>all.</span>
</li>
<li>
<span>Learning to isolate dependencies per project, as hashed dependencies can</span>&rsquo;<span>t be installed into a</span>
<span>global shared namespace.</span>
</li>
</ul>
<p><em><span>These</span></em><span> things are what actually make developing software easier.</span></p>
]]></content>
</entry>

<entry>
<title type="text">STD Doesn't Have to Abstract OS IO</title>
<link href="https://matklad.github.io/2024/08/12/std-io.html" rel="alternate" type="text/html" title="STD Doesn't Have to Abstract OS IO" />
<published>2024-08-12T00:00:00+00:00</published>
<updated>2024-08-12T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/08/12/std-io</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[A short note on what goes into a language's standard library, and what's left for third party
libraries to implement!]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/08/12/std-io.html"><![CDATA[
<h1><span>STD Doesn</span>&rsquo;<span>t Have to Abstract OS IO</span> <time class="meta" datetime="2024-08-12">Aug 12, 2024</time></h1>
<p><span>A short note on what goes into a language</span>&rsquo;<span>s standard library, and what</span>&rsquo;<span>s left for third party</span>
<span>libraries to implement!</span></p>
<p><span>Usually, the main underlying driving factor here is cardinality. If it is important that there</span>&rsquo;<span>s</span>
<span>only one of a thing, it goes into std. If having many of a thing is a requirement, it is better</span>
<span>handled by a third-party library. That is, the usual physical constraint is that there</span>&rsquo;<span>s only a</span>
<em><span>single</span></em><span> standard library, and everyone uses the same standard library. In contrast, there are many</span>
<span>different third-party libraries, and they all can be used at the same time.</span></p>
<p><span>So, until very recently, my set of rules of thumb for what goes into stdlib looked roughly like</span>
<span>this:</span></p>
<ol>
<li>
<span>If this is a vocabulary type, which will be used by APIs of different libraries, it should be in</span>
<span>the stdlib.</span>
</li>
<li>
<span>If this is a cross platform abstraction around an IO facility provided by an OS, and this IO</span>
<span>facility has a reasonable common subset across most OSes, it should be in the stdlib.</span>
</li>
<li>
<span>If there</span>&rsquo;<span>s one obvious way to implement it, it might go to stdlib.</span>
</li>
</ol>
<p><span>So for example something like </span><a href="https://doc.rust-lang.org/stable/std/vec/struct.Vec.html"><code>Vec</code></a><span> goes</span>
<span>into a standard library, because all </span><em><span>other</span></em><span> libraries are going to use vectors at the interfaces.</span></p>
<p><span>Something like </span><a href="https://docs.rs/lazy_static/1.5.0/lazy_static/macro.lazy_static.html"><code>lazy_static</code></a>
<span>doesn</span>&rsquo;<span>t: while it is often needed, it is </span><em><span>not</span></em><span> a vocabulary interface type.</span></p>
<p><span>But it is acceptable for something like</span>
<a href="https://docs.rs/once_cell/1.19.0/once_cell/sync/struct.OnceCell.html"><code>OnceCell</code></a><span> to be in </span><code>std</code>
&mdash;<span> it is still not a vocabulary type, but, unlike </span><code>lazy_static</code><span>, it is clear that the API is more</span>
<span>or less optimal, and that there aren</span>&rsquo;<span>t that many good options to do this differently.</span></p>
<p><span>But I</span>&rsquo;<span>ve changed my mind about the second bullet point, about facilities like file IO or TCP</span>
<span>sockets. I was </span><em><span>always</span></em><span> under the impression that these things are a must for a standard library.</span>
<span>But now I think that</span>&rsquo;<span>s not necessarily true!</span></p>
<p><span>Consider randomness. Not the PRNG kind of randomness you</span>&rsquo;<span>d use to make a game fun, but a</span>
<span>cryptographically secure randomness that you</span>&rsquo;<span>d use to generate an SSH key pair. This sort of</span>
<span>randomness ultimately bottoms out in hardware, and fundamentally requires talking to the OS and</span>
<span>doing IO. This is squarely the bullet point number 2. And Rust is an interesting case study here: it</span>
<span>failed to provide this abstraction in std, even though std itself actually needs it! But this turned</span>
<span>out to be mostly a non-issue in practice </span>&mdash;<span> a third party crate, </span><code>getrandom</code><span>, took the job of</span>
<span>writing all the relevant bindings to various platform-specific API and using a bunch of conditional</span>
<span>compilation to abstract that all away and provide a nice cross-platform API.</span></p>
<p><span>So, no, it is not a </span><em><span>requirement</span></em><span> that std has to wrap any wrappable IOing API. This </span><em><span>could</span></em><span> be</span>
<span>handled by the library ecosystem, </span><em><span>if</span></em><span> the language allows first-class bindings to raw OS APIs</span>
<span>outside of compiler-privileged code (and Rust certainly allows for that).</span></p>
<p><span>So perhaps it won</span>&rsquo;<span>t be too unreasonable to leave even things like files and sockets to community</span>
<span>experimentation? In a sense, that is happening in the async land anyway.</span></p>
<hr>
<p><span>To clarify, I still believe that Rust </span><em><span>should</span></em><span> provide bindings to OS-sourced crypto randomness, and</span>
<span>I am extremely happy to see recent motion in that area. But the reason for this belief changed. I no</span>
<span>longer feel the mere fact that OS-specific APIs are involved to be particularly salient. However, it</span>
<span>is still true that there</span>&rsquo;<span>s more or less </span><a href="https://fuchsia.dev/reference/syscalls/cprng_draw"><span>one correct way to do</span>
<span>this</span></a><span>.</span></p>
]]></content>
</entry>

<entry>
<title type="text">Primitive Recursive Functions For A Working Programmer</title>
<link href="https://matklad.github.io/2024/08/01/primitive-recursive-functions.html" rel="alternate" type="text/html" title="Primitive Recursive Functions For A Working Programmer" />
<published>2024-08-01T00:00:00+00:00</published>
<updated>2024-08-01T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/08/01/primitive-recursive-functions</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[Programmers on the internet often use Turing-completeness terminology. Typically, not being
Turing-complete is extolled as a virtue or even a requirement in specific domains. I claim that most
such discussions are misinformed --- that not being Turing complete doesn't actually mean what folks
want it to mean, and is instead a stand-in for a bunch of different practically useful properties,
which are mostly orthogonal to actual Turing completeness.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/08/01/primitive-recursive-functions.html"><![CDATA[
<h1><span>Primitive Recursive Functions For A Working Programmer</span> <time class="meta" datetime="2024-08-01">Aug 1, 2024</time></h1>
<p><span>Programmers on the internet often use </span>&ldquo;<span>Turing-completeness</span>&rdquo;<span> terminology. Typically, not being</span>
<span>Turing-complete is extolled as a virtue or even a requirement in specific domains. I claim that most</span>
<span>such discussions are misinformed </span>&mdash;<span> that not being Turing complete doesn</span>&rsquo;<span>t actually mean what folks</span>
<span>want it to mean, and is instead a stand-in for a bunch of different practically useful properties,</span>
<span>which are mostly orthogonal to actual Turing completeness.</span></p>
<p><span>While I am generally descriptivist in nature and am ok with words losing their </span><em><span>original</span></em><span> meaning</span>
<span>as long as the new meaning is sufficiently commonly understood, Turing completeness is a hill I will</span>
<span>die on. It is a term from math, it has a very specific meaning, and you are not allowed to</span>
<span>re-purpose it for anything else, sorry!</span></p>
<p><span>I understand why this happens: to really understand what Turing completeness is and is not you need</span>
<span>to know one (simple!) theoretical result about so-called primitive recursive functions. And,</span>
<span>although this result is simple, I was only made aware of it in a fairly advanced course during my</span>
<span>masters. That</span>&rsquo;<span>s the CS education deficiency I want to rectify </span>&mdash;<span> you can</span>&rsquo;<span>t teach students the</span>
<span>halting problem without </span><em><span>also</span></em><span> teaching them about primitive recursion!</span></p>
<p><span>The post is going to be rather meaty, and will be split in three parts:</span></p>
<p><span>In Part I, I give a TL;DR for the theoretical result and some of its consequences. Part II is going</span>
<span>to be a whirlwind tour of Turing Machines, Finite State Automata and Primitive Recursive Functions.</span>
<span>And then Part III will circle back to practical matters.</span></p>
<p><span>If math makes you slightly nauseous, you might to skip Part II. But maybe give it a try? The math</span>
<span>we</span>&rsquo;<span>ll need will be baby math from first principles, without reference to any advanced results.</span></p>
<section id="Part-I-TL-DR">

    <h2>
    <a href="#Part-I-TL-DR"><span>Part I: TL;DR</span> </a>
    </h2>
<p><span>Here</span>&rsquo;<span>s the key result </span>&mdash;<span> suppose you have a program in some Turing complete language, and you also</span>
<span>know that it</span>&rsquo;<span>s not too slow. Suppose it runs faster than</span>
<span class="display"><span>O(2</span><sup><span>2</span><sup><span>N</span></sup></sup><span>).</span></span>
<span>That is, two to the power of two to the power of N, a very large number. In this case, you can</span>
<span>implement this algorithm in a non-Turing complete language.</span></p>
<p><span>Most practical problems fall into this </span>&ldquo;<span>faster than two to the two to the power of two</span>&rdquo;<span> space.</span>
<span>Hence it follows that you don</span>&rsquo;<span>t need the full power of a Turing Machine to tackle them. Hence, a</span>
<span>language not being Turing complete doesn</span>&rsquo;<span>t in any way restrict you in practice, or give you extra</span>
<span>powers to control the computation.</span></p>
<p><span>Or, to restate this: in practice, a program which doesn</span>&rsquo;<span>t terminate, and a program that needs a</span>
<span>billion billion steps to terminate are equivalent. Making something non-Turing complete by itself</span>
<span>doesn</span>&rsquo;<span>t help with the second problem in any way. And there</span>&rsquo;<span>s a trivial approach that solves the</span>
<span>first problem for any existing Turing-complete language </span>&mdash;<span> in the implementation, count the steps</span>
<span>and bail with an error after a billion.</span></p>
</section>
<section id="Part-II-Weird-Machines">

    <h2>
    <a href="#Part-II-Weird-Machines"><span>Part II: Weird Machines</span> </a>
    </h2>
<p><span>The actual theoretical result is quite a bit more general than that. It is (unsurprisingly)</span>
<span>recursive:</span></p>

<figure class="blockquote">
<blockquote><p><span>If a function is computed by a Turing Machine, and the runtime of this machine is bounded by some</span>
<span>primitive recursive function of input, then the original function itself can be written as a</span>
<span>primitive recursive function.</span></p>
</blockquote>

</figure>
<p><span>It is expected that this sounds like gibberish at this point! So let</span>&rsquo;<span>s just go and prove this thing,</span>
<span>right here in this blog post! Will work up slowly towards this result. The plan is as follows:</span></p>
<ul>
<li>
<em><span>First</span></em><span>, to brush up notation, we</span>&rsquo;<span>ll define Finite State Machines.</span>
</li>
<li>
<em><span>Second</span></em><span>, we</span>&rsquo;<span>ll turn our humble Finite State Machine into the all-powerful Turing Machine (spoiler</span>
&mdash;<span> a Turing Machine is an FSM with a pair of stacks), and, as is customary, wave our hands about</span>
<span>the Universal Turing Machine.</span>
</li>
<li>
<em><span>Third</span></em><span>, we leave the cozy world of imperative programming and define primitive recursive</span>
<span>functions.</span>
</li>
<li>
<em><span>Finally</span></em><span>, we</span>&rsquo;<span>ll talk about the relative computational power of TMs and PRFs, including the teased</span>
<span>up result and more!</span>
</li>
</ul>
</section>
<section id="Finite-State-Machines">

    <h2>
    <a href="#Finite-State-Machines"><span>Finite State Machines</span> </a>
    </h2>
<p><dfn><span>Finite State Machines</span></dfn><span> are simple! An FSM takes a string as input, and returns a binary</span>
<span>answer, </span>&ldquo;<span>yes</span>&rdquo;<span> or </span>&ldquo;<span>no</span>&rdquo;<span>. Unsurprisingly an FSM has a finite number of states: Q0, Q1, </span>&hellip;<span>, Qn.</span>
<span>A subset of states are designated as </span>&ldquo;<span>yes</span>&rdquo;<span> states, the rest are </span>&ldquo;<span>no</span>&rdquo;<span> states. There</span>&rsquo;<span>s also one</span>
<span>specific starting state.</span></p>
<p><span>The behavior of the state machine is guided by a transition (step) function, </span><code>s</code><span>. This function</span>
<span>takes the current state of FSM, the next symbol of input, and returns a new state.</span></p>
<p><span>The semantics of FSM is determined by repeatably applying the single step function for all symbols of</span>
<span>the input, and noting whether the final state is a </span>&ldquo;<span>yes</span>&rdquo;<span> state or a </span>&ldquo;<span>no</span>&rdquo;<span> state.</span></p>
<p><span>Here</span>&rsquo;<span>s an FSM which accepts only strings of zeros and ones of even length:</span></p>

<figure class="code-block">


<pre><code><span class="line">States:     { Q0, Q1 }</span>
<span class="line">Yes States: { Q0 }</span>
<span class="line">Start State:  Q0</span>
<span class="line"></span>
<span class="line">s :: State -&gt; Symbol -&gt; State</span>
<span class="line">s Q0 0 = Q1</span>
<span class="line">s Q0 1 = Q1</span>
<span class="line">s Q1 0 = Q0</span>
<span class="line">s Q1 1 = Q0</span></code></pre>

</figure>
<p><span>This machine ping-pongs between states Q0 and Q1 ends up in Q0 only for inputs of even length</span>
<span>(including an empty input).</span></p>
<p><span>What can FSMs do? As they give a binary answer, they are recognizers </span>&mdash;<span> they don</span>&rsquo;<span>t compute</span>
<span>functions, but rather just characterize certain sets of strings. A famous result is that the</span>
<span>expressive power of FSMs is equivalent to the expressive power of regular expressions. If you can</span>
<span>write a regular expression for it, you could also do an FSM!</span></p>
<p><span>There are also certain things that state machines can</span>&rsquo;<span>t do. For example they can</span>&rsquo;<span>t enter an infinite</span>
<span>loop. Any FSM is linear in the input size and always terminates. But there are much more specific</span>
<span>sets of strings that couldn</span>&rsquo;<span>t be recognized by an FSM. Consider this set:</span></p>

<figure class="code-block">


<pre><code><span class="line">1</span>
<span class="line">010</span>
<span class="line">00100</span>
<span class="line">0001000</span>
<span class="line">...</span></code></pre>

</figure>
<p><span>That is, an infinite set which contains </span>&lsquo;<span>1</span>&rsquo;<span>s surrounded by the equal number of </span>&lsquo;<span>0</span>&rsquo;<span>s on the both</span>
<span>sides. Let</span>&rsquo;<span>s prove that there isn</span>&rsquo;<span>t a state machine that recognizes this set!</span></p>
<p><span>As usually, suppose there </span><em><span>is</span></em><span> such a state machine. It has a certain number of states </span>&mdash;<span> maybe a</span>
<span>dozen, maybe a hundred, maybe a thousand, maybe even more. But let</span>&rsquo;<span>s say fewer than a million.</span>
<span>Then, let</span>&rsquo;<span>s take a string which looks like a million zeros, followed by one, followed by million</span>
<span>zeros. And let</span>&rsquo;<span>s observe our FSM eating this particular string.</span></p>
<p><span>First of all, because the string is in fact a one surrounded by the equal number of zeros on both</span>
<span>sides, the FSM ends up in a </span>&ldquo;<span>yes</span>&rdquo;<span> state. Moreover, because the length of the string is much greater</span>
<span>than the number of states in the state machine, the state machine necessarily visits some state twice.</span>
<span>There is a cycle, where the machine goes from A to B to C to D and back to A. This cycle might be</span>
<span>pretty long, but it</span>&rsquo;<span>s definitely shorter than the total number of states we have.</span></p>
<p><span>And now we can fool the state machine. Let</span>&rsquo;<span>s make it eat our string again, but this time, once it</span>
<span>completes the ABCDA cycle, we</span>&rsquo;<span>ll force it to traverse this cycle again. That is, the original cycle</span>
<span>corresponds to some portion of our giant string:</span></p>

<figure class="code-block">


<pre><code><span class="line">0000 0000000000000000000 00 .... 1 .... 00000</span>
<span class="line">     &lt;- cycle portion -&gt;</span></code></pre>

</figure>
<p><span>If we duplicate this portion, our string will no longer look like one surrounded by equal number of</span>
<span>twos, but the state machine will still in the </span>&ldquo;<span>yes</span>&rdquo;<span> state. Which is a contradiction that completes</span>
<span>the proof.</span></p>
</section>
<section id="Turing-Machine-Definition">

    <h2>
    <a href="#Turing-Machine-Definition"><span>Turing Machine: Definition</span> </a>
    </h2>
<p><span>A </span><dfn><span>Turing Machine</span></dfn><span> is only slightly more complex than an FSM. Like an FSM, a TM has a bunch of states</span>
<span>and a single-step transition function. While an FSM has an immutable input which is being fed to it</span>
<span>symbol by symbol, a TM operates with a mutable tape. The input gets written to the tape at the</span>
<span>start. At each step, a TM looks at the current symbol on the tape, changes its state according to a</span>
<span>transition function and, additionally:</span></p>
<ul>
<li>
<span>Replaces the current symbol with a new one (which might or might not be different).</span>
</li>
<li>
<span>Moves the reading head that points at the current symbol one position to the left or to the right.</span>
</li>
</ul>
<p><span>When a machine reaches a designated halt state, it stops, and whatever is written on the tape at</span>
<span>that moment is the result. That is, while FSMs are binary recognizers, TMs are functions. Keep in</span>
<span>mind that a TM does not necessarily stop. It might be the case that a TM goes back and forth over the</span>
<span>tape, overwrites it, changes its internal state, but never quite gets to the final state.</span></p>
<p><span>Here</span>&rsquo;<span>s an example Turing Machine:</span></p>

<figure class="code-block">


<pre><code><span class="line">States:  {A, B, C, H}</span>
<span class="line">Start State: A</span>
<span class="line">Final State: H</span>
<span class="line"></span>
<span class="line">s :: State -&gt; Symbol -&gt; (State, Symbol, Left | Right)</span>
<span class="line">s A 0 = (B, 1, Right)</span>
<span class="line">s A 1 = (H, 1, Right)</span>
<span class="line">s B 0 = (C, 0, Right)</span>
<span class="line">s B 1 = (B, 1, Right)</span>
<span class="line">s C 0 = (C, 1, Left)</span>
<span class="line">s C 1 = (A, 1, Left)</span></code></pre>

</figure>
<p><span>If the configuration of the machine looks like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">000010100000</span>
<span class="line">     ^</span>
<span class="line">     A</span></code></pre>

</figure>
<p><span>Then we are in the </span><code>s A 0 = (B, 1, Right)</code><span> case, so we should change the state to B, replace 0 with</span>
<span>1, and move to the right:</span></p>

<figure class="code-block">


<pre><code><span class="line">000011100000</span>
<span class="line">      ^</span>
<span class="line">      B</span></code></pre>

</figure>
</section>
<section id="Turing-Machine-Programming">

    <h2>
    <a href="#Turing-Machine-Programming"><span>Turing Machine: Programming</span> </a>
    </h2>
<p><span>There are a bunch of fiddly details to Turing Machines!</span></p>
<p><span>The tape is conceptually infinite, so beyond the input, everything is just zeros. This creates a</span>
<span>problem: it might be hard to say where the input (or the output) ends! There are a couple of</span>
<span>technical solutions here. One is to say that there are three different symbols on the tape </span>&mdash;
<span>zeros, ones, and blanks, and require that the tape is initialized with blanks. A different solution</span>
<span>is to invent some encoding scheme. For example, we can say that the input is a sequence of 8-bit</span>
<span>bytes, without interior null bytes. So, eight consecutive zeros at a byte boundary designate the end</span>
<span>of input/output.</span></p>
<p><span>It</span>&rsquo;<span>s useful to think about how this byte-oriented TM could be implemented. We could have one large</span>
<span>state for each byte of input. So, Q142 would mean that the head is on the byte with value 142. And</span>
<span>then we</span>&rsquo;<span>ll have a bunch of small states to read out the current byte. Eg, we start reading a byte in</span>
<span>state </span><code>S</code><span>. Depending on the next bit we move to S0 or S1, then to S00, or S01, etc. Once we reached</span>
<span>something like S01111001, we move back 8 positions and enter state Q121. This is one of the patterns</span>
<span>of Turing Machine programming </span>&mdash;<span> while your main memory is the tape, you can represent some</span>
<span>constant amount of memory directly in the states.</span></p>
<p><span>What we</span>&rsquo;<span>ve done here is essentially lowering a byte-oriented Turing Machine to a bit-oriented</span>
<span>machine. So, we could think only in terms of big states operating on bytes, as we know the general</span>
<span>pattern for converting that to direct bit-twiddling.</span></p>
<p><span>With this encoding scheme in place, we now can feed arbitrary files to a Turing Machine! Which will</span>
<span>be handy to the next observation:</span></p>
<p><span>You can</span>&rsquo;<span>t actually program a Turing Machine. What I mean is that, counter-intuitively, there isn</span>&rsquo;<span>t</span>
<span>some user-supplied program that a Turing Machine executes. Rather, the program is hard-wired into</span>
<span>the machine. The transition function </span><em><span>is</span></em><span> the program.</span></p>
<p><span>But with some ingenuity we can regain our ability to write programs. Recall that we</span>&rsquo;<span>ve just learned</span>
<span>to feed arbitrary files to a TM. So what we could do is to write a text file that specifies a TM and</span>
<span>its input, and then feed that entire file as an input to an </span>&ldquo;<span>interpreter</span>&rdquo;<span> Turing Machine which would</span>
<span>read the file, and act as the machine specified there. A Turing Machine can have an </span><code>eval</code>
<span>function.</span></p>
<p><span>Is such an </span>&ldquo;<span>interpreter</span>&rdquo;<span> Turing Machine possible? Yes! And it is not hard: if you spend a couple of hours</span>
<span>programming Turing Machines by hand, you</span>&rsquo;<span>ll see that you pretty much can do anything </span>&mdash;<span> you can do</span>
<span>numbers, arithmetic, loops, control flow. It</span>&rsquo;<span>s just very very tedious.</span></p>
<p><span>So let</span>&rsquo;<span>s just declare that we</span>&rsquo;<span>ve actually coded up this Universal Turing Machine which simulates a</span>
<span>TM given to it as an input in a particular encoding.</span></p>
<p><span>This sort of construct also gives rise to the Church-Turing thesis. We have a TM which can run other</span>
<span>TMs. And you can implement a TM interpreter in something like Python. And, with a bit of legwork,</span>
<span>you could </span><em><span>also</span></em><span> implement a Python interpreter as a TM (you likely want to avoid doing that</span>
<span>directly, and instead do a simpler interpreter for WASM, and then use a Python interpreter compiled</span>
<span>to WASM). This sort of bidirectional interpretation shows that Python and TMs have equivalent</span>
<span>computing power. Moreover, it</span>&rsquo;<span>s quite hard to come up with a reasonable computational device which</span>
<span>is more powerful than a Turing Machine.</span></p>
<p><span>There are computational devices that are strictly weaker than TMs though. Recall FSMs. By this point,</span>
<span>it should be obvious that a TM can simulate an FSM. Everything a Finite State Machine can do, a</span>
<span>Turing Machine can do as well. And it should be intuitively clear that a TM is more powerful than an</span>
<span>FSM. An FSM gets to use only a finite number of states. A TM has these same states, but it also posses</span>
<span>a tape which serves like an infinitely sized external memory.</span></p>
<p><span>Directly proving that you </span><em><span>can</span>&rsquo;<span>t</span></em><span> encode a Universal Turing Machine as an FSM sounds complicated,</span>
<span>so let</span>&rsquo;<span>s prove something simpler. Recall that we have established that there</span>&rsquo;<span>s no FSM that accepts</span>
<span>only ones surrounded by an equal number of zeros on both sides (because a sufficiently large word</span>
<span>of this form would necessary enter a cycle in a state machine, which could then be further pumped).</span>
<span>But it</span>&rsquo;<span>s actually easy to write a Turing Machine that does this:</span></p>
<ul>
<li>
<span>Erase zero (at the left side of the tape)</span>
</li>
<li>
<span>Go to the right end of the tape</span>
</li>
<li>
<span>Erase zero</span>
</li>
<li>
<span>Go to the left side of the tape</span>
</li>
<li>
<span>Repeat</span>
</li>
<li>
<span>If what</span>&rsquo;<span>s left is a single </span><code>1</code><span> the answer is </span>&ldquo;<span>yes</span>&rdquo;<span>, otherwise it is a </span>&ldquo;<span>no</span>&rdquo;
</li>
</ul>
<p><span>We found a specific problem that can be solved by a TM, but is out of reach of any FSM. So it</span>
<span>necessarily follows that there isn</span>&rsquo;<span>t an FSM that can simulate an arbitrary TM.</span></p>
<p><span>It is also useful to take a closer look at the tape. It is a convenient skeuomorphic abstraction</span>
<span>which makes the behavior of the machine intuitive, but it is inconvenient to implement in a normal</span>
<span>programming language. There isn</span>&rsquo;<span>t a standard data structure that behaves just like a tape.</span></p>
<p><span>One cool practical trick is to simulate the tape as a pair of stacks. Take this:</span></p>

<figure class="code-block">


<pre><code><span class="line">Tape: A B C D E F G</span>
<span class="line">Head:     ^</span></code></pre>

</figure>
<p><span>And transform it to something like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">Left Stack:  [A, B, C]</span>
<span class="line">Right Stack: [G, F, E, D]</span></code></pre>

</figure>
<p><span>That is, everything to the left of the head is one stack, everything to the right, reversed, is the</span>
<span>other.  Here, moving the reading head left or right corresponds to popping a value off one stack and</span>
<span>pushing it onto another.</span></p>
<p><span>So, an equivalent-in-power definition would be to say that a TM is an FSM endowed with two</span>
<span>stacks.</span></p>
<p><span>This of course creates an obvious question: is an FSM with just one stack a thing? Yes! It would be</span>
<span>called a pushdown automaton, and it would correspond to context-free languages. But that</span>&rsquo;<span>s beyond</span>
<span>the scope of this post!</span></p>
<p><span>There</span>&rsquo;<span>s yet another way to look at the tape, or the pair of stacks, if the set of symbols is 0 and</span>
<span>1. You could say that a stack is just a number! So, something like</span>
<code class="display">[1, 0, 1, 1]</code>
<span>will be</span>
<span class="display"><code>1 + 2 + 8 = 11</code><span>.</span></span>
<span>Looking at the top of the stack is </span><code>stack % 2</code><span>, removing an item from the stack is </span><code>stack / 2</code><span> and</span>
<span>pushing x onto the stack is </span><code>stack * 2 + x</code><span>. We won</span>&rsquo;<span>t need this </span><em><span>right</span></em><span> now, so just hold onto this</span>
<span>for a brief moment.</span></p>
</section>
<section id="Turing-Machine-Limits">

    <h2>
    <a href="#Turing-Machine-Limits"><span>Turing Machine: Limits</span> </a>
    </h2>
<p><span>Ok, so we have some idea about the lower bound for the power of a Turing Machine </span>&mdash;<span> FSMs are strictly</span>
<span>less expressive. What about the opposite direction? Is there some computation that a Turing Machine</span>
<span>is incapable of doing?</span></p>
<p><span>Yes! Let</span>&rsquo;<span>s construct a function which maps natural numbers to natural numbers, which can</span>&rsquo;<span>t be</span>
<span>implemented by a Turing Machine. Recall that we can encode an arbitrary Turing Machine as text. That</span>
<span>means that we can actually enumerate all possible Turing Machines, and write them in a giant line,</span>
<span>from the most simple Turing Machine to more complex ones:</span></p>

<figure class="code-block">


<pre><code><span class="line">TM_0</span>
<span class="line">TM_1</span>
<span class="line">TM_2</span>
<span class="line">...</span>
<span class="line">TM_326</span>
<span class="line">...</span></code></pre>

</figure>
<p><span>This is of course going to be an infinite list.</span></p>
<p><span>Now, let</span>&rsquo;<span>s see how TM0 behaves on input </span><code>0</code><span>: it either prints something, or doesn</span>&rsquo;<span>t terminate. Then,</span>
<span>note how TM1 behaves on input </span><code>1</code><span>, and generalizing, create function </span><code>f</code><span> that behaves as the nth TM</span>
<span>on input </span><code>n</code><span>. It might look something like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(0) = 0</span>
<span class="line">f(1) = 111011</span>
<span class="line">f(2) = doesn't terminate</span>
<span class="line">f(3) = 0</span>
<span class="line">f(4) = 101</span>
<span class="line">...</span></code></pre>

</figure>
<p><span>Now, let</span>&rsquo;<span>s construct function </span><code>g</code><span> which is maximally diffed from </span><code>f</code><span>: where </span><code>f</code><span> gives </span><code>0</code><span>, </span><code>g</code><span> will</span>
<span>return </span><code>1</code><span>, and it will return </span><code>0</code><span> in all other cases:</span></p>

<figure class="code-block">


<pre><code><span class="line">g(0) = 1</span>
<span class="line">g(1) = 0</span>
<span class="line">g(2) = 0</span>
<span class="line">g(3) = 1</span>
<span class="line">g(4) = 0</span>
<span class="line">...</span></code></pre>

</figure>
<p><span>There isn</span>&rsquo;<span>t a Turing machine that computes </span><code>g</code><span>. For suppose there is. Then, it exists in our list of</span>
<span>all Turing Machines somewhere. Let</span>&rsquo;<span>s say it is TM1000064. So, if we feed </span><code>0</code><span> to it, it will return</span>
<code>g(0)</code><span>, which is </span><code>1</code><span>, which is different from </span><code>f(0)</code><span>. And the same holds for </span><code>1</code><span>, and </span><code>2</code><span>, and </span><code>3</code><span>.</span>
<span>But once we get to </span><code>g(1000064)</code><span>, we are in trouble, because, by the definition of </span><code>g</code><span>, </span><code>g(1000064)</code>
<span>is different from what is computed by TM1000064! So such a machine is impossible.</span></p>
<p><span>Those math savvy might express this more succinctly </span>&mdash;<span> there</span>&rsquo;<span>s a countably-infinite number of</span>
<span>Turing Machines, and an uncountably-infinite number of functions. So there </span><em><span>must</span></em><span> be some functions</span>
<span>which do not have a corresponding Turing Machine. It is the same proof </span>&mdash;<span> the diagonalization</span>
<span>argument is hiding in the claim that the set of all functions is an uncountable set.</span></p>
<p><span>But this is super weird and abstract. Let</span>&rsquo;<span>s rather come up with some very specific problem which</span>
<span>isn</span>&rsquo;<span>t solvable by a Turing Machine. The halting problem: given source code for a Turing Machine and</span>
<span>its input, determine if the machine halts on this input eventually.</span></p>
<p><span>As we have waved our hands sufficiently vigorously to establish that Python and Turing Machines have</span>
<span>equivalent computational power, I am going to try to solve this in Python:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">halts</span>(<span class="hl-params">program_source_code: <span class="hl-built_in">str</span>, program_input: <span class="hl-built_in">str</span></span>) -&gt; Bool:</span>
<span class="line">    <span class="hl-comment"># One million lines of readable, but somewhat</span></span>
<span class="line">    <span class="hl-comment"># unsettling and intimidating Python code.</span></span>
<span class="line">    <span class="hl-keyword">return</span> the_answer</span>
<span class="line"></span>
<span class="line">raw_input = <span class="hl-built_in">input</span>()</span>
<span class="line">[program_source_code, program_input] = parse(raw_input)</span>
<span class="line"><span class="hl-built_in">print</span>(<span class="hl-string">&quot;Yes&quot;</span> <span class="hl-keyword">if</span> halts(program_source_code, program_input) <span class="hl-keyword">else</span> <span class="hl-string">&quot;No&quot;</span>)</span></code></pre>

</figure>
<p><span>Now, I will do a weird thing and start asking whether a program terminates, if it is fed its own</span>
<span>source code, in a reverse-quine of sorts:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">halts_on_self</span>(<span class="hl-params">program_source_code: <span class="hl-built_in">str</span></span>) -&gt; Bool:</span>
<span class="line">    program_input = program_source_code</span>
<span class="line">    <span class="hl-keyword">return</span> halts(program_source_code, program_input)</span></code></pre>

</figure>
<p><span>and finally I construct this weird beast of a program:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">halts</span>(<span class="hl-params">program_source_code: <span class="hl-built_in">str</span>, program_input: <span class="hl-built_in">str</span></span>) -&gt; Bool:</span>
<span class="line">    <span class="hl-comment"># ...</span></span>
<span class="line">    <span class="hl-keyword">return</span> the_answer</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">halts_on_self</span>(<span class="hl-params">program_source_code: <span class="hl-built_in">str</span></span>) -&gt; Bool:</span>
<span class="line">    program_input = program_source_code</span>
<span class="line">    <span class="hl-keyword">return</span> halts(program_source_code, program_input)</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">weird</span>(<span class="hl-params">program_input</span>):</span>
<span class="line">    <span class="hl-keyword">if</span> halts_on_self(program_input):</span>
<span class="line">        <span class="hl-keyword">while</span> <span class="hl-literal">True</span>:</span>
<span class="line">            <span class="hl-keyword">pass</span></span>
<span class="line"></span>
<span class="line">weird(<span class="hl-built_in">input</span>())</span></code></pre>

</figure>
<p><span>To make this even worse, I</span>&rsquo;<span>ll feed the text of this </span><code>weird</code><span> program to itself. Does it terminate</span>
<span>with this input? Well, if it terminates, and if our </span><code>halts</code><span> function is implemented correctly, then</span>
<span>the </span><code>halts_on_self(program_input)</code><span> invocation above returns </span><code>True</code><span>. But then we enter the infinite</span>
<span>loop and don</span>&rsquo;<span>t actually terminate.</span></p>
<p><span>Hence, it must be the case that </span><code>weird</code><span> does not terminate when self-applied. But then</span>
<code>halts_on_self</code><span> returns </span><code>False</code><span>, and it should terminate. So we get a contradiction both ways. Which</span>
<span>necessarily means that either our </span><code>halts</code><span> sometimes returns a straight-up incorrect answer, or that it</span>
<span>sometimes does not terminate.</span></p>
<p><span>So this is the flip side of a Turing Machine</span>&rsquo;<span>s power </span>&mdash;<span> it is so powerful that it becomes impossible</span>
<span>to tell whether it</span>&rsquo;<span>ll terminate or not!</span></p>
<p><span>It actually gets much worse, because this result can be generalized to an unreasonable degree!</span>
<span>In general, there</span>&rsquo;<span>s very little we can say about arbitrary programs.</span></p>
<p><span>We can easily check syntactic properties (is the program text shorter than 4 kilobytes?), but they</span>
<span>are, in some sense, not very interesting, as they depend a lot on how exactly one writes a program.</span>
<span>It would be much more interesting to check some refactoring-invariant properties, which hold when</span>
<span>you change the text of the program, but leave the behavior intact. Indeed, </span>&ldquo;<span>does this change</span>
<span>preserve behavior?</span>&rdquo;<span> would be one very useful property to check!</span></p>
<p><span>So let</span>&rsquo;<span>s define two TMs to be equivalent, if they have identical behavior. That is, for each</span>
<span>specific input, either both machines don</span>&rsquo;<span>t terminate, or they both halt, and give identical results.</span></p>
<p><span>Then, our refactoring-invariant properties are, by definition, properties that hold (or do not hold)</span>
<span>for the entire classes of equivalence of TMs.</span></p>
<p><span>And a somewhat depressing result here is that there are no non-trivial refactoring-invariant</span>
<span>properties that you can algorithmically check.</span></p>
<p><span>Suppose we have some magic TM, called P, which checks such a property. Let</span>&rsquo;<span>s show that, using P, we can</span>
<span>solve the problem we know we can not solve </span>&mdash;<span> the halting problem.</span></p>
<p><span>Consider a Turing Machine that is just an infinite loop and never terminates, M1. P might or might</span>
<span>not hold for it. But, because P is non-trivial (it holds for some machines and doesn</span>&rsquo;<span>t hold for some</span>
<span>machines), there</span>&rsquo;<span>s some different machine M2 which differs from M1 with respect to P. That is,</span>
<code>P(M1) xor P(M2)</code><span> holds.</span></p>
<p><span>Let</span>&rsquo;<span>s use these M1 and M2 to figure out whether a given machine M halts on input I. Using Universal</span>
<span>Turing Machine (interpreter), we can construct a new machine, M12 that just runs M on input I, then</span>
<span>erases the contents of the tape and runs M2. Now, if M halts on I, then the resulting machine M12 is</span>
<span>behaviorally-equivalent to M2. If M doesn</span>&rsquo;<span>t halt on I, then the result is equivalent to the infinite</span>
<span>loop program, M1. Or, in pseudo-code:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">M1</span>(<span class="hl-params"><span class="hl-built_in">input</span></span>):</span>
<span class="line">    <span class="hl-keyword">while</span> <span class="hl-literal">True</span>:</span>
<span class="line">        <span class="hl-keyword">pass</span></span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">M2</span>(<span class="hl-params"><span class="hl-built_in">input</span></span>):</span>
<span class="line">    <span class="hl-comment"># We don&#x27;t actually know what&#x27;s here</span></span>
<span class="line">    <span class="hl-comment"># but we know that such a machine exists.</span></span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">assert</span>(P(M1) != P(M2))</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">halts</span>(<span class="hl-params">M, I</span>):</span>
<span class="line">    <span class="hl-keyword">def</span> <span class="hl-title function_">M12</span>(<span class="hl-params"><span class="hl-built_in">input</span></span>):</span>
<span class="line">        M(I) <span class="hl-comment"># might or might not halt</span></span>
<span class="line">        <span class="hl-keyword">return</span> M2(<span class="hl-built_in">input</span>)</span>
<span class="line"></span>
<span class="line">    <span class="hl-keyword">return</span> P(M12) == P(M2)</span></code></pre>

</figure>
<p><span>This is pretty bad and depressing </span>&mdash;<span> we can</span>&rsquo;<span>t learn anything meaningful about an arbitrary Turing</span>
<span>Machine! So let</span>&rsquo;<span>s finally get to the actual topic of today</span>&rsquo;<span>s post:</span></p>
</section>
<section id="Primitive-Recursive-Functions">

    <h2>
    <a href="#Primitive-Recursive-Functions"><span>Primitive Recursive Functions</span> </a>
    </h2>
<p><span>This is going to be another computational device, like FSMs and TMs. Like an FSM, it</span>&rsquo;<span>s going to be a</span>
<span>nice, always terminating, non-Turing complete device. But it will turn out to have quite a bit of</span>
<span>the power of a full Turing Machine!</span></p>
<p><span>However, unlike both TMs and FSMs, </span><dfn><span>Primitive Recursive Functions</span></dfn><span> are defined directly as</span>
<span>functions which take a tuple of natural numbers and return a natural number. The two simplest ones</span>
<span>are </span><code>zero</code><span> (that is, zero-arity function that returns </span><code>0</code><span>) and </span><code>succ</code><span> </span>&mdash;<span> a unary function that</span>
<span>just adds 1. Everything else is going to get constructed out of these two:</span></p>

<figure class="code-block">


<pre><code><span class="line">zero = 0</span>
<span class="line">succ(x) = x + 1</span></code></pre>

</figure>
<p><span>One way we are allowed to combine these functions is by composition. So we can get all the constants</span>
<span>right off the bat:</span></p>

<figure class="code-block">


<pre><code><span class="line">succ(zero) = 1</span>
<span class="line">succ(succ(zero)) = 2</span>
<span class="line">succ(succ(succ(zero))) = 3</span></code></pre>

</figure>
<p><span>We aren</span>&rsquo;<span>t going to be allowed to use general recursion (because it can trivially non-terminate),</span>
<span>but we do get to use a restricted form of C-style loop. It is a bit fiddly to define formally! The</span>
<span>overall shape is </span><span class="display"><code>LOOP(init, f, n)</code><span>.</span></span></p>
<p><span>Here, </span><code>init</code><span> and </span><code>n</code><span> are numbers </span>&mdash;<span> the initial value of the accumulator and the total number of</span>
<span>iterations. The </span><code>f</code><span> is a unary function that specifies the loop body </span>&ndash;<span> it takes the current value</span>
<span>of the accumulator and returns the new value. So</span></p>

<figure class="code-block">


<pre><code><span class="line">LOOP(init, f, 0) = init</span>
<span class="line">LOOP(init, f, 1) = f(init)</span>
<span class="line">LOOP(init, f, 2) = f(f(init))</span>
<span class="line">LOOP(init, f, 3) = f(f(f(init)))</span></code></pre>

</figure>
<p><span>While this is </span><em><span>similar</span></em><span> to a C-style loop, the crucial difference here is that the total number of</span>
<span>iterations </span><code>n</code><span> is fixed up-front. There</span>&rsquo;<span>s no way to mutate the loop counter in the loop body.</span></p>
<p><span>This allows us to define addition:</span></p>

<figure class="code-block">


<pre><code><span class="line">add(x, y) = LOOP(x, succ, y)</span></code></pre>

</figure>
<p><span>Multiplication is trickier. Conceptually, to multiply </span><code>x</code><span> and </span><code>y</code><span>, we want to </span><code>LOOP</code><span> from zero, and</span>
<span>repeat </span>&ldquo;<span>add </span><code>x</code>&rdquo;<span> </span><code>y</code><span> times. The problem here is that we can</span>&rsquo;<span>t write an </span>&ldquo;<span>add </span><code>x</code>&rdquo;<span> function yet</span></p>

<figure class="code-block">


<pre><code><span class="line"># Doesn't work, add is a binary function!</span>
<span class="line">mul(x, y) = LOOP(0, add, y)</span></code></pre>

</figure>

<figure class="code-block">


<pre><code><span class="line"># Doesn't work either, no x in scope!</span>
<span class="line">add_x v = add(x, v)</span>
<span class="line">mul(x, y) = LOOP(0, add_x, y)</span></code></pre>

</figure>
<p><span>One way around this is to define </span><code>LOOP</code><span> as a family of operators, which can pass extra arguments to</span>
<span>the iteration function:</span></p>

<figure class="code-block">


<pre><code><span class="line">LOOP0(init, f, 2) = f(f(init))</span>
<span class="line">LOOP1(c1, init, f, 2) = f(c1, f(c1, init))</span>
<span class="line">LOOP2(c1, c2, init, f, 2) = f(c1, c2, f(c1, c2, init))</span></code></pre>

</figure>
<p><span>That is, </span><code>LOOP_N</code><span> takes an extra </span><code>n</code><span> arguments, and passes them through to any invocation of the body</span>
<span>function. To express this idea a little bit more succinctly, let</span>&rsquo;<span>s just allow to partially  apply</span>
<span>the second argument of </span><code>LOOP</code><span>. That is:</span></p>
<ul>
<li>
<span>All our functions are going to be first order. All arguments are numbers, the result is a number.</span>
<span>There aren</span>&rsquo;<span>t higher order functions, there aren</span>&rsquo;<span>t closures.</span>
</li>
<li>
<span>The </span><code>LOOP</code><span> is not a function in our language </span>&mdash;<span> it</span>&rsquo;<span>s a builtin operator, a keyword. So, for</span>
<span>convenience, we allow passing partially applied functions to it. But semantically this is</span>
<span>equivalent to just passing in extra arguments on each iteration.</span>
</li>
</ul>
<p><span>Which finally allows us to write</span></p>

<figure class="code-block">


<pre><code><span class="line">mul(x, y) = LOOP(0, add x, y)</span></code></pre>

</figure>
<p><span>Ok, so that</span>&rsquo;<span>s progress </span>&mdash;<span> we made something as complicated as multiplication, and we still are in</span>
<span>the guaranteed-to-terminate land. Because each loop has a fixed number of iterations, everything</span>
<span>eventually finishes.</span></p>
<p><span>We can go on and define x</span><sup><span>y</span></sup><span>:</span></p>

<figure class="code-block">


<pre><code><span class="line">pow(x, y) = LOOP(1, mul x, y)</span></code></pre>

</figure>
<p><span>And this in turn allows us to define a couple of concerning fast growing functions:</span></p>

<figure class="code-block">


<pre><code><span class="line">pow_2(n) = pow(2, n)</span>
<span class="line">pow_2_2(n) = pow_2(pow_2(n))</span></code></pre>

</figure>
<p><span>That</span>&rsquo;<span>s fun, but to do some programming, we</span>&rsquo;<span>ll need an </span><code>if</code><span>. We</span>&rsquo;<span>ll get to it, but first we</span>&rsquo;<span>ll need</span>
<span>some boolean operations. We can encode </span><code>false</code><span> as </span><code>0</code><span> and </span><code>true</code><span> as </span><code>1</code><span>. Then</span></p>

<figure class="code-block">


<pre><code><span class="line">and(x, y) = mul(x, y)</span></code></pre>

</figure>
<p><span>But </span><code>or</code><span> creates a problem: we</span>&rsquo;<span>ll need a subtraction.</span></p>

<figure class="code-block">


<pre><code><span class="line">or(x, y) = sub(</span>
<span class="line">  add(x, y),</span>
<span class="line">  mul(x, y),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>Defining </span><code>sub</code><span> is tricky, due to two problems:</span></p>
<p><span>First, we only have natural numbers, no negatives. This one is easy to solve </span>&mdash;<span> we</span>&rsquo;<span>ll just define</span>
<span>subtraction to saturate.</span></p>
<p><span>The second problem is more severe </span>&mdash;<span> I think we actually can</span>&rsquo;<span>t express subtraction given the set of</span>
<span>allowable operations so far. That is because all our operations are monotonic </span>&mdash;<span> the result is</span>
<span>never less than the arguments. One way to solve this problem is to define the LOOP in such a way</span>
<span>that the body function also gets passed a second argument </span>&mdash;<span> the current iteration. So, if you</span>
<span>iterate up to </span><code>n</code><span>, the last iteration will observe </span><code>n - 1</code><span>, and that would be the non-monotonic</span>
<span>operation that creates subtraction. But that seems somewhat inelegant to me, so instead I will just</span>
<span>add a </span><code>pred</code><span> function to the basis, and use that to add loop counters to our iterations.</span></p>

<figure class="code-block">


<pre><code><span class="line">pred(0) = 0 # saturate</span>
<span class="line">pred(1) = 0</span>
<span class="line">pred(2) = 1</span>
<span class="line">...</span></code></pre>

</figure>
<p><span>Now we can say:</span></p>

<figure class="code-block">


<pre><code><span class="line">sub(x, y) = LOOP(x, pred, y)</span>
<span class="line"></span>
<span class="line">and(x, y) = mul(x, y)</span>
<span class="line">or(x, y) = sub(</span>
<span class="line">  add(x, y),</span>
<span class="line">  mul(x, y)</span>
<span class="line">)</span>
<span class="line">not(x) = sub(1, x)</span>
<span class="line"></span>
<span class="line">if(cond, a, b) = add(</span>
<span class="line">  mul(a, cond),</span>
<span class="line">  mul(b, not(cond)),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>And now we can do a bunch of comparison operators:</span></p>

<figure class="code-block">


<pre><code><span class="line">is_zero(x) = sub(1, x)</span>
<span class="line"></span>
<span class="line"># x &gt;= y</span>
<span class="line">ge(x, y) = is_zero(sub(y, x))</span>
<span class="line"></span>
<span class="line"># x == y</span>
<span class="line">eq(x, y) = and(ge(x, y), ge(y, x))</span>
<span class="line"></span>
<span class="line"># x &gt; y</span>
<span class="line">gt(x, y) = and(ge(x, y), not(eq(x, y)))</span>
<span class="line"></span>
<span class="line"># x &lt; y</span>
<span class="line">lt(x, y) = gt(y, x)</span></code></pre>

</figure>
<p><span>With that we could implement modulus. To compute </span><code>x % m</code><span> we will start with </span><code>x</code><span>, and will be</span>
<span>subtracting </span><code>m</code><span> until we get a number smaller than </span><code>m</code><span>. We</span>&rsquo;<span>ll need at most </span><code>x</code><span> iterations for that.</span></p>
<p><span>In pseudo-code:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">mod</span>(<span class="hl-params">x, m</span>):</span>
<span class="line">  current = x</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">for</span> _ <span class="hl-keyword">in</span> <span class="hl-number">0.</span>.x:</span>
<span class="line">    <span class="hl-keyword">if</span> current &lt; m:</span>
<span class="line">      current = current</span>
<span class="line">    <span class="hl-keyword">else</span>:</span>
<span class="line">      current = current - m</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">return</span> current</span></code></pre>

</figure>
<p><span>And as a bona fide PRF:</span></p>

<figure class="code-block">


<pre><code><span class="line">mod_iter(m, x) = if(</span>
<span class="line">  lt(x, m),</span>
<span class="line">  x,        # then</span>
<span class="line">  sub(x, m) # else</span>
<span class="line">)</span>
<span class="line">mod(x, m) = LOOP(x, mod_iter m, x)</span></code></pre>

</figure>
<p><span>That</span>&rsquo;<span>s a curious structure </span>&mdash;<span> rather than computing the modulo directly, we essentially search for</span>
<span>it using trial and error, and relying on the fact that the search has a clear upper bound.</span></p>
<p><span>Division can be done similarly: to divide x by y, start with 0, and then repeatedly add one to the</span>
<span>accumulator until the product of the accumulator and y exceeds x:</span></p>

<figure class="code-block">


<pre><code><span class="line">div_iter x y acc = if(</span>
<span class="line">  le(mul(succ(acc), y), y),</span>
<span class="line">  succ(acc), # then</span>
<span class="line">  acc        # else</span>
<span class="line">)</span>
<span class="line">div(x, y) = LOOP(0, div_iter x y, x)</span></code></pre>

</figure>
<p><span>This really starts to look like programming! One thing we are currently missing are data structures.</span>
<span>While our functions take multiple arguments, they only return one number. But it</span>&rsquo;<span>s easy enough to</span>
<span>pack two numbers into one: to represent an </span><code>(a, b)</code><span> pair, we</span>&rsquo;<span>ll use 2</span><sup><span>a</span></sup><span> 3</span><sup><span>b</span></sup><span> number:</span></p>

<figure class="code-block">


<pre><code><span class="line">mk_pair(a, b) = mul(pow(2, a), pow(3, b))</span></code></pre>

</figure>
<p><span>To deconstruct such a pair into its first and second components, we need to find the maximum power</span>
<span>of 2 or 3 that divides our number. Which is exactly the same shape we used to implement </span><code>div</code><span>:</span></p>

<figure class="code-block">


<pre><code><span class="line">max_factor_iter p m acc = if(</span>
<span class="line">  is_zero(mod(p, pow(m, succ(acc)))),</span>
<span class="line">  succ(acc), # then</span>
<span class="line">  acc,       # else</span>
<span class="line">)</span>
<span class="line">max_factor(p, m) = LOOP(0, max_factor_iter p m, p)</span>
<span class="line"></span>
<span class="line">fst(p) = max_factor(p, 2)</span>
<span class="line">snd(p) = max_factor(p, 3)</span></code></pre>

</figure>
<p><span>Here again we use the fact that the maximal power of two that divides </span><code>p</code><span> is not larger than </span><code>p</code>
<span>itself, so we can over-estimate the number of iterations we</span>&rsquo;<span>ll need as </span><code>p</code><span>.</span></p>
<p><span>Using this pair construction, we can finally add a loop counter to our </span><code>LOOP</code><span> construct. To track</span>
<span>the counter, we pack it as a pair with the accumulator:</span></p>

<figure class="code-block">


<pre><code><span class="line">LOOP(mk_pair(init, 0), f, n)</span></code></pre>

</figure>
<p><span>And then inside f, we first unpack that pair into accumulator and counter, pass them to actual loop</span>
<span>iteration, and then pack the result again, incrementing the counter:</span></p>

<figure class="code-block">


<pre><code><span class="line">f acc = mk_pair(</span>
<span class="line">  g(fst(acc), snd(acc)),</span>
<span class="line">  succ(snd(acc)),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>Ok, so we have achieved something remarkable: while we are writing terminating-by-construction</span>
<span>programs, which are definitely not Turing complete, we have constructed basic programming staples,</span>
<span>like boolean logic and data structures, and we have also built some rather complicated mathematical</span>
<span>functions, like </span><span class="display"><span>2</span><sup><span>2</span><sup><span>N</span></sup></sup><span>.</span></span></p>
<p><span>We could try to further enrich our little primitive recursive kingdom by adding more and more</span>
<span>functions on an ad hoc basis, but let</span>&rsquo;<span>s try to be really ambitious and go for the main prize </span>&mdash;
<span>simulating Turing Machines.</span></p>
<p><span>We know that we will fail: Turing machines can enter an infinite loop, but PRFs necessarily terminate.</span>
<span>That means, that, if a PRF were able to simulate an arbitrary TM, it would have to say after a certain</span>
<span>finite amount of steps that </span>&ldquo;<span>this TM doesn</span>&rsquo;<span>t terminate</span>&rdquo;<span>.  And, while we didn</span>&rsquo;<span>t do this, it</span>&rsquo;<span>s easy to</span>
<span>see that you </span><em><span>could</span></em><span> simulate the other way around and implement PRFs in a TM. But that would give</span>
<span>us a TM algorithm to decide if an arbitrary TM halts, which we know doesn</span>&rsquo;<span>t exist.</span></p>
<p><span>So, this is hopeless! But we might still be able to learn something from failing.</span></p>
<p><span>Ok! So let</span>&rsquo;<span>s start with a configuration of a TM which we somehow need to encode into a single</span>
<span>number. First, we need the state variable proper (Q0, Q1, etc), which seems easy enough to represent</span>
<span>with a number. Then, we need a tape and a position of the reading head. Recall how we used a pair of</span>
<span>stacks to represent exactly the tape and the position. And recall that we can look at a stack of</span>
<span>zeros and ones as a number in binary form, where push and pop operations are implemented using </span><code>%</code><span>,</span>
<code>*</code><span>, and </span><code>/</code><span> </span>&mdash;<span> exactly the operations we already can do. So, our configuration is just three</span>
<span>numbers: </span><span class="display"><code>(S, stack1, stack2)</code><span>.</span></span></p>
<p><span>And, using the 2</span><sup><span>a</span></sup><span>3</span><sup><span>b</span></sup><span>5</span><sup><span>c</span></sup><span> trick, we can pack this triple into just a single number. But that means we</span>
<span>could directly encode a single step of a Turing Machine:</span></p>

<figure class="code-block">


<pre><code><span class="line">single_step(config) = if(</span>
<span class="line">  # if the state is Q0 ...</span>
<span class="line">  eq(fst(config), 0)</span>
<span class="line"></span>
<span class="line">  # and the symbol at the top of left stack is 0</span>
<span class="line">  if(is_zero(mod(snd(config), 2))</span>
<span class="line">    mk_triple(</span>
<span class="line">      1,                    # move to state Q1</span>
<span class="line">      div(snd(config), 2),  # pop value from the left stack</span>
<span class="line">      mul(trd(config), 2),  # push zero onto the right stack</span>
<span class="line">    ),</span>
<span class="line">    ... # Handle symbol 1 in state Q1</span>
<span class="line">  )</span>
<span class="line">  # if the state is Q1 ...</span>
<span class="line">  if(eq(fst(config), 1)</span>
<span class="line">    ...</span>
<span class="line">  )</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>And now we could plug that into our </span><code>LOOP</code><span> to simulate a Turing Machine running for N steps:</span></p>

<figure class="code-block">


<pre><code><span class="line">n_steps initial_config n =</span>
<span class="line">  LOOP(initial_config, single_step, n)</span></code></pre>

</figure>
<p><span>The catch of course is that we can</span>&rsquo;<span>t know the </span><code>N</code><span> that</span>&rsquo;<span>s going to be enough. But we can have a very</span>
<span>good guess! We could do something like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">hopefully_enough_steps initial_config =</span>
<span class="line">  LOOP(initial_config, single_step, pow_2_2(initial_config))</span></code></pre>

</figure>
<p><span>That is, run for some large tower of exponents of the initial state. Which would be plenty for</span>
<span>normal algorithms, which are usually 2</span><sup><span>N</span></sup><span> at worst!</span></p>
<p><span>Or, generalizing:</span></p>

<figure class="blockquote">
<blockquote><p><span>If a TM has a runtime which is bounded by some primitive-recursive function, then the entire</span>
<span>TM can be replaced with a PRF. Be advised that PRFs can grow </span><em><span>really</span></em><span> fast.</span></p>
</blockquote>

</figure>
<p><span>Which is the headline result we have set out to prove!</span></p>
</section>
<section id="Primitive-Recursive-Functions-Limit">

    <h2>
    <a href="#Primitive-Recursive-Functions-Limit"><span>Primitive Recursive Functions: Limit</span> </a>
    </h2>
<p><span>It might seem that non-termination is the only principle obstacle. That anything that terminates at</span>
<span>all has to be implementable as a PRF. Alas, that</span>&rsquo;<span>s not so. Let</span>&rsquo;<span>s go and construct a function that is</span>
<span>surmountable by a TM, but is out of reach of PRFs.</span></p>
<p><span>We will combine the ideas of the impossibility proofs for FSMs (noting that if a function is</span>
<span>computed by some machine, that machine has a specific finite size) and TMs (diagonalization).</span></p>
<p><span>So, suppose we have some function </span><code>f</code><span> that can</span>&rsquo;<span>t be computed by a PRF. How would we go about proving</span>
<span>that? Well, we</span>&rsquo;<span>d start with </span>&ldquo;<span>suppose that we have a PRF P that computes </span><code>f</code>&rdquo;<span>. And then we could</span>
<span>notice that P would have some finite size. If you look at it abstractly, the P is its syntax tree,</span>
<span>with lots of </span><code>LOOP</code><span> constructs, but it always boils down to some </span><code>succ</code><span>s and </span><code>zero</code><span>s at the leaves.</span>
<span>Let</span>&rsquo;<span>s say that the depth of P is </span><code>d</code><span>.</span></p>
<p><span>And, actually, if you look at it, there are only a finite number of PRFs with depth at most </span><code>d</code><span>. Some</span>
<span>of them describe pretty fast growing functions. But probably there</span>&rsquo;<span>s a limit to how fast a function</span>
<span>can grow, given that it is computed by a PRF of size </span><code>d</code><span>. Or, to use a concrete example: we have</span>
<span>constructed a PRF of depth 5 that computes two to the power of two to the power of N. Probably if we</span>
<span>were smarter, we could have squeezed a couple more levels into that tower of exponents. But</span>
<span>intuitively it seems that if you build a tower of, say, 10 exponents, that would grow faster than</span>
<em><span>any</span></em><span> PRF of depth </span><code>5</code><span>. And that this generalizes </span>&mdash;<span> for any fixed depth, there</span>&rsquo;<span>s a high-enough</span>
<span>tower of exponents that grows faster than any PRF with that depth.</span></p>
<p><span>So we could conceivably build an </span><code>f</code><span> that defeats our </span><code>d</code><span>-deep P. But that</span>&rsquo;<span>s not quite a victory</span>
<span>yet: maybe that </span><code>f</code><span> is feasible for </span><code>d+2</code><span>-deep PRFs! So here we</span>&rsquo;<span>ll additionally apply</span>
<span>diagonalization: for each depth, we</span>&rsquo;<span>ll build it</span>&rsquo;<span>s own depth-specific nemesis </span><code>f_d</code><span>. And then we</span>&rsquo;<span>ll</span>
<span>define our overall function as</span></p>

<figure class="code-block">


<pre><code><span class="line">a(n) = f_n(n)</span></code></pre>

</figure>
<p><span>So, for </span><code>n</code><span> large enough it</span>&rsquo;<span>ll grow faster than a PRF with any fixed depth.</span></p>
<p><span>So that</span>&rsquo;<span>s the general plan, the rest of the own is basically just calculating the upper bound on the</span>
<span>growth of a PRF of depth </span><code>d</code><span>.</span></p>
<p><span>One technical difficulty here is that PRFs tend to have different arities:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y)</span>
<span class="line">g(x, y, z, t)</span>
<span class="line">h(x)</span></code></pre>

</figure>
<p><span>Ideally, we</span>&rsquo;<span>d use just one upper bound of them all. So we</span>&rsquo;<span>ll be looking for an upper bound of the</span>
<span>following form:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y, z, t) &lt;= A_d(max(x, y, z, t))</span></code></pre>

</figure>
<p><span>That is:</span></p>
<ul>
<li>
<span>Compute the depth of </span><code>f</code><span>, </span><code>d</code><span>.</span>
</li>
<li>
<span>Compute the largest of its arguments.</span>
</li>
<li>
<span>And plug that into unary function for depth </span><code>d</code><span>.</span>
</li>
</ul>
<p><span>Let</span>&rsquo;<span>s start with </span><code>d=1</code><span>. We have only primitive functions on this level, </span><code>succ</code><span>, </span><code>zero</code><span>, and </span><code>pred</code><span>,</span>
<span>so we could say that</span></p>

<figure class="code-block">


<pre><code><span class="line">A_1(x) = x + 1</span></code></pre>

</figure>
<p><span>Now, let</span>&rsquo;<span>s handle an arbitrary other depth </span><code>d + 1</code><span>. In that case, our function is non-primitive, so at</span>
<span>the root of the syntax tree we have either a composition or a </span><code>LOOP</code><span>.</span></p>
<p><span>Composition would look like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y, z, ...) = g(</span>
<span class="line">  h1(x, y, z, ...),</span>
<span class="line">  h2(x, y, z, ...),</span>
<span class="line">  h3(x, y, z, ...),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>where </span><code>g</code><span> and </span><code>h_n</code><span> are </span><code>d</code><span> deep and the resulting </span><code>f</code><span> is </span><code>d+1</code><span> deep. We can immediately estimate</span>
<span>the </span><code>h_n</code><span> then:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(args...) &lt;= g(</span>
<span class="line">  A_d(maxarg),</span>
<span class="line">  A_d(maxarg),</span>
<span class="line">  A_d(maxarg),</span>
<span class="line">  ...</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>In this somewhat loose notation, </span><code>args...</code><span> stands for a tuple of arguments, and </span><code>maxarg</code><span> stands for</span>
<span>the largest one.</span></p>
<p><span>And then we could use the same estimate for </span><code>g</code><span>:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(args...) &lt;= A_d(A_d(maxarg))</span></code></pre>

</figure>
<p><span>This is super high-order, so let</span>&rsquo;<span>s do a concrete example for a depth-2 two-argument function which</span>
<span>starts with a composition:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y) &lt;= A_1(A_1(max(x, y)))</span>
<span class="line">         = A_1(max(x, y) + 1)</span>
<span class="line">         = max(x, y) + 2</span></code></pre>

</figure>
<p><span>This sounds legit: if we don</span>&rsquo;<span>t use LOOP, then </span><code>f(x, y)</code><span> is either </span><code>succ(succ(x))</code><span> or </span><code>succ(succ(y))</code>
<span>so </span><code>max(x, y) + 2</code><span> indeed is the bound!</span></p>
<p><span>Ok, now the fun case! If the top-level node is a </span><code>LOOP</code><span>, then we have</span></p>

<figure class="code-block">


<pre><code><span class="line">f(args...) = LOOP(</span>
<span class="line">  g(args...),</span>
<span class="line">  h(args...),</span>
<span class="line">  t(args...),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>This sounds complicated to estimate, especially due to that last </span><code>t(args...)</code><span> argument, which is the</span>
<span>number of iterations. So we</span>&rsquo;<span>ll be cowards and </span><em><span>won</span>&rsquo;<span>t</span></em><span> actually try to estimate this case. Instead,</span>
<span>we will require that our PRF is written in a simplified form, where the first and the last arguments</span>
<span>to </span><code>LOOP</code><span> are simple.</span></p>
<p><span>So, if your PRF looks like</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y) = LOOP(x + y, mul, pow2(x))</span></code></pre>

</figure>
<p><span>you are required to re-write it first as</span></p>

<figure class="code-block">


<pre><code><span class="line">helper(u, v) = LOOP(u, mul, v)</span>
<span class="line">f(x, y) = helper(x + y, pow2(x))</span></code></pre>

</figure>
<p><span>So now we only have to deal with this:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(args...) = LOOP(</span>
<span class="line">  arg,</span>
<span class="line">  g(args...),</span>
<span class="line">  arg,</span>
<span class="line">)</span></code></pre>

</figure>
<p><code>f</code><span> has depth </span><code>d+1</code><span>, </span><code>g</code><span> has depth </span><code>d</code><span>.</span></p>
<p><span>On the first iteration, we</span>&rsquo;<span>ll call </span><code>g(args..., arg)</code><span>, which we can estimate as </span><code>A_d(maxarg)</code><span>. That</span>
<span>is, </span><code>g</code><span> does get an </span><em><span>extra</span></em><span> argument, but it is one of the original arguments of </span><code>f</code><span>, and we are</span>
<span>looking at the maximum argument anyway, so it doesn</span>&rsquo;<span>t matter.</span></p>
<p><span>On the second iteration, we are going to call</span>
<code class="display">g(args..., prev_iteration)</code>
<span>which we can estimate as</span>
<span class="display"><code>A_d(max(maxarg, prev_iteration))</code><span>.</span></span></p>
<p><span>Now we plug our estimation for the first iteration:</span></p>

<figure class="code-block">


<pre><code><span class="line">g(args..., prev_iteration)</span>
<span class="line">  &lt;= A_d(max(maxarg, prev_iteration))</span>
<span class="line">  &lt;= A_d(max(maxarg, A_d(maxarg)))</span>
<span class="line">  =  A_d(A_d(maxarg))</span></code></pre>

</figure>
<p><span>That is, the estimate for the first iteration is </span><code>A_d(maxarg)</code><span>. The estimation for the second</span>
<span>iteration adds one more layer: </span><code>A_d(A_d(maxarg))</code><span>. For the third iteration we</span>&rsquo;<span>ll get</span>
<span class="display"><code>A_d(A_d(A_d(maxarg)))</code><span>.</span></span></p>
<p><span>So the overall thing is going to be smaller than </span><code>A_d</code><span> iteratively applied to itself some number of</span>
<span>times, where </span>&ldquo;<span>some number</span>&rdquo;<span> is one of the </span><code>f</code><span> original arguments. But no harm</span>&rsquo;<span>s done if we iterate up</span>
<span>to </span><code>maxarg</code><span>.</span></p>
<p><span>As a sanity check, the worst depth-2 function constructed with iteration is probably</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y) = LOOP(x, succ, y)</span></code></pre>

</figure>
<p><span>which is </span><code>x + y</code><span>. And our estimate gives </span><code>x + 1</code><span> applied </span><code>maxarg</code><span> times to </span><code>maxarg</code><span>, which is </span><code>2 *
maxarg</code><span>, which is indeed the correct upper bound!</span></p>
<p><span>Combining everything together, we have:</span></p>

<figure class="code-block">


<pre><code><span class="line">A_1(x) = x + 1</span>
<span class="line"></span>
<span class="line">f(args...) &lt;= max(</span>
<span class="line">  A_d(A_d(maxarg)),               # composition case</span>
<span class="line">  A_d(A_d(A_d(... A_d(maxarg)))), # LOOP case,</span>
<span class="line">   &lt;-    maxarg A's         -&gt;</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>That </span><code>max</code><span> there is significant </span>&mdash;<span> although it seems like the second line, with </span><code>maxarg</code>
<span>applications, is always going to be longer, </span><code>maxarg</code><span>, in fact, could be as small as zero. But we</span>
<span>can take </span><code>maxarg + 2</code><span> repetitions to fix this:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(args...) &lt;=</span>
<span class="line">  A_d(A_d(A_d(... A_d(maxarg)))),</span>
<span class="line">  &lt;-    maxarg + 2 A's         -&gt;</span></code></pre>

</figure>
<p><span>So let</span>&rsquo;<span>s just define </span><code>A_{d+1}(x)</code><span> to make that inequality work:</span></p>

<figure class="code-block">


<pre><code><span class="line">A_{d+1}(x) = A_d(A_d( .... A_d(x)))</span>
<span class="line">            &lt;- x + 2 A_d's in total-&gt;</span></code></pre>

</figure>
<p><span>Unpacking:</span></p>
<p><span>We define a family of unary functions </span><code>A_d</code><span>, such that each </span><code>A_d</code><span> </span>&ldquo;<span>grows faster</span>&rdquo;<span> than any n-ary PRF</span>
<span>of depth </span><code>d</code><span>. If </span><code>f</code><span> is a ternary PRF of depth 3, then </span><span class="display"><code>f(1, 92, 10) &lt;= A_3(92)</code><span>.</span></span></p>
<p><span>To evaluate </span><code>A_d</code><span> at point </span><code>x</code><span>, we use the following recursive procedure:</span></p>
<ul>
<li>
<span>If </span><code>d</code><span> is </span><code>1</code><span>, return </span><code>x + 1</code><span>.</span>
</li>
<li>
<span>Otherwise, evaluate </span><code>A_{d-1}</code><span> at point </span><code>x</code><span> to get, say, </span><code>v</code><span>. Then evaluate </span><code>A_{d-1}</code><span> again at</span>
<span>point </span><code>v</code><span> this time, yielding </span><code>u</code><span>. Then compute </span><code>A_{d-1}(u)</code><span>. Overall, repeat this process </span><code>x+2</code>
<span>times, and return the final number.</span>
</li>
</ul>
<p><span>We can simplify this a bit if we stop treating </span><code>d</code><span> as a kind of function </span><em><span>index</span></em><span>, and instead say</span>
<span>that our </span><code>A</code><span> is just a function of two arguments. Then we have the following equations:</span></p>

<figure class="code-block">


<pre><code><span class="line">A(1, x) = x + 1</span>
<span class="line">A(d + 1, x) = A(d, A(d, A(d, ..., A(d, x))))</span>
<span class="line">                &lt;- x + 2 A_d's in total-&gt;</span></code></pre>

</figure>
<p><span>The last equation can re-formatted as</span></p>

<figure class="code-block">


<pre><code><span class="line">A(</span>
<span class="line">  d,</span>
<span class="line">  A(d, A(d, ..., A(d, x))),</span>
<span class="line">  &lt;- x + 1 A_d's in total-&gt;</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>And for non-zero x that is just</span></p>

<figure class="code-block">


<pre><code><span class="line">A(</span>
<span class="line">  d,</span>
<span class="line">  A(d + 1, x - 1),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>So we get the following recursive definition for A(d, x):</span></p>

<figure class="code-block">


<pre><code><span class="line">A(1, x) = x + 1</span>
<span class="line">A(d + 1, 0) = A(d, A(d, 0))</span>
<span class="line">A(d + 1, x) = A(d, A(d + 1, x - 1))</span></code></pre>

</figure>
<p><span>As a Python program:</span></p>

<figure class="code-block">


<pre><code><span class="line">def A(d, x):</span>
<span class="line">  if d == 1: return x + 1</span>
<span class="line">  if x == 0: return A(d-1, A(d-1, 0))</span>
<span class="line">  return A(d-1, A(d, x - 1))</span></code></pre>

</figure>
<p><span>It</span>&rsquo;<span>s easy to see that computing </span><code>A</code><span> on a Turing Machine using this definition terminates </span>&mdash;<span> this</span>
<span>is a function with two arguments, and every recursive call uses a lexicographically smaller pair of</span>
<span>arguments. And we constructed A in such a way that </span><code>A(d, x)</code><span> as a function of </span><code>x</code><span> is larger than any</span>
<span>PRF with a single argument of depth d. But that means that the following function with one argument</span>
<code class="display">a(x) = A(x, x) </code></p>
<p><span>grows faster than </span><em><span>any</span></em><span> PRF. And that</span>&rsquo;<span>s an example of a function which a Turing Machine has no</span>
<span>trouble computing (given sufficient time), but which is beyond the capabilities of PRFs.</span></p>
</section>
<section id="Part-III-Descent-From-the-Ivory-Tower">

    <h2>
    <a href="#Part-III-Descent-From-the-Ivory-Tower"><span>Part III, Descent From the Ivory Tower</span> </a>
    </h2>
<p><span>Remember, this is a three-part post! And are finally at the part 3! So let</span>&rsquo;<span>s circle back to the</span>
<span>practical matters. We have learned that:</span></p>
<ul>
<li>
<span>Turing machines don</span>&rsquo;<span>t necessarily terminate.</span>
</li>
<li>
<span>While other computational devices, like FSMs and PRFs, can be made to always terminate, there</span>&rsquo;<span>s no</span>
<span>guarantee that they</span>&rsquo;<span>ll terminate fast. PRFs in particular can compute quite large functions!</span>
</li>
<li>
<span>And non-Turing complete devices can be quite expressive. For example, any real-world algorithm</span>
<span>that works on a TM can be adapted to run as a PRF.</span>
</li>
<li>
<span>Moreover, you don</span>&rsquo;<span>t even have to contort the algorithm much to make it fit. There</span>&rsquo;<span>s a universal</span>
<span>recipe for how to take something Turing complete and make it a primitive recursive function</span>
<span>instead </span>&mdash;<span> just add an iteration counter to the device, and forcibly halt it if the counter grows</span>
<span>too large.</span>
</li>
</ul>
<p><span>Or, more succinctly: there</span>&rsquo;<span>s no practical difference between a program that doesn</span>&rsquo;<span>t terminate, and</span>
<span>the one that terminates after a billion years. As a practitioner, if you think you need to solve the</span>
<span>first problem, you need to solve the second problem as well. And making your programming language</span>
<span>non-Turing complete doesn</span>&rsquo;<span>t really help with this.</span></p>
<p><span>And yet, there are a lot of configuration languages out there that use non-Turing completeness as</span>
<span>one of their key design goals. Why is that?</span></p>
<p><span>I would say that we are never interested in Turing-completeness per-se. We usually want some </span><em><span>much</span></em>
<span>stronger properties. And yet there</span>&rsquo;<span>s no convenient catchy name for that bag of features of a good</span>
<span>configuration language. So, </span>&ldquo;<span>non-Turing-complete</span>&rdquo;<span> gets used as a sort of rallying cry to signal that</span>
<span>something is a good configuration language, and maybe sometimes even to justify to others inventing</span>
<span>a new language instead of taking something like Lua. That is, the </span><em><span>real</span></em><span> reason why you want at</span>
<span>least a different implementation is all those properties you really need, but they are kinda hard to</span>
<span>explain, or at least much harder than </span>&ldquo;<span>we can</span>&rsquo;<span>t use Python/Lua/JavaScript because they are</span>
<span>Turing-complete</span>&rdquo;<span>.</span></p>
<p><span>So what </span><em><span>are</span></em><span> the properties of a good configuration language?</span></p>
<p><em><span>First</span></em><span>, we need the language to be deterministic. If you launch Python and type </span><code>id([])</code><span>, you</span>&rsquo;<span>ll</span>
<span>see some number. If you hit </span><code>^C</code><span>, and than do this again, you</span>&rsquo;<span>ll see a different number. This is OK</span>
<span>for </span>&ldquo;<span>normal</span>&rdquo;<span> programming, but is usually anathema for configuration. Configuration is often used as a</span>
<span>key in some incremental, caching system, and letting in non-determinism there wreaks absolute chaos!</span></p>
<p><em><span>Second</span></em><span>, you need the language to be well-defined. You can compile Python with ASLR disabled, and</span>
<span>use some specific allocator, such that </span><code>id([])</code><span> always returns the same result. But that result</span>
<span>would be hard to predict! And if someone tries to do an alternative implementation, even if they</span>
<span>disable ASLR as well, they are likely to get a different deterministic number! Or the same could</span>
<span>happen if you just update the version of Python. So, the semantics of the language should be clearly</span>
<span>pinned-down by some sort of a reference, such that it is possible to guarantee not only</span>
<span>deterministic behavior, but fully identical behavior across different implementations.</span></p>
<p><em><span>Third</span></em><span>, you need the language to be pure. If your configuration can access environment variables or</span>
<span>read files on disk, than the meaning of the configuration would depend on the environment where the</span>
<span>configuration is evaluated, and you again don</span>&rsquo;<span>t want that, to make caching work.</span></p>
<p><em><span>Fourth</span></em><span>, a thing that is closely related to purity is security and sandboxing. The </span><em><span>mechanism</span></em><span> to</span>
<span>achieve both purity and security is the same </span>&mdash;<span> you don</span>&rsquo;<span>t expose general IO to your language. But</span>
<span>the purpose is different: purity is about not letting the results be non-deterministic, while</span>
<span>security is about not exposing access tokens to the attacker.</span></p>
<p><span>And now this gets tricky. One particular possible attack is a denial of service </span>&mdash;<span> sending some bad</span>
<span>config which makes our system just spin there burning the CPU. Even if you control all IO, you</span>
<span>are generally still open to these kinds of attacks. It might be OK to say this is outside of the</span>
<span>threat model </span>&mdash;<span> that no one would find it valuable enough to just burn your CPU, if they can</span>&rsquo;<span>t also</span>
<span>do IO, and that, even in the event that this happens, there</span>&rsquo;<span>s going to be some easy mitigation in the</span>
<span>form of a higher-level timeout.</span></p>
<p><span>But you also might choose to provide some sort of guarantees about execution time, and that</span>&rsquo;<span>s really</span>
<span>hard. Two approaches work. One is to make sure that processing is </span><em><span>obviously linear</span></em><span>. Not just</span>
<span>terminates, but is actually proportional to the size of inputs, and in a very direct way. If the</span>
<span>correspondence is not direct, than it</span>&rsquo;<span>s highly likely that it is in fact non linear. The second</span>
<span>approach is to ensure </span><em><span>metered execution</span></em><span> </span>&mdash;<span> during processing, decrement a counter for every</span>
<span>simple atomic step and terminate processing when the counter reaches zero.</span></p>
<p><em><span>Finally</span></em><span> one more vague property you</span>&rsquo;<span>d want from a configuration language is for it to be simple.</span>
<span>That is, to ensure that, when people use your language, they write simple programs. It seems to me</span>
<span>that this might actually be the case where banning recursion and unbounded loops could help, though</span>
<span>I am not sure. As we know from the PRF exercise, this won</span>&rsquo;<span>t actually prevent people from writing</span>
<span>arbitrary recursive programs. It</span>&rsquo;<span>ll just require </span><a href="https://mochiro.moe/posts/09-meson-raytracer/"><span>some roundabout</span>
<span>code</span></a><span> to do that. But maybe that</span>&rsquo;<span>ll be enough of a</span>
<span>speedbump to make someone invent a simple solution, instead of brute-forcing the most obvious one?</span></p>
<p><span>That</span>&rsquo;<span>s all for today! Have a great weekend, and remember:</span></p>

<figure class="blockquote">
<blockquote><p><span>Any algorithm that can be implemented by a Turing Machine such that its runtime is bounded by some</span>
<span>primitive recursive function of input can also be implemented by a primitive recursive function!</span></p>
</blockquote>

</figure>
</section>
]]></content>
</entry>

<entry>
<title type="text">How I Use Git Worktrees</title>
<link href="https://matklad.github.io/2024/07/25/git-worktrees.html" rel="alternate" type="text/html" title="How I Use Git Worktrees" />
<published>2024-07-25T00:00:00+00:00</published>
<updated>2024-07-25T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/07/25/git-worktrees</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[There are a bunch of posts on the internet about using git worktree command. As far as I can tell,
most of them are primarily about using worktrees as a replacement of, or a supplement to git
branches. Instead of switching branches, you just change directories. This is also how I originally
had used worktrees, but that didn't stick, and I abandoned them. But recently worktrees grew
on me, though my new use-case is unlike branching.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/07/25/git-worktrees.html"><![CDATA[
<h1><span>How I Use Git Worktrees</span> <time class="meta" datetime="2024-07-25">Jul 25, 2024</time></h1>
<p><span>There are a bunch of posts on the internet about using </span><code>git worktree</code><span> command. As far as I can tell,</span>
<span>most of them are primarily about using worktrees as a replacement of, or a supplement to git</span>
<span>branches. Instead of switching branches, you just change directories. This is also how I originally</span>
<span>had used worktrees, but that didn</span>&rsquo;<span>t stick, and I abandoned them. But recently worktrees grew</span>
<span>on me, though my new use-case is unlike branching.</span></p>
<section id="When-a-Branch-is-Enough">

    <h2>
    <a href="#When-a-Branch-is-Enough"><span>When a Branch is Enough</span> </a>
    </h2>
<p><span>If you use worktrees as a replacement for branching, that</span>&rsquo;<span>s great, no need to change anything! But</span>
<span>let me start with explaining why that workflow isn</span>&rsquo;<span>t for me.</span></p>
<p><span>The principal problem with using branches is that it</span>&rsquo;<span>s hard to context switch in the middle of doing</span>
<span>something. You have your branch, your commit, a bunch of changes in the work tree, some of them</span>
<span>might be stages and some unstaged. You can</span>&rsquo;<span>t really tell Git </span>&ldquo;<span>save all this context and restore it</span>
<span>later.</span>&rdquo;<span> The solution that Git suggests here is to use stashing, but that</span>&rsquo;<span>s awkward, as it is too</span>
<span>easy to get lost when stashing several things at the same time, and then applying the stash on top</span>
<span>of the wrong branch.</span></p>
<p><span>Managing Git state became much easier for me when I realized that the staging area and the stash are just bad</span>
<span>features, and life is easier if I avoid them. Instead, I just commit whatever and deal with</span>
<span>it later. So, when I need to switch a branch in the middle of things, what I do is, basically:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-title function_">$</span> git add .</span>
<span class="line"><span class="hl-title function_">$</span> git commit -m.</span>
<span class="line"><span class="hl-title function_">$</span> git switch another-branch</span></code></pre>

</figure>
<p><span>And, to switch back,</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-title function_">$</span> git switch -</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># Undo the last commit, but keep its changes in the working tree</span></span>
<span class="line"><span class="hl-title function_">$</span> git reset HEAD~</span></code></pre>

</figure>
<p><span>To make this more streamlined, I have a </span><code>ggc</code><span> utility which does </span>&ldquo;<span>commit all with a trivial message</span>&rdquo;
<span>atomically.</span></p>

<aside class="admn note">
<svg class="icon"><use href="/assets/icons.svg#info"/></svg>
<div><p><span>Reminder: Git is not a version control system, Git is a toolbox for building a VCS. Do have a</span>
<span>low-friction way to add your own scripts for common git operations.</span></p>
</div>
</aside><p><span>And I don</span>&rsquo;<span>t always </span><code>reset HEAD~</code><span> </span>&mdash;<span> I usually just continue hacking with </span><code>.</code><span> in my Git log and then amend the commit</span>
<span>once I am satisfied with subset of changes</span></p>

<aside class="admn note">
<svg class="icon"><use href="/assets/icons.svg#info"/></svg>
<div><p><span>Reminder: magit, for </span><a href="https://magit.vc"><span>Emacs</span></a><span> and </span><a href="https://github.com/kahole/edamagit"><span>VS Code</span></a><span>, is</span>
<span>excellent for making such commit surgery easy. In particular, </span><strong><strong><span>instant fixup</span></strong></strong><span> is excellent. Even</span>
<span>if you don</span>&rsquo;<span>t use magit, you should have an equivalent of instant fixup among your Git scripts.</span></p>
</div>
</aside><p><span>So that</span>&rsquo;<span>s how I deal with switching branches. But why worktrees then?</span></p>
</section>
<section id="Worktree-Per-Concurrent-Activity">

    <h2>
    <a href="#Worktree-Per-Concurrent-Activity"><span>Worktree Per Concurrent Activity</span> </a>
    </h2>
<p><span>It</span>&rsquo;<span>s a bit hard to describe, but:</span></p>
<ul>
<li>
<span>I have a fixed number of worktrees (5, to be exact)</span>
</li>
<li>
<span>worktrees are mostly uncorrelated to branches</span>
</li>
<li>
<span>but instead correspond to my concurrent activities during coding.</span>
</li>
</ul>
<p><span>Specifically:</span></p>
<ul>
<li>
<p><span>The </span><strong><span>main</span></strong><span> worktree is a readonly worktree that contains a recent snapshot of the remote main</span>
<span>branch. I use this tree to compare the code I am currently working on and/or reviewing with the</span>
<span>master version (this includes things like </span>&ldquo;<span>how long the build takes</span>&rdquo;<span>, </span>&ldquo;<span>what is the behavior of</span>
<span>this test</span>&rdquo;<span> and the like, so not just the actual source code).</span></p>
</li>
<li>
<p><span>The </span><strong><span>work</span></strong><span> worktree, where I write most of the code. I often need to write new code and compare it</span>
<span>with old code at the same time. But can</span>&rsquo;<span>t actually work on two different things in parallel.</span>
<span>That</span>&rsquo;<span>s why </span><code>main</code><span> and </span><code>work</code><span> are different worktrees, but </span><code>work</code><span> also constantly switches branches.</span></p>
</li>
<li>
<p><span>The </span><strong><span>review</span></strong><span> worktree, where I checkout code for code review. While I can</span>&rsquo;<span>t review code and write</span>
<span>code at the same time, there is one thing I am implementing, and one thing I am reviewing, but the</span>
<span>review and implementation proceed concurrently.</span></p>
</li>
<li>
<p><span>Then, there</span>&rsquo;<span>s the </span><strong><span>fuzz</span></strong><span> tree, where I run long-running fuzzing jobs for the code I am actively working</span>
<span>on. My overall idealized feature workflow looks like this:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-comment"># go to the `work` worktree</span></span>
<span class="line"><span class="hl-title function_">$</span> cd ~/projects/tigerbeetle/work</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># Create a new branch. As we work with a centralized repo,</span></span>
<span class="line"><span class="hl-comment"># rather than personal forks, I tend to prefix my branch names</span></span>
<span class="line"><span class="hl-comment"># with `matklad/`</span></span>
<span class="line"><span class="hl-title function_">$</span> git switch -c matklad/awesome-feature</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># Start with a reasonably clean slate.</span></span>
<span class="line"><span class="hl-comment"># In reality, I have yet another script to start a branch off</span></span>
<span class="line"><span class="hl-comment"># fresh from the main remote, but this reset is a good enough approximation.</span></span>
<span class="line"><span class="hl-title function_">$</span> git reset --hard origin/main</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># For more complicated features, I start with an empty commit</span></span>
<span class="line"><span class="hl-comment"># and write the commit message _first_, before starting the work.</span></span>
<span class="line"><span class="hl-comment"># That's a good way to collect your thoughts and discover dead</span></span>
<span class="line"><span class="hl-comment"># ends more gracefully than hitting a brick wall coding at 80 WPM.</span></span>
<span class="line"><span class="hl-title function_">$</span> git commit --allow-empty</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># Hack furiously writing throughway code.</span></span>
<span class="line"><span class="hl-title function_">$</span> code .</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># At this point, I have something that I hope works</span></span>
<span class="line"><span class="hl-comment"># but would be embarrassed to share with anyone!</span></span>
<span class="line"><span class="hl-comment"># So that's the good place to kick off fuzzing.</span></span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># First, I commit everything so far.</span></span>
<span class="line"><span class="hl-comment"># Remember, I have `ggc` one liner for this:</span></span>
<span class="line"><span class="hl-title function_">$</span> git add . &amp;&amp; git commit -m.</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># Now I go to my `fuzz` worktree and kick off fuzzing.</span></span>
<span class="line"><span class="hl-comment"># I usually split screen here.</span></span>
<span class="line"><span class="hl-comment"># On the left, I copy the current commit hash.</span></span>
<span class="line"><span class="hl-comment"># On the right, I switch to the fuzzing worktree,</span></span>
<span class="line"><span class="hl-comment"># switch to the copied commit, and start fuzzing:</span></span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-title function_">$</span> git add . &amp;&amp; git commit -m.  |</span>
<span class="line"><span class="hl-title function_">$</span> git rev-parse HEAD | ctrlc   | $ cd ../fuzz</span>
<span class="line"><span class="hl-title function_">$</span>                              | $ git switch -d $(ctrlv)</span>
<span class="line"><span class="hl-title function_">$</span>                              | $ ./zig/zig build fuzz</span>
<span class="line"><span class="hl-title function_">$</span>                              |</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># While the fuzzer hums on the right, I continue to furiously refactor</span></span>
<span class="line"><span class="hl-comment"># the code on the left and hammer my empty commit with a wishful</span></span>
<span class="line"><span class="hl-comment"># thinking message and my messy code commit with `.` message into</span></span>
<span class="line"><span class="hl-comment"># a semblance of clean git history</span></span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-title function_">$</span> code .</span>
<span class="line"><span class="hl-title function_">$</span> magit-goes-brrrrr</span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-comment"># At this point, in the work tree, I am happy with both the code</span></span>
<span class="line"><span class="hl-comment"># and the Git history, so, if the fuzzer on the right is happy,</span></span>
<span class="line"><span class="hl-comment"># a PR is opened!</span></span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-title function_">$</span>                              |</span>
<span class="line"><span class="hl-title function_">$</span> git push --force-with-lease  | $ ./zig/zig build fuzz</span>
<span class="line"><span class="hl-title function_">$</span> gh pr create --web           | # Still hasn't failed</span>
<span class="line"><span class="hl-title function_">$</span>                              |</span></code></pre>

</figure>
<p><span>This is again concurrent: I can hack on the branch while the fuzzer tests the </span>&ldquo;<span>same</span>&rdquo;<span> code. Note</span>
<span>that it is crucial that the fuzzing tree operates in the detached head state (</span><code>-d</code><span> flag for </span><code>git</code>
<code>switch</code><span>). In general, </span><code>-d</code><span> is very helpful with this style of worktree work. I am also</span>
<span>sympathetic to </span><a href="https://martinvonz.github.io/jj/latest/"><span>the argument</span></a><span> that, like the staging area</span>
<span>and the stash, Git branches are a misfeature, but I haven</span>&rsquo;<span>t made the plunge personally yet.</span></p>
</li>
<li>
<p><span>Finally, the last tree I have is </span><strong><span>scratch</span></strong><span> </span>&ndash;<span> this is a tree for arbitrary random things I need</span>
<span>to do while working on something else. For example, if I am working on </span><code>matklad/my-feature</code><span> in</span>
<code>work</code><span>, and reviewing </span><code>#6292</code><span> in </span><code>review</code><span>, and, while reviewing, notice a tiny unrelated typo, the</span>
<span>PR for that typo is quickly prepped in the </span><code>scratch</code><span> worktree:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-title function_">$</span> cd ../scratch</span>
<span class="line"><span class="hl-title function_">$</span> git switch -c matklad/quick-fix</span>
<span class="line"><span class="hl-title function_">$</span> code . &amp;&amp; git add . &amp;&amp; git commit -m 'typo' &amp;&amp; git push</span>
<span class="line"><span class="hl-title function_">$</span> cd -</span></code></pre>

</figure>
</li>
</ul>
<p><span>TL;DR: consider using worktrees not as a replacement for branches, but as a means to manage</span>
<span>concurrency in your tasks. My level of concurrency is:</span></p>
<ul>
<li>
<code>main</code><span> for looking at the pristine code,</span>
</li>
<li>
<code>work</code><span> for looking at my code,</span>
</li>
<li>
<code>review</code><span> for looking at someone else</span>&rsquo;<span>s code,</span>
</li>
<li>
<code>fuzz</code><span> for my computer to look at my code,</span>
</li>
<li>
<code>scratch</code><span> for everything else!</span>
</li>
</ul>
</section>
]]></content>
</entry>

<entry>
<title type="text">Properly Testing Concurrent Data Structures</title>
<link href="https://matklad.github.io/2024/07/05/properly-testing-concurrent-data-structures.html" rel="alternate" type="text/html" title="Properly Testing Concurrent Data Structures" />
<published>2024-07-05T00:00:00+00:00</published>
<updated>2024-07-05T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/07/05/properly-testing-concurrent-data-structures</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[There's a fascinating Rust library, loom, which can be used to
thoroughly test lock-free data structures. I always wanted to learn how it works. I still do! But
recently I accidentally implemented a small toy which, I think, contains some of the loom's ideas,
and it seems worthwhile to write about that. The goal here isn't to teach you what you should be
using in practice (if you need that, go read loom's docs), but rather to derive a couple of neat
ideas from first principles.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/07/05/properly-testing-concurrent-data-structures.html"><![CDATA[
<h1><span>Properly Testing Concurrent Data Structures</span> <time class="meta" datetime="2024-07-05">Jul 5, 2024</time></h1>
<p><span>There</span>&rsquo;<span>s a fascinating Rust library, </span><a href="https://github.com/tokio-rs/loom"><span>loom</span></a><span>, which can be used to</span>
<span>thoroughly test lock-free data structures. I always wanted to learn how it works. I still do! But</span>
<span>recently I accidentally implemented a small toy which, I think, contains some of the loom</span>&rsquo;<span>s ideas,</span>
<span>and it seems worthwhile to write about that. The goal here isn</span>&rsquo;<span>t to teach you what you should be</span>
<span>using in practice (if you need that, go read loom</span>&rsquo;<span>s docs), but rather to derive a couple of neat</span>
<span>ideas from first principles.</span></p>
<section id="One-Two-Three-Two">

    <h2>
    <a href="#One-Two-Three-Two"><span>One, Two, Three, Two</span> </a>
    </h2>
<p><span>As usual, we need the simplest possible model program to mess with. The example we use comes from</span>
<a href="https://stevana.github.io/the_sad_state_of_property-based_testing_libraries.html"><span>this excellent article</span></a><span>.</span>
<span>Behold, a humble (and broken) concurrent counter:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">use</span> std::sync::atomic::{</span>
<span class="line">  AtomicU32,</span>
<span class="line">  Ordering::SeqCst,</span>
<span class="line">};</span>
<span class="line"></span>
<span class="line"><span class="hl-meta">#[derive(Default)]</span></span>
<span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">struct</span> <span class="hl-title class_">Counter</span> {</span>
<span class="line">  value: AtomicU32,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">Counter</span> {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">increment</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">value</span> = <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">load</span>(SeqCst);</span>
<span class="line">    <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">store</span>(value + <span class="hl-number">1</span>, SeqCst);</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">get</span>(&amp;<span class="hl-keyword">self</span>) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">u32</span> {</span>
<span class="line">    <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">load</span>(SeqCst)</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>The bug is obvious here </span>&mdash;<span> the increment is not atomic. But what is the best test we can write to</span>
<span>expose it?</span></p>
</section>
<section id="Trivial-Test">

    <h2>
    <a href="#Trivial-Test"><span>Trivial Test</span> </a>
    </h2>
<p><span>The simplest idea that comes to mind is to just hammer the same counter from multiple threads and</span>
<span>check the result at the end;</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[test]</span></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">threaded_test</span>() {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">counter</span> = Counter::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">thread_count</span> = <span class="hl-number">100</span>;</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">increment_count</span> = <span class="hl-number">100</span>;</span>
<span class="line"></span>
<span class="line">  std::thread::<span class="hl-title function_ invoke__">scope</span>(|scope| {</span>
<span class="line">    <span class="hl-keyword">for</span> <span class="hl-variable">_</span> <span class="hl-keyword">in</span> <span class="hl-number">0</span>..thread_count {</span>
<span class="line">      scope.<span class="hl-title function_ invoke__">spawn</span>(|| {</span>
<span class="line">        <span class="hl-keyword">for</span> <span class="hl-variable">_</span> <span class="hl-keyword">in</span> <span class="hl-number">0</span>..increment_count {</span>
<span class="line">          counter.<span class="hl-title function_ invoke__">increment</span>()</span>
<span class="line">        }</span>
<span class="line">      });</span>
<span class="line">    }</span>
<span class="line">  });</span>
<span class="line"></span>
<span class="line">  <span class="hl-built_in">assert_eq!</span>(counter.<span class="hl-title function_ invoke__">get</span>(), thread_count * increment_count);</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>This fails successfully:</span></p>

<figure class="code-block">


<pre><code><span class="line">thread 'counter::trivial' panicked:</span>
<span class="line">assertion `left == right` failed</span>
<span class="line">  left: 9598</span>
<span class="line"> right: 10000</span></code></pre>

</figure>
<p><span>But I wouldn</span>&rsquo;<span>t call this test satisfactory </span>&mdash;<span> it very much depends on the timing, so you can</span>&rsquo;<span>t</span>
<span>reproduce it deterministically and you can</span>&rsquo;<span>t debug it. You also can</span>&rsquo;<span>t minimize it </span>&mdash;<span> if you reduce</span>
<span>the number of threads and increments, chances are the test passes by luck!</span></p>
</section>
<section id="PBT">

    <h2>
    <a href="#PBT"><span>PBT</span> </a>
    </h2>
<p><span>Of course the temptation is to apply property based testing here! The problem </span><em><span>almost</span></em><span> fits: we have</span>
<span>easy-to-generate input (the sequence of increments spread over several threads), a good property to</span>
<span>check (result of concurrent increments is identical to that of sequential execution) and the desire</span>
<span>to minimize the test.</span></p>
<p><span>But just how can we plug threads into a property-based test?</span></p>
<p><span>PBTs are great for testing state machines. You can run your state machine through a series of steps</span>
<span>where at each step a PBT selects an arbitrary next action to apply to the state:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[test]</span></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">state_machine_test</span>() {</span>
<span class="line">  arbtest::<span class="hl-title function_ invoke__">arbtest</span>(|rng| {</span>
<span class="line">    <span class="hl-comment">// This is our state machine!</span></span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">state</span>: <span class="hl-type">i32</span> = <span class="hl-number">0</span>;</span>
<span class="line"></span>
<span class="line">    <span class="hl-comment">// We&#x27;ll run it for up to 100 steps.</span></span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">step_count</span>: <span class="hl-type">usize</span> = rng.<span class="hl-title function_ invoke__">int_in_range</span>(<span class="hl-number">0</span>..=<span class="hl-number">100</span>)?;</span>
<span class="line"></span>
<span class="line">    <span class="hl-keyword">for</span> <span class="hl-variable">_</span> <span class="hl-keyword">in</span> <span class="hl-number">0</span>..step_count {</span>
<span class="line">      <span class="hl-comment">// At each step, we flip a coin and</span></span>
<span class="line">      <span class="hl-comment">// either increment or decrement.</span></span>
<span class="line">      <span class="hl-keyword">match</span> *rng.<span class="hl-title function_ invoke__">choose</span>(&amp;[<span class="hl-string">&quot;inc&quot;</span>, <span class="hl-string">&quot;dec&quot;</span>])? {</span>
<span class="line">        <span class="hl-string">&quot;inc&quot;</span> =&gt; state += <span class="hl-number">1</span>,</span>
<span class="line">        <span class="hl-string">&quot;dec&quot;</span> =&gt; state -= <span class="hl-number">1</span>,</span>
<span class="line">        _ =&gt; <span class="hl-built_in">unreachable!</span>(),</span>
<span class="line">      }</span>
<span class="line">    }</span>
<span class="line">    <span class="hl-title function_ invoke__">Ok</span>(())</span>
<span class="line">  });</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>And it </span><em><span>feels</span></em><span> like we should be able to apply the same technique here. At every iteration, pick a</span>
<span>random thread and make it do a single step. If you can step the threads manually, it should be easy</span>
<span>to maneuver one thread in between load&amp;store of a different thread.</span></p>
<p><span>But we can</span>&rsquo;<span>t step through threads! Or can we?</span></p>
</section>
<section id="Simple-Instrumentation">

    <h2>
    <a href="#Simple-Instrumentation"><span>Simple Instrumentation</span> </a>
    </h2>
<p><span>Ok, let</span>&rsquo;<span>s fake it until we make it! Let</span>&rsquo;<span>s take a look at the buggy increment method:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">increment</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">value</span> = <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">load</span>(SeqCst);</span>
<span class="line">  <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">store</span>(value + <span class="hl-number">1</span>, SeqCst);</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Ideally, we</span>&rsquo;<span>d love to be able to somehow </span>&ldquo;<span>pause</span>&rdquo;<span> the thread in-between atomic operations. Something</span>
<span>like this:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">increment</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">  <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">value</span> = <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">load</span>(SeqCst);</span>
<span class="line">  <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">  <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">store</span>(value + <span class="hl-number">1</span>, SeqCst);</span>
<span class="line">  <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">pause</span>() {</span>
<span class="line">    <span class="hl-comment">// ¯\_(ツ)_/¯</span></span>
<span class="line">}</span></code></pre>

</figure>
<p><span>So let</span>&rsquo;<span>s start with implementing our own wrapper for </span><code>AtomicU32</code><span> which includes calls to pause.</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">use</span> std::sync::atomic::Ordering;</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">struct</span> <span class="hl-title class_">AtomicU32</span> {</span>
<span class="line">  inner: std::sync::atomic::AtomicU32,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">AtomicU32</span> {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">load</span>(&amp;<span class="hl-keyword">self</span>, ordering: Ordering) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">u32</span> {</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">result</span> = <span class="hl-keyword">self</span>.inner.<span class="hl-title function_ invoke__">load</span>(ordering);</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">    result</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">store</span>(&amp;<span class="hl-keyword">self</span>, value: <span class="hl-type">u32</span>, ordering: Ordering) {</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">    <span class="hl-keyword">self</span>.inner.<span class="hl-title function_ invoke__">store</span>(value, ordering);</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">  }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">pause</span>() {</span>
<span class="line">  <span class="hl-comment">// still no idea :(</span></span>
<span class="line">}</span></code></pre>

</figure>
</section>
<section id="Managed-Threads-API">

    <h2>
    <a href="#Managed-Threads-API"><span>Managed Threads API</span> </a>
    </h2>
<p><span>One rule of a great API design is that you start by implement a single </span><em><span>user</span></em><span> of an API, to</span>
<span>understand how the API should </span><em><span>feel</span></em><span>, and only then proceed to the actual implementation.</span></p>
<p><span>So, in the spirit of faking, let</span>&rsquo;<span>s just write a PBT using these pausable, managed threads, even if</span>
<span>we still have no idea how to actually implement pausing.</span></p>
<p><span>We start with creating a counter and two managed threads. And we probably want to pass a reference</span>
<span>to the counter to each of the threads:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">counter</span> = Counter::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">t1</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(&amp;counter);</span>
<span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">t2</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(&amp;counter);</span></code></pre>

</figure>
<p><span>Now, we want to step through the threads:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">while</span> !rng.<span class="hl-title function_ invoke__">is_empty</span>() {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">coin_flip</span>: <span class="hl-type">bool</span> = rng.<span class="hl-title function_ invoke__">arbitrary</span>()?;</span>
<span class="line">  <span class="hl-keyword">if</span> t1.<span class="hl-title function_ invoke__">is_paused</span>() {</span>
<span class="line">    <span class="hl-keyword">if</span> coin_flip {</span>
<span class="line">      t1.<span class="hl-title function_ invoke__">unpause</span>();</span>
<span class="line">    }</span>
<span class="line">  }</span>
<span class="line">  <span class="hl-keyword">if</span> t2.<span class="hl-title function_ invoke__">is_paused</span>() {</span>
<span class="line">    <span class="hl-keyword">if</span> coin_flip {</span>
<span class="line">      t2.<span class="hl-title function_ invoke__">unpause</span>();</span>
<span class="line">    }</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Or, refactoring this a bit to semantically compress:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">counter</span> = Counter::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">t1</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(&amp;counter);</span>
<span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">t2</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(&amp;counter);</span>
<span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">threads</span> = [t1, t2];</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">while</span> !rng.<span class="hl-title function_ invoke__">is_empty</span>() {</span>
<span class="line">  <span class="hl-keyword">for</span> <span class="hl-variable">t</span> <span class="hl-keyword">in</span> &amp;<span class="hl-keyword">mut</span> threads {</span>
<span class="line">    <span class="hl-keyword">if</span> t.<span class="hl-title function_ invoke__">is_paused</span>() &amp;&amp; rng.<span class="hl-title function_ invoke__">arbitrary</span>()? {</span>
<span class="line">      t.<span class="hl-title function_ invoke__">unpause</span>()</span>
<span class="line">    }</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>That is, on each step of our state machine, we loop through all threads and unpause a random subset</span>
<span>of them.</span></p>
<p><span>But besides pausing and unpausing, we need our threads to actually </span><em><span>do</span></em><span> something, to increment the</span>
<span>counter. One idea is to mirror the </span><code>std::spawn</code><span> API and pass a closure in:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">t1</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>({</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">counter</span> = &amp;counter;</span>
<span class="line">  <span class="hl-keyword">move</span> || {</span>
<span class="line">    <span class="hl-keyword">for</span> <span class="hl-variable">_</span> <span class="hl-keyword">in</span> <span class="hl-number">0</span>..<span class="hl-number">100</span> {</span>
<span class="line">      counter.<span class="hl-title function_ invoke__">increment</span>();</span>
<span class="line">    }</span>
<span class="line">  }</span>
<span class="line">});</span></code></pre>

</figure>
<p><span>But as these are managed threads, and we want to control them from our tests, lets actually go all</span>
<span>the way there and give the controlling thread an ability to change the code running in a managed</span>
<span>thread. That is, we</span>&rsquo;<span>ll start managed threads without a </span>&ldquo;<span>main</span>&rdquo;<span> function, and provide an API to</span>
<span>execute arbitrary closures in the context of this by-default inert thread (</span><a href="https://joearms.github.io/published/2013-11-21-My-favorite-erlang-program.html"><span>universal</span>
<span>server</span></a><span> anyone?):</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">counter</span> = Counter::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line"></span>
<span class="line"><span class="hl-comment">// We pass the state, &amp;counter, in, but otherwise the thread is inert.</span></span>
<span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">t</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(&amp;counter);</span>
<span class="line"></span>
<span class="line"><span class="hl-comment">// But we can manually poke it:</span></span>
<span class="line">t.<span class="hl-title function_ invoke__">submit</span>(|thread_state: &amp;Counter| thread_state.<span class="hl-title function_ invoke__">increment</span>());</span>
<span class="line">t.<span class="hl-title function_ invoke__">submit</span>(|thread_state: &amp;Counter| thread_state.<span class="hl-title function_ invoke__">increment</span>());</span></code></pre>

</figure>
<p><span>Putting everything together, we get a nice-looking property test:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[cfg(test)]</span></span>
<span class="line"><span class="hl-keyword">use</span> managed_thread::AtomicU32;</span>
<span class="line"><span class="hl-meta">#[cfg(not(test))]</span></span>
<span class="line"><span class="hl-keyword">use</span> std::sync::atomic::AtomicU32;</span>
<span class="line"></span>
<span class="line"><span class="hl-meta">#[derive(Default)]</span></span>
<span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">struct</span> <span class="hl-title class_">Counter</span> {</span>
<span class="line">  value: AtomicU32,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">Counter</span> {</span>
<span class="line">  <span class="hl-comment">// ...</span></span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-meta">#[test]</span></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">test_counter</span>() {</span>
<span class="line">  arbtest::<span class="hl-title function_ invoke__">arbtest</span>(|rng| {</span>
<span class="line">    <span class="hl-comment">// Our &quot;Concurrent System Under Test&quot;.</span></span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">counter</span> = Counter::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line"></span>
<span class="line">    <span class="hl-comment">// The sequential model we&#x27;ll compare the result against.</span></span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">counter_model</span>: <span class="hl-type">u32</span> = <span class="hl-number">0</span>;</span>
<span class="line"></span>
<span class="line">    <span class="hl-comment">// Two managed threads which we will be stepping through</span></span>
<span class="line">    <span class="hl-comment">// manually.</span></span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">t1</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(&amp;counter);</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">t2</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(&amp;counter);</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">threads</span> = [t1, t2];</span>
<span class="line"></span>
<span class="line">    <span class="hl-comment">// Bulk of the test: in a loop, flip a coin and advance</span></span>
<span class="line">    <span class="hl-comment">// one of the threads.</span></span>
<span class="line">    <span class="hl-keyword">while</span> !rng.<span class="hl-title function_ invoke__">is_empty</span>() {</span>
<span class="line">      <span class="hl-keyword">for</span> <span class="hl-variable">t</span> <span class="hl-keyword">in</span> &amp;<span class="hl-keyword">mut</span> [t1, t2] {</span>
<span class="line">        <span class="hl-keyword">if</span> rng.<span class="hl-title function_ invoke__">arbitrary</span>() {</span>
<span class="line">          <span class="hl-keyword">if</span> t.<span class="hl-title function_ invoke__">is_paused</span>() {</span>
<span class="line">            t.<span class="hl-title function_ invoke__">unpause</span>()</span>
<span class="line">          } <span class="hl-keyword">else</span> {</span>
<span class="line">            <span class="hl-comment">// Standard &quot;model equivalence&quot; property: apply</span></span>
<span class="line">            <span class="hl-comment">// isomorphic actions to the system and its model.</span></span>
<span class="line">            t.<span class="hl-title function_ invoke__">submit</span>(|c| c.<span class="hl-title function_ invoke__">increment</span>());</span>
<span class="line">            counter_model += <span class="hl-number">1</span>;</span>
<span class="line">          }</span>
<span class="line">        }</span>
<span class="line">      }</span>
<span class="line">    }</span>
<span class="line"></span>
<span class="line">    <span class="hl-keyword">for</span> <span class="hl-variable">t</span> <span class="hl-keyword">in</span> threads {</span>
<span class="line">      t.<span class="hl-title function_ invoke__">join</span>();</span>
<span class="line">    }</span>
<span class="line"></span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(counter_model, counter.<span class="hl-title function_ invoke__">get</span>());</span>
<span class="line"></span>
<span class="line">    <span class="hl-title function_ invoke__">Ok</span>(())</span>
<span class="line">  });</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Now, if only we could make this API work</span>&hellip;<span> Remember, our </span><code>pause</code><span> implementation is a shrug emoji!</span></p>
<p><span>At this point, you might be mightily annoyed at me for this rhetorical device where I pretend that I</span>
<span>don</span>&rsquo;<span>t know the answer. No need for annoyance </span>&mdash;<span> when writing this code for the first time, I traced</span>
<span>exactly these steps </span>&mdash;<span> I realized that I need a </span>&ldquo;<span>pausing </span><code>AtomicU32</code>&rdquo;<span> so I did that (with dummy</span>
<span>pause calls), then I played with the API I </span><em><span>wanted</span></em><span> to have, ending at roughly this spot, without</span>
<span>yet knowing how I would make it work or, indeed, if it is possible at all.</span></p>
<p><span>Well, if I am being honest, there is a bit of up-front knowledge here. I don</span>&rsquo;<span>t think we can avoid</span>
<span>spawning real threads here, unless we do something really cursed with inline assembly. When</span>
<em><span>something</span></em><span> calls that </span><code>pause()</code><span> function, and we want it to stay paused until further notice, that</span>
<span>just has to happen in a thread which maintains a stack separate from the stack of our test. And, if</span>
<span>we are going to spawn threads, we might as well spawn scoped threads, so that we can freely borrow</span>
<span>stack-local data. And to spawn a scope thread, you need a</span>
<a href="https://doc.rust-lang.org/stable/std/thread/struct.Scope.html"><code>Scope</code></a><span> parameter. So in reality</span>
<span>we</span>&rsquo;<span>ll need one more level of indentation here:</span></p>

<figure class="code-block">


<pre><code><span class="line">    std::thread::<span class="hl-title function_ invoke__">scope</span>(|scope| {</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">t1</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(scope, &amp;counter);</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">t2</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(scope, &amp;counter);</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">threads</span> = [t1, t2];</span>
<span class="line">      <span class="hl-keyword">while</span> !rng.<span class="hl-title function_ invoke__">is_empty</span>() {</span>
<span class="line">        <span class="hl-keyword">for</span> <span class="hl-variable">t</span> <span class="hl-keyword">in</span> &amp;<span class="hl-keyword">mut</span> [t1, t2] {</span>
<span class="line">          <span class="hl-comment">// ...</span></span>
<span class="line">        }</span>
<span class="line">      }</span>
<span class="line">    });</span></code></pre>

</figure>
</section>
<section id="Managed-Threads-Implementation">

    <h2>
    <a href="#Managed-Threads-Implementation"><span>Managed Threads Implementation</span> </a>
    </h2>
<p><span>Now, the fun part: how the heck are we going to make pausing and unpausing work? For starters, there</span>
<span>clearly needs to be some communication between the main thread (</span><code>t.unpause()</code><span>) and the managed</span>
<span>thread (</span><code>pause()</code><span>). And, because we don</span>&rsquo;<span>t want to change </span><code>Counter</code><span> API to thread some kind of</span>
<span>test-only context, the context needs to be smuggled. So </span><code>thread_local!</code><span> it is. And this context</span>
<span>is going to be shared between two threads, so it must be wrapped in an </span><code>Arc</code><span>.</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">struct</span> <span class="hl-title class_">SharedContext</span> {</span>
<span class="line">  <span class="hl-comment">// 🤷</span></span>
<span class="line">}</span>
<span class="line"></span>
<span class="line">thread_local! {</span>
<span class="line">  <span class="hl-keyword">static</span> INSTANCE: RefCell&lt;<span class="hl-type">Option</span>&lt;Arc&lt;SharedContext&gt;&gt;&gt; =</span>
<span class="line">    RefCell::<span class="hl-title function_ invoke__">new</span>(<span class="hl-literal">None</span>);</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">SharedContext</span> {</span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">set</span>(ctx: Arc&lt;SharedContext&gt;) {</span>
<span class="line">    INSTANCE.<span class="hl-title function_ invoke__">with</span>(|it| *it.<span class="hl-title function_ invoke__">borrow_mut</span>() = <span class="hl-title function_ invoke__">Some</span>(ctx));</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">get</span>() <span class="hl-punctuation">-&gt;</span> <span class="hl-type">Option</span>&lt;Arc&lt;SharedContext&gt;&gt; {</span>
<span class="line">    INSTANCE.<span class="hl-title function_ invoke__">with</span>(|it| it.<span class="hl-title function_ invoke__">borrow</span>().<span class="hl-title function_ invoke__">clone</span>())</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>As usual when using </span><code>thread_local!</code><span> or </span><code>lazy_static!</code><span>, it is convenient to immediately wrap it into</span>
<span>better typed accessor functions. And, given that we are using an </span><code>Arc</code><span> here anyway, we can</span>
<span>conveniently escape </span><code>thread_local</code>&rsquo;<span>s </span><code>with</code><span> by cloning the </span><code>Arc</code><span>.</span></p>
<p><span>So now we finally can implement the global </span><code>pause</code><span> function (or at least can kick the proverbial can</span>
<span>a little bit farther):</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">pause</span>() {</span>
<span class="line">  <span class="hl-keyword">if</span> <span class="hl-keyword">let</span> <span class="hl-variable">Some</span>(ctx) = SharedContext::<span class="hl-title function_ invoke__">get</span>() {</span>
<span class="line">    ctx.<span class="hl-title function_ invoke__">pause</span>()</span>
<span class="line">  }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">SharedContext</span> {</span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">pause</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-comment">// 😕</span></span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Ok, what to do next? We somehow need to coordinate the control thread and the managed thread. And we</span>
<span>need some sort of notification mechanism, so that the managed thread knows when it can continue. The</span>
<span>most brute force solution here is a pair of a mutex protecting some state and a condition variable.</span>
<span>Mutex guards the state that can be manipulated by either of the threads. Condition variable can be</span>
<span>used to signal about the changes.</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">struct</span> <span class="hl-title class_">SharedContext</span> {</span>
<span class="line">  state: Mutex&lt;State&gt;,</span>
<span class="line">  cv: Condvar,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">struct</span> <span class="hl-title class_">State</span> {</span>
<span class="line">  <span class="hl-comment">// 🤡</span></span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Okay, it looks like I am running out of emojies here. There</span>&rsquo;<span>s no more layers of indirection or</span>
<span>infrastructure left, we need to write some real code that actually does do that pausing thing. So</span>
<span>let</span>&rsquo;<span>s say that the state is tracking, well, the state of our managed thread, which can be either</span>
<span>running or paused:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[derive(PartialEq, Eq, Default)]</span></span>
<span class="line"><span class="hl-keyword">enum</span> <span class="hl-title class_">State</span> {</span>
<span class="line">  <span class="hl-meta">#[default]</span></span>
<span class="line">  Running,</span>
<span class="line">  Paused,</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>And then the logic of the pause function </span>&mdash;<span> flip the state from </span><code>Running</code><span> to </span><code>Paused</code><span>, notify the</span>
<span>controlling thread that we are </span><code>Paused</code><span>, and wait until the controlling thread flips our state back</span>
<span>to </span><code>Running</code><span>:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">SharedContext</span> {</span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">pause</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Running);</span>
<span class="line">    *guard = State::Paused;</span>
<span class="line">    <span class="hl-keyword">self</span>.cv.<span class="hl-title function_ invoke__">notify_all</span>();</span>
<span class="line">    <span class="hl-keyword">while</span> *guard == State::Paused {</span>
<span class="line">      guard = <span class="hl-keyword">self</span>.cv.<span class="hl-title function_ invoke__">wait</span>(guard).<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    }</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Running);</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Aside: Rust</span>&rsquo;<span>s API for condition variables is beautiful. Condvars are tricky, and I didn</span>&rsquo;<span>t really</span>
<span>understood them until seeing the signatures of Rust functions. Notice how the </span><code>wait</code><span> function</span>
<em><span>takes</span></em><span> a mutex guard as an argument, and returns a mutex guard. This protects you from the logical</span>
<span>races and guides you towards the standard pattern of using condvars:</span></p>
<p><span>First, you lock the mutex around the shared state. Then, you inspect whether the state is what you</span>
<span>need. If that</span>&rsquo;<span>s the case, great, you do what you wanted to do and unlock the mutex. If not, then,</span>
<em><span>while still holding the mutex</span></em><span>, you </span><em><span>wait</span></em><span> on the condition variable. Which means that the</span>
<span>mutex gets unlocked, and other threads get the chance to change the shared state. When they do</span>
<span>change it, and notify the condvar, your thread wakes up, and it gets the locked mutex back (but the</span>
<span>state now is different). Due to the possibility of spurious wake-ups, you need to double check the</span>
<span>state and be ready to loop back again to waiting.</span></p>
<p><span>Naturally, there</span>&rsquo;<span>s a helper that encapsulates this whole pattern:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">SharedContext</span> {</span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">pause</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Running);</span>
<span class="line">    *guard = State::Paused;</span>
<span class="line">    <span class="hl-keyword">self</span>.cv.<span class="hl-title function_ invoke__">notify_all</span>();</span>
<span class="line">    guard = <span class="hl-keyword">self</span></span>
<span class="line">      .cv</span>
<span class="line">      .<span class="hl-title function_ invoke__">wait_while</span>(guard, |state| *state == State::Paused)</span>
<span class="line">      .<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Running)</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Ok, this actually does look like a reasonable implementation of </span><code>pause</code><span>. Let</span>&rsquo;<span>s move on to</span>
<code>managed_thread::spawn</code><span>:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">spawn</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T: <span class="hl-symbol">&#x27;scope</span> + <span class="hl-built_in">Send</span>&gt;(</span>
<span class="line">  scope: &amp;Scope&lt;<span class="hl-symbol">&#x27;scope</span>, <span class="hl-symbol">&#x27;_</span>&gt;,</span>
<span class="line">  state: T,</span>
<span class="line">) {</span>
<span class="line">  <span class="hl-comment">// ? ? ?? ??? ?????</span></span>
<span class="line">}</span></code></pre>

</figure>
<p><span>There</span>&rsquo;<span>s a bunch of stuff that needs to happen here:</span></p>
<ul>
<li>
<span>As we have established, we are going to spawn a (scoped) thread, so we need the </span><code>scope</code><span> parameter</span>
<span>with its three lifetimes. I don</span>&rsquo;<span>t know how it works, so I am just going by the docs here!</span>
</li>
<li>
<span>We are going to return some kind of handle, which we can use to pause and unpause our managed</span>
<span>thread. And that handle is going to be parametrized over the same </span><code>'scope</code><span> lifetime, because it</span>&rsquo;<span>ll</span>
<span>hold onto the actual join handle.</span>
</li>
<li>
<span>We are going to pass the generic state to our new thread, and that state needs to be </span><code>Send</code><span>, and</span>
<span>bounded by the same lifetime as our scoped thread.</span>
</li>
<li>
<span>Inside, we are going to spawn a thread for sure, and we</span>&rsquo;<span>ll need to setup the </span><code>INSTANCE</code><span> thread</span>
<span>local on that thread.</span>
</li>
<li>
<span>And it would actually be a good idea to stuff a reference to that </span><code>SharedContext</code><span> into the handle</span>
<span>we return.</span>
</li>
</ul>
<p><span>A bunch of stuff, in other words. Let</span>&rsquo;<span>s do it:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">struct</span> <span class="hl-title class_">ManagedHandle</span>&lt;<span class="hl-symbol">&#x27;scope</span>&gt; {</span>
<span class="line">  inner: std::thread::ScopedJoinHandle&lt;<span class="hl-symbol">&#x27;scope</span>, ()&gt;,</span>
<span class="line">  ctx: Arc&lt;SharedContext&gt;,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">spawn</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T: <span class="hl-symbol">&#x27;scope</span> + <span class="hl-built_in">Send</span>&gt;(</span>
<span class="line">  scope: &amp;<span class="hl-symbol">&#x27;scope</span> Scope&lt;<span class="hl-symbol">&#x27;scope</span>, <span class="hl-symbol">&#x27;_</span>&gt;,</span>
<span class="line">  state: T,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> ManagedHandle&lt;<span class="hl-symbol">&#x27;scope</span>&gt; {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">ctx</span>: Arc&lt;SharedContext&gt; = <span class="hl-built_in">Default</span>::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">inner</span> = scope.<span class="hl-title function_ invoke__">spawn</span>({</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">ctx</span> = Arc::<span class="hl-title function_ invoke__">clone</span>(&amp;ctx);</span>
<span class="line">    <span class="hl-keyword">move</span> || {</span>
<span class="line">      SharedContext::<span class="hl-title function_ invoke__">set</span>(ctx);</span>
<span class="line">      <span class="hl-title function_ invoke__">drop</span>(state); <span class="hl-comment">// <span class="hl-doctag">TODO:</span> ¿</span></span>
<span class="line">    }</span>
<span class="line">  });</span>
<span class="line">  ManagedHandle { inner, ctx }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>The essentially no-op function we spawn looks sus. We</span>&rsquo;<span>ll fix later! Let</span>&rsquo;<span>s try to implement</span>
<code>is_paused</code><span> and </span><code>unpause</code><span> first! They should be relatively straightforward. For </span><code>is_paused</code><span>, we just</span>
<span>need to lock the mutex and check the state:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">ManagedHandle</span>&lt;<span class="hl-symbol">&#x27;_</span>&gt; {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">is_paused</span>(&amp;<span class="hl-keyword">self</span>,) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">bool</span> {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    *guard == State::Paused</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>For </span><code>unpause</code><span>, we should additionally flip the state back to </span><code>Running</code><span> and notify the other thread:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">ManagedHandle</span>&lt;<span class="hl-symbol">&#x27;_</span>&gt; {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">unpause</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Paused);</span>
<span class="line">    *guard = State::Running;</span>
<span class="line">    <span class="hl-keyword">self</span>.ctx.cv.<span class="hl-title function_ invoke__">notify_all</span>();</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>But I think that</span>&rsquo;<span>s not quite correct. Can you see why?</span></p>
<p><span>With this implementation, after </span><code>unpause</code><span>, the controlling and the managed threads will be running</span>
<span>concurrently. And that can lead to non-determinism, the very problem we are trying to avoid here! In</span>
<span>particular, if you call </span><code>is_paused</code><span> </span><em><span>right</span></em><span> after you </span><code>unpause</code><span> the thread, you</span>&rsquo;<span>ll most likely get</span>
<code>false</code><span> back, as the other thread will still be running. But it might also hit the </span><em><span>next</span></em><span> </span><code>pause</code>
<span>call, so, depending on timing, you might also get </span><code>true</code><span>.</span></p>
<p><span>What we want is actually completely eliminating all unmanaged concurrency. That means that at any</span>
<span>given point in time, only one thread (controlling or managed) should be running. So the right</span>
<span>semantics for </span><code>unpause</code><span> is to unblock the managed thread, and then block the controlling thread</span>
<span>until the managed one hits the next pause!</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">ManagedHandle</span>&lt;<span class="hl-symbol">&#x27;_</span>&gt; {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">unpause</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Paused);</span>
<span class="line">    *guard = State::Running;</span>
<span class="line">    <span class="hl-keyword">self</span>.ctx.cv.<span class="hl-title function_ invoke__">notify_all</span>();</span>
<span class="line">    guard = <span class="hl-keyword">self</span></span>
<span class="line">      .ctx</span>
<span class="line">      .cv</span>
<span class="line">      .<span class="hl-title function_ invoke__">wait_while</span>(guard, |state| *state == State::Running)</span>
<span class="line">      .<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>At this point we can spawn a managed thread, pause it and resume. But right now it doesn</span>&rsquo;<span>t do</span>
<span>anything. Next step is implementing that idea where the controlling thread can directly send an</span>
<span>arbitrary closure to the managed one to make it do something:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span>&lt;<span class="hl-symbol">&#x27;scope</span>&gt; ManagedHandle&lt;<span class="hl-symbol">&#x27;scope</span>&gt; {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">submit</span>&lt;F: FnSomething&gt;(&amp;<span class="hl-keyword">self</span>, f: F)</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Let</span>&rsquo;<span>s figure this </span><code>FnSomething</code><span> bound! We are going to yeet this </span><code>f</code><span> over to the managed thread and</span>
<span>run it there once, so it is </span><code>FnOnce</code><span>. It is crossing thread-boundary, so it needs to be </span><code>+ Send</code><span>.</span>
<span>And, because we are using scoped threads, it </span><em><span>doesn</span>&rsquo;<span>t</span></em><span> have to be </span><code>'static</code><span>, just </span><code>'scope</code><span> is</span>
<span>enough. Moreover, in that managed thread the </span><code>f</code><span> will have exclusive access to thread</span>&rsquo;<span>s state, </span><code>T</code><span>.</span>
<span>So we have:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span>&lt;<span class="hl-symbol">&#x27;scope</span>&gt; ManagedHandle&lt;<span class="hl-symbol">&#x27;scope</span>&gt; {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">submit</span>&lt;F: <span class="hl-title function_ invoke__">FnOnce</span>(&amp;<span class="hl-keyword">mut</span> T) + <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;scope</span>&gt;(<span class="hl-keyword">self</span>, f: F)</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Implementing this is a bit tricky. First, we</span>&rsquo;<span>ll need some sort of the channel to actually move the</span>
<span>function. Then, similarly to the </span><code>unpause</code><span> logic, we</span>&rsquo;<span>ll need synchronization to make sure that the</span>
<span>control thread doesn</span>&rsquo;<span>t resume until the managed thread starts running </span><code>f</code><span> and hits a pause (or maybe</span>
<span>completes </span><code>f</code><span>). And we</span>&rsquo;<span>ll also need a new state, </span><code>Ready</code><span>, because now there are two different</span>
<span>reasons why a managed thread might be blocked </span>&mdash;<span> it might wait for an </span><code>unpause</code><span> event, or it might</span>
<span>wait for the next </span><code>f</code><span> to execute. This is the new code:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[derive(Default)]</span></span>
<span class="line"><span class="hl-keyword">enum</span> <span class="hl-title class_">State</span> {</span>
<span class="line hl-line">  <span class="hl-meta">#[default]</span></span>
<span class="line hl-line">  Ready,</span>
<span class="line">  Running,</span>
<span class="line">  Paused,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line hl-line"><span class="hl-keyword">struct</span> <span class="hl-title class_">ManagedHandle</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; {</span>
<span class="line">  inner: std::thread::ScopedJoinHandle&lt;<span class="hl-symbol">&#x27;scope</span>, ()&gt;,</span>
<span class="line">  ctx: Arc&lt;SharedContext&gt;,</span>
<span class="line hl-line">  sender: mpsc::Sender&lt;<span class="hl-type">Box</span>&lt;<span class="hl-keyword">dyn</span> <span class="hl-title function_ invoke__">FnOnce</span>(&amp;<span class="hl-keyword">mut</span> T) + <span class="hl-symbol">&#x27;scope</span> + <span class="hl-built_in">Send</span>&gt;&gt;,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">spawn</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T: <span class="hl-symbol">&#x27;scope</span> + <span class="hl-built_in">Send</span>&gt;(</span>
<span class="line">  scope: &amp;<span class="hl-symbol">&#x27;scope</span> Scope&lt;<span class="hl-symbol">&#x27;scope</span>, <span class="hl-symbol">&#x27;_</span>&gt;,</span>
<span class="line">  <span class="hl-keyword">mut</span> state: T,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> ManagedHandle&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">ctx</span>: Arc&lt;SharedContext&gt; = <span class="hl-built_in">Default</span>::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line hl-line">  <span class="hl-keyword">let</span> (sender, receiver) =</span>
<span class="line hl-line">    mpsc::channel::&lt;<span class="hl-type">Box</span>&lt;<span class="hl-keyword">dyn</span> <span class="hl-title function_ invoke__">FnOnce</span>(&amp;<span class="hl-keyword">mut</span> T) + <span class="hl-symbol">&#x27;scope</span> + <span class="hl-built_in">Send</span>&gt;&gt;();</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">inner</span> = scope.<span class="hl-title function_ invoke__">spawn</span>({</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">ctx</span> = Arc::<span class="hl-title function_ invoke__">clone</span>(&amp;ctx);</span>
<span class="line">    <span class="hl-keyword">move</span> || {</span>
<span class="line">      SharedContext::<span class="hl-title function_ invoke__">set</span>(Arc::<span class="hl-title function_ invoke__">clone</span>(&amp;ctx));</span>
<span class="line"></span>
<span class="line hl-line">      <span class="hl-keyword">for</span> <span class="hl-variable">f</span> <span class="hl-keyword">in</span> receiver {</span>
<span class="line hl-line">        <span class="hl-title function_ invoke__">f</span>(&amp;<span class="hl-keyword">mut</span> state);</span>
<span class="line hl-line"></span>
<span class="line hl-line">        <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line hl-line">        <span class="hl-built_in">assert_eq!</span>(*guard, State::Running);</span>
<span class="line hl-line">        *guard = State::Ready;</span>
<span class="line hl-line">        ctx.cv.<span class="hl-title function_ invoke__">notify_all</span>()</span>
<span class="line hl-line">      }</span>
<span class="line">    }</span>
<span class="line">  });</span>
<span class="line">  ManagedHandle { inner, ctx, sender }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; ManagedHandle&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">submit</span>&lt;F: <span class="hl-title function_ invoke__">FnOnce</span>(&amp;<span class="hl-keyword">mut</span> T) + <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;scope</span>&gt;(&amp;<span class="hl-keyword">self</span>, f: F) {</span>
<span class="line hl-line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line hl-line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Ready);</span>
<span class="line hl-line">    *guard = State::Running;</span>
<span class="line hl-line">    <span class="hl-keyword">self</span>.sender.<span class="hl-title function_ invoke__">send</span>(<span class="hl-type">Box</span>::<span class="hl-title function_ invoke__">new</span>(f)).<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line hl-line">    guard = <span class="hl-keyword">self</span></span>
<span class="line hl-line">      .ctx</span>
<span class="line hl-line">      .cv</span>
<span class="line hl-line">      .<span class="hl-title function_ invoke__">wait_while</span>(guard, |state| *state == State::Running)</span>
<span class="line hl-line">      .<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>The last small piece of the puzzle is the </span><code>join</code><span> function. It</span>&rsquo;<span>s </span><em><span>almost</span></em><span> standard! First we close</span>
<span>our side of the channel. This serves as a natural stop signal for the other thread, so it exits.</span>
<span>Which in turn allows us to join it. The small wrinkle here is that the thread might be paused when</span>
<span>we try to join it, so we need to unpause it beforehand:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; ManagedHandle&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">join</span>(<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">while</span> <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">is_paused</span>() {</span>
<span class="line">      <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">unpause</span>();</span>
<span class="line">    }</span>
<span class="line">    <span class="hl-title function_ invoke__">drop</span>(<span class="hl-keyword">self</span>.sender);</span>
<span class="line">    <span class="hl-keyword">self</span>.inner.<span class="hl-title function_ invoke__">join</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>That</span>&rsquo;<span>s it! Let</span>&rsquo;<span>s put everything together!</span></p>
<p><span>Helper library, </span><code>managed_thread.rs</code><span>:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">use</span> std::{</span>
<span class="line">  cell::RefCell,</span>
<span class="line">  sync::{atomic::Ordering, mpsc, Arc, Condvar, Mutex},</span>
<span class="line">  thread::Scope,</span>
<span class="line">};</span>
<span class="line"></span>
<span class="line"><span class="hl-meta">#[derive(Default)]</span></span>
<span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">struct</span> <span class="hl-title class_">AtomicU32</span> {</span>
<span class="line">  inner: std::sync::atomic::AtomicU32,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">AtomicU32</span> {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">load</span>(&amp;<span class="hl-keyword">self</span>, ordering: Ordering) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">u32</span> {</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">result</span> = <span class="hl-keyword">self</span>.inner.<span class="hl-title function_ invoke__">load</span>(ordering);</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">    result</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">store</span>(&amp;<span class="hl-keyword">self</span>, value: <span class="hl-type">u32</span>, ordering: Ordering) {</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">    <span class="hl-keyword">self</span>.inner.<span class="hl-title function_ invoke__">store</span>(value, ordering);</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">  }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">pause</span>() {</span>
<span class="line">  <span class="hl-keyword">if</span> <span class="hl-keyword">let</span> <span class="hl-variable">Some</span>(ctx) = SharedContext::<span class="hl-title function_ invoke__">get</span>() {</span>
<span class="line">    ctx.<span class="hl-title function_ invoke__">pause</span>()</span>
<span class="line">  }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-meta">#[derive(Default)]</span></span>
<span class="line"><span class="hl-keyword">struct</span> <span class="hl-title class_">SharedContext</span> {</span>
<span class="line">  state: Mutex&lt;State&gt;,</span>
<span class="line">  cv: Condvar,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-meta">#[derive(Default, PartialEq, Eq, Debug)]</span></span>
<span class="line"><span class="hl-keyword">enum</span> <span class="hl-title class_">State</span> {</span>
<span class="line">  <span class="hl-meta">#[default]</span></span>
<span class="line">  Ready,</span>
<span class="line">  Running,</span>
<span class="line">  Paused,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line">thread_local! {</span>
<span class="line">  <span class="hl-keyword">static</span> INSTANCE: RefCell&lt;<span class="hl-type">Option</span>&lt;Arc&lt;SharedContext&gt;&gt;&gt; =</span>
<span class="line">    RefCell::<span class="hl-title function_ invoke__">new</span>(<span class="hl-literal">None</span>);</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">SharedContext</span> {</span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">set</span>(ctx: Arc&lt;SharedContext&gt;) {</span>
<span class="line">    INSTANCE.<span class="hl-title function_ invoke__">with</span>(|it| *it.<span class="hl-title function_ invoke__">borrow_mut</span>() = <span class="hl-title function_ invoke__">Some</span>(ctx));</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">get</span>() <span class="hl-punctuation">-&gt;</span> <span class="hl-type">Option</span>&lt;Arc&lt;SharedContext&gt;&gt; {</span>
<span class="line">    INSTANCE.<span class="hl-title function_ invoke__">with</span>(|it| it.<span class="hl-title function_ invoke__">borrow</span>().<span class="hl-title function_ invoke__">clone</span>())</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">pause</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Running);</span>
<span class="line">    *guard = State::Paused;</span>
<span class="line">    <span class="hl-keyword">self</span>.cv.<span class="hl-title function_ invoke__">notify_all</span>();</span>
<span class="line">    guard = <span class="hl-keyword">self</span></span>
<span class="line">      .cv</span>
<span class="line">      .<span class="hl-title function_ invoke__">wait_while</span>(guard, |state| *state == State::Paused)</span>
<span class="line">      .<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Running)</span>
<span class="line">  }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">struct</span> <span class="hl-title class_">ManagedHandle</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; {</span>
<span class="line">  inner: std::thread::ScopedJoinHandle&lt;<span class="hl-symbol">&#x27;scope</span>, ()&gt;,</span>
<span class="line">  sender: mpsc::Sender&lt;<span class="hl-type">Box</span>&lt;<span class="hl-keyword">dyn</span> <span class="hl-title function_ invoke__">FnOnce</span>(&amp;<span class="hl-keyword">mut</span> T) + <span class="hl-symbol">&#x27;scope</span> + <span class="hl-built_in">Send</span>&gt;&gt;,</span>
<span class="line">  ctx: Arc&lt;SharedContext&gt;,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">spawn</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T: <span class="hl-symbol">&#x27;scope</span> + <span class="hl-built_in">Send</span>&gt;(</span>
<span class="line">  scope: &amp;<span class="hl-symbol">&#x27;scope</span> Scope&lt;<span class="hl-symbol">&#x27;scope</span>, <span class="hl-symbol">&#x27;_</span>&gt;,</span>
<span class="line">  <span class="hl-keyword">mut</span> state: T,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> ManagedHandle&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">ctx</span>: Arc&lt;SharedContext&gt; = <span class="hl-built_in">Default</span>::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line">  <span class="hl-keyword">let</span> (sender, receiver) =</span>
<span class="line">    mpsc::channel::&lt;<span class="hl-type">Box</span>&lt;<span class="hl-keyword">dyn</span> <span class="hl-title function_ invoke__">FnOnce</span>(&amp;<span class="hl-keyword">mut</span> T) + <span class="hl-symbol">&#x27;scope</span> + <span class="hl-built_in">Send</span>&gt;&gt;();</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">inner</span> = scope.<span class="hl-title function_ invoke__">spawn</span>({</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">ctx</span> = Arc::<span class="hl-title function_ invoke__">clone</span>(&amp;ctx);</span>
<span class="line">    <span class="hl-keyword">move</span> || {</span>
<span class="line">      SharedContext::<span class="hl-title function_ invoke__">set</span>(Arc::<span class="hl-title function_ invoke__">clone</span>(&amp;ctx));</span>
<span class="line">      <span class="hl-keyword">for</span> <span class="hl-variable">f</span> <span class="hl-keyword">in</span> receiver {</span>
<span class="line">        <span class="hl-title function_ invoke__">f</span>(&amp;<span class="hl-keyword">mut</span> state);</span>
<span class="line">        <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">        <span class="hl-built_in">assert_eq!</span>(*guard, State::Running);</span>
<span class="line">        *guard = State::Ready;</span>
<span class="line">        ctx.cv.<span class="hl-title function_ invoke__">notify_all</span>()</span>
<span class="line">      }</span>
<span class="line">    }</span>
<span class="line">  });</span>
<span class="line">  ManagedHandle { inner, ctx, sender }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span>&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; ManagedHandle&lt;<span class="hl-symbol">&#x27;scope</span>, T&gt; {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">is_paused</span>(&amp;<span class="hl-keyword">self</span>) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">bool</span> {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    *guard == State::Paused</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">unpause</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Paused);</span>
<span class="line">    *guard = State::Running;</span>
<span class="line">    <span class="hl-keyword">self</span>.ctx.cv.<span class="hl-title function_ invoke__">notify_all</span>();</span>
<span class="line">    guard = <span class="hl-keyword">self</span></span>
<span class="line">      .ctx</span>
<span class="line">      .cv</span>
<span class="line">      .<span class="hl-title function_ invoke__">wait_while</span>(guard, |state| *state == State::Running)</span>
<span class="line">      .<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">submit</span>&lt;F: <span class="hl-title function_ invoke__">FnOnce</span>(&amp;<span class="hl-keyword">mut</span> T) + <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;scope</span>&gt;(&amp;<span class="hl-keyword">self</span>, f: F) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">guard</span> = <span class="hl-keyword">self</span>.ctx.state.<span class="hl-title function_ invoke__">lock</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    <span class="hl-built_in">assert_eq!</span>(*guard, State::Ready);</span>
<span class="line">    *guard = State::Running;</span>
<span class="line">    <span class="hl-keyword">self</span>.sender.<span class="hl-title function_ invoke__">send</span>(<span class="hl-type">Box</span>::<span class="hl-title function_ invoke__">new</span>(f)).<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">    guard = <span class="hl-keyword">self</span></span>
<span class="line">      .ctx</span>
<span class="line">      .cv</span>
<span class="line">      .<span class="hl-title function_ invoke__">wait_while</span>(guard, |state| *state == State::Running)</span>
<span class="line">      .<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">join</span>(<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">while</span> <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">is_paused</span>() {</span>
<span class="line">      <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">unpause</span>();</span>
<span class="line">    }</span>
<span class="line">    <span class="hl-title function_ invoke__">drop</span>(<span class="hl-keyword">self</span>.sender);</span>
<span class="line">    <span class="hl-keyword">self</span>.inner.<span class="hl-title function_ invoke__">join</span>().<span class="hl-title function_ invoke__">unwrap</span>();</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>System under test, not-exactly-atomic counter:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">use</span> std::sync::atomic::Ordering::SeqCst;</span>
<span class="line"></span>
<span class="line"><span class="hl-meta">#[cfg(test)]</span></span>
<span class="line"><span class="hl-keyword">use</span> managed_thread::AtomicU32;</span>
<span class="line"><span class="hl-meta">#[cfg(not(test))]</span></span>
<span class="line"><span class="hl-keyword">use</span> std::sync::atomic::AtomicU32;</span>
<span class="line"></span>
<span class="line"><span class="hl-meta">#[derive(Default)]</span></span>
<span class="line"><span class="hl-keyword">pub</span> <span class="hl-keyword">struct</span> <span class="hl-title class_">Counter</span> {</span>
<span class="line">  value: AtomicU32,</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">Counter</span> {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">increment</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">value</span> = <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">load</span>(SeqCst);</span>
<span class="line">    <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">store</span>(value + <span class="hl-number">1</span>, SeqCst);</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">get</span>(&amp;<span class="hl-keyword">self</span>) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">u32</span> {</span>
<span class="line">    <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">load</span>(SeqCst)</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>And the test itself:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[test]</span></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">test_counter</span>() {</span>
<span class="line">  arbtest::<span class="hl-title function_ invoke__">arbtest</span>(|rng| {</span>
<span class="line">    eprintln!(<span class="hl-string">&quot;begin trace&quot;</span>);</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">counter</span> = Counter::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">counter_model</span>: <span class="hl-type">u32</span> = <span class="hl-number">0</span>;</span>
<span class="line"></span>
<span class="line">    std::thread::<span class="hl-title function_ invoke__">scope</span>(|scope| {</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">t1</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(scope, &amp;counter);</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">t2</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(scope, &amp;counter);</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">threads</span> = [t1, t2];</span>
<span class="line"></span>
<span class="line">      <span class="hl-keyword">while</span> !rng.<span class="hl-title function_ invoke__">is_empty</span>() {</span>
<span class="line">        <span class="hl-title function_ invoke__">for</span> (tid, t) <span class="hl-keyword">in</span> threads.<span class="hl-title function_ invoke__">iter_mut</span>().<span class="hl-title function_ invoke__">enumerate</span>() {</span>
<span class="line">          <span class="hl-keyword">if</span> rng.<span class="hl-title function_ invoke__">arbitrary</span>()? {</span>
<span class="line">            <span class="hl-keyword">if</span> t.<span class="hl-title function_ invoke__">is_paused</span>() {</span>
<span class="line">              eprintln!(<span class="hl-string">&quot;{tid}: unpause&quot;</span>);</span>
<span class="line">              t.<span class="hl-title function_ invoke__">unpause</span>()</span>
<span class="line">            } <span class="hl-keyword">else</span> {</span>
<span class="line">              eprintln!(<span class="hl-string">&quot;{tid}: increment&quot;</span>);</span>
<span class="line">              t.<span class="hl-title function_ invoke__">submit</span>(|c| c.<span class="hl-title function_ invoke__">increment</span>());</span>
<span class="line">              counter_model += <span class="hl-number">1</span>;</span>
<span class="line">            }</span>
<span class="line">          }</span>
<span class="line">        }</span>
<span class="line">      }</span>
<span class="line"></span>
<span class="line">      <span class="hl-keyword">for</span> <span class="hl-variable">t</span> <span class="hl-keyword">in</span> threads {</span>
<span class="line">        t.<span class="hl-title function_ invoke__">join</span>();</span>
<span class="line">      }</span>
<span class="line">      <span class="hl-built_in">assert_eq!</span>(counter_model, counter.<span class="hl-title function_ invoke__">get</span>());</span>
<span class="line"></span>
<span class="line">      <span class="hl-title function_ invoke__">Ok</span>(())</span>
<span class="line">    })</span>
<span class="line">  });</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Running it identifies a failure:</span></p>

<figure class="code-block">


<pre><code><span class="line">---- test_counter stdout ----</span>
<span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">0: increment</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">thread 'test_counter' panicked at src/lib.rs:56:7:</span>
<span class="line">assertion `left == right` failed</span>
<span class="line">  left: 4</span>
<span class="line"> right: 3</span>
<span class="line"></span>
<span class="line">arbtest failed!</span>
<span class="line">    Seed: 0x4fd7ddff00000020</span></code></pre>

</figure>
<p><span>Which </span>&hellip;<span> is something we got like 5% into this article already, with normal threads! But there</span>&rsquo;<span>s</span>
<span>more to this failure. First, it is reproducible. If I specify the same seed, I get the </span><em><span>exact</span></em><span> same</span>
<span>interleaving:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[test]</span></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">test_counter</span>() {</span>
<span class="line">  arbtest::<span class="hl-title function_ invoke__">arbtest</span>(|rng| {</span>
<span class="line">    eprintln!(<span class="hl-string">&quot;begin trace&quot;</span>);</span>
<span class="line">    ...</span>
<span class="line">  })</span>
<span class="line hl-line">    .<span class="hl-title function_ invoke__">seed</span>(<span class="hl-number">0x71aafcd900000020</span>);</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>And this is completely machine independent! If </span><em><span>you</span></em><span> specify this seed, you</span>&rsquo;<span>ll get exact same</span>
<span>interleaving. So, if I am having trouble debugging this, I can DM you this hex in Zulip, and</span>
<span>you</span>&rsquo;<span>ll be able to help out!</span></p>
<p><span>But there</span>&rsquo;<span>s more </span>&mdash;<span> we don</span>&rsquo;<span>t need to debug this failure, we can minimize it!</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[test]</span></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">test_counter</span>() {</span>
<span class="line">  arbtest::<span class="hl-title function_ invoke__">arbtest</span>(|rng| {</span>
<span class="line">    eprintln!(<span class="hl-string">&quot;begin trace&quot;</span>);</span>
<span class="line">    ...</span>
<span class="line">  })</span>
<span class="line">    .<span class="hl-title function_ invoke__">seed</span>(<span class="hl-number">0x71aafcd900000020</span>)</span>
<span class="line hl-line">    .<span class="hl-title function_ invoke__">minimize</span>();</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>This gives me the following minimization trace:</span></p>

<figure class="code-block">


<pre><code><span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">0: increment</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">seed 0x4fd7ddff00000020, seed size 32, search time 106.00ns</span>
<span class="line"></span>
<span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">1: unpause</span>
<span class="line">1: increment</span>
<span class="line">seed 0x540c0c1c00000010, seed size 16, search time 282.16µs</span>
<span class="line"></span>
<span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">1: unpause</span>
<span class="line">1: unpause</span>
<span class="line">seed 0x084ca71200000008, seed size 8, search time 805.74µs</span>
<span class="line"></span>
<span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">seed 0x5699b19400000004, seed size 4, search time 1.44ms</span>
<span class="line"></span>
<span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">seed 0x4bb0ea5c00000002, seed size 2, search time 4.03ms</span>
<span class="line"></span>
<span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">seed 0x9c2a13a600000001, seed size 1, search time 4.31ms</span>
<span class="line"></span>
<span class="line">minimized</span>
<span class="line">seed 0x9c2a13a600000001, seed size 1, search time 100.03ms</span></code></pre>

</figure>
<p><span>That is, we ended up with this tiny, minimal example:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[test]</span></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">test_counter</span>() {</span>
<span class="line">  arbtest::<span class="hl-title function_ invoke__">arbtest</span>(|rng| {</span>
<span class="line">    eprintln!(<span class="hl-string">&quot;begin trace&quot;</span>);</span>
<span class="line">    ...</span>
<span class="line">  })</span>
<span class="line hl-line">    .<span class="hl-title function_ invoke__">seed</span>(<span class="hl-number">0x9c2a13a600000001</span>);</span>
<span class="line">}</span></code></pre>

</figure>

<figure class="code-block">


<pre><code><span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span></code></pre>

</figure>
<p><span>And </span><em><span>this</span></em><span> is how you properly test concurrent data structures.</span></p>
</section>
<section id="Postscript">

    <h2>
    <a href="#Postscript"><span>Postscript</span> </a>
    </h2>
<p><span>Of course, this is just a toy. But you can see some ways to extend it. For example, right now our</span>
<code>AtomicU32</code><span> just delegates to the real one. But what you </span><em><span>could</span></em><span> do instead is, for each atomic, to</span>
<span>maintain a set of values written and, on read, return an </span><em><span>arbitrary</span></em><span> written value consistent with a</span>
<span>weak memory model.</span></p>
<p><span>You could also be smarter with exploring interleavings. Instead of interleaving threads at random,</span>
<span>like we do here, you can try to apply model checking approaches and prove that you have considered</span>
<span>all meaningfully different interleavings.</span></p>
<p><span>Or you can apply the approach from </span><a href="https://matklad.github.io/2021/11/07/generate-all-the-things.html"><em><span>Generate All The</span>
<span>Things</span></em></a><span> and exhaustively</span>
<span>enumerate </span><em><span>all</span></em><span> interleavings for up to, say, five increments. In fact, why don</span>&rsquo;<span>t we just do this?</span></p>
<p><code class="display">$ cargo add exhaustigen</code></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-meta">#[test]</span></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">exhaustytest</span>() {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">g</span> = exhaustigen::Gen::<span class="hl-title function_ invoke__">new</span>();</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">interleavings_count</span> = <span class="hl-number">0</span>;</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">while</span> !g.<span class="hl-title function_ invoke__">done</span>() {</span>
<span class="line">    interleavings_count += <span class="hl-number">1</span>;</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">counter</span> = Counter::<span class="hl-title function_ invoke__">default</span>();</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-keyword">mut </span><span class="hl-variable">counter_model</span>: <span class="hl-type">u32</span> = <span class="hl-number">0</span>;</span>
<span class="line"></span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">increment_count</span> = g.<span class="hl-title function_ invoke__">gen</span>(<span class="hl-number">5</span>) <span class="hl-keyword">as</span> <span class="hl-type">u32</span>;</span>
<span class="line">    std::thread::<span class="hl-title function_ invoke__">scope</span>(|scope| {</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">t1</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(scope, &amp;counter);</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">t2</span> = managed_thread::<span class="hl-title function_ invoke__">spawn</span>(scope, &amp;counter);</span>
<span class="line"></span>
<span class="line">      <span class="hl-symbol">&#x27;outer</span>: <span class="hl-keyword">while</span> t1.<span class="hl-title function_ invoke__">is_paused</span>()</span>
<span class="line">        || t2.<span class="hl-title function_ invoke__">is_paused</span>()</span>
<span class="line">        || counter_model &lt; increment_count</span>
<span class="line">      {</span>
<span class="line">        <span class="hl-keyword">for</span> <span class="hl-variable">t</span> <span class="hl-keyword">in</span> [&amp;t1, &amp;t2] {</span>
<span class="line">          <span class="hl-keyword">if</span> g.<span class="hl-title function_ invoke__">flip</span>() {</span>
<span class="line">            <span class="hl-keyword">if</span> t.<span class="hl-title function_ invoke__">is_paused</span>() {</span>
<span class="line">              t.<span class="hl-title function_ invoke__">unpause</span>();</span>
<span class="line">              <span class="hl-keyword">continue</span> <span class="hl-symbol">&#x27;outer</span>;</span>
<span class="line">            }</span>
<span class="line">            <span class="hl-keyword">if</span> counter_model &lt; increment_count {</span>
<span class="line">              t.<span class="hl-title function_ invoke__">submit</span>(|c| c.<span class="hl-title function_ invoke__">increment</span>());</span>
<span class="line">              counter_model += <span class="hl-number">1</span>;</span>
<span class="line">              <span class="hl-keyword">continue</span> <span class="hl-symbol">&#x27;outer</span>;</span>
<span class="line">            }</span>
<span class="line">          }</span>
<span class="line">        }</span>
<span class="line">        <span class="hl-keyword">return</span> <span class="hl-keyword">for</span> <span class="hl-variable">t</span> <span class="hl-keyword">in</span> [t1, t2] {</span>
<span class="line">          t.<span class="hl-title function_ invoke__">join</span>()</span>
<span class="line">        };</span>
<span class="line">      }</span>
<span class="line"></span>
<span class="line">      <span class="hl-built_in">assert_eq!</span>(counter_model, counter.<span class="hl-title function_ invoke__">get</span>());</span>
<span class="line">    });</span>
<span class="line">  }</span>
<span class="line">  eprintln!(<span class="hl-string">&quot;interleavings_count = {:?}&quot;</span>, interleavings_count);</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>The shape of the test is more or less the same, except that we need to make sure that there are no</span>
&ldquo;<span>dummy</span>&rdquo;<span> iterations, and that we always either unpause a thread or submit an increment.</span></p>
<p><span>It finds the same bug, naturally:</span></p>

<figure class="code-block">


<pre><code><span class="line">thread 'exhaustytest' panicked at src/lib.rs:103:7:</span>
<span class="line">assertion `left == right` failed</span>
<span class="line">  left: 2</span>
<span class="line"> right: 1</span></code></pre>

</figure>
<p><span>But the cool thing is, if we fix the issue by using atomic increment, </span>&hellip;</p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">AtomicU32</span> {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">fetch_add</span>(</span>
<span class="line">    &amp;<span class="hl-keyword">self</span>,</span>
<span class="line">    value: <span class="hl-type">u32</span>,</span>
<span class="line">    ordering: Ordering,</span>
<span class="line">  ) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">u32</span> {</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">result</span> = <span class="hl-keyword">self</span>.inner.<span class="hl-title function_ invoke__">fetch_add</span>(value, ordering);</span>
<span class="line">    <span class="hl-title function_ invoke__">pause</span>();</span>
<span class="line">    result</span>
<span class="line">  }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">Counter</span> {</span>
<span class="line">  <span class="hl-keyword">pub</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">increment</span>(&amp;<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">self</span>.value.<span class="hl-title function_ invoke__">fetch_add</span>(<span class="hl-number">1</span>, SeqCst);</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p>&hellip;<span> we can get a rather specific correctness statements out of our test, that </span><em><span>any</span></em><span> sequence of at</span>
<span>most five increments is correct:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-title function_">$</span> t cargo t -r -- exhaustytest --nocapture</span>
<span class="line"><span class="hl-output">running 1 test</span></span>
<span class="line"><span class="hl-output">all 81133 interleavings are fine!</span></span>
<span class="line"><span class="hl-output">test exhaustytest ... ok</span></span>
<span class="line"><span class="hl-output"></span></span>
<span class="line"><span class="hl-output">real 8.65s</span></span>
<span class="line"><span class="hl-output">cpu  8.16s (2.22s user + 5.94s sys)</span></span>
<span class="line"><span class="hl-output">rss  63.91mb</span></span></code></pre>

</figure>
<p><span>And the last small thing. Recall that our PBT minimized the first sequence it found </span>&hellip;<span>:</span></p>

<figure class="code-block">


<pre><code><span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">0: increment</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">0: unpause</span>
<span class="line">thread 'test_counter' panicked at src/lib.rs:56:7:</span>
<span class="line">assertion `left == right` failed</span>
<span class="line">  left: 4</span>
<span class="line"> right: 3</span>
<span class="line"></span>
<span class="line">arbtest failed!</span>
<span class="line">    Seed: 0x4fd7ddff00000020</span></code></pre>

</figure>
<p>&hellip;<span> down to just</span></p>

<figure class="code-block">


<pre><code><span class="line">begin trace</span>
<span class="line">0: increment</span>
<span class="line">1: increment</span>
<span class="line">0: unpause</span>
<span class="line">1: unpause</span>
<span class="line">thread 'test_counter' panicked at src/lib.rs:57:7:</span>
<span class="line">assertion `left == right` failed</span>
<span class="line">  left: 2</span>
<span class="line"> right: 1</span>
<span class="line"></span>
<span class="line">arbtest failed!</span>
<span class="line">    Seed: 0x9c2a13a600000001</span></code></pre>

</figure>
<p><span>But we never implemented shrinking! How is this possible? Well, strictly speaking, this is out of</span>
<span>scope for this post. And I</span>&rsquo;<span>ve already described this</span>
<a href="https://tigerbeetle.com/blog/2023-03-28-random-fuzzy-thoughts"><span>elsewhere</span></a><span>. And, at 32k, this is the</span>
<span>third-longest post on this blog. And it</span>&rsquo;<span>s 3AM here in Lisbon right now. But of course I</span>&rsquo;<span>ll explain!</span></p>
<p><span>The trick is the simplified </span><a href="https://hypothesis.works/articles/compositional-shrinking/"><span>hypothesis</span>
<span>approach</span></a><span>. The</span>
<a href="https://docs.rs/arbtest/latest/arbtest/"><span>arbtest</span></a><span> PBT library we use in this post is based on a</span>
<span>familiar interface of a PRNG:</span></p>

<figure class="code-block">


<pre><code><span class="line">arbtest::<span class="hl-title function_ invoke__">arbtest</span>(|rng| {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">random_int</span>: <span class="hl-type">usize</span> = rng.<span class="hl-title function_ invoke__">int_in_range</span>(<span class="hl-number">0</span>..=<span class="hl-number">100</span>)?;</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">random_bool</span>: <span class="hl-type">bool</span> = rng.<span class="hl-title function_ invoke__">arbitrary</span>()?;</span>
<span class="line">  <span class="hl-title function_ invoke__">Ok</span>(())</span>
<span class="line">});</span></code></pre>

</figure>
<p><span>But there</span>&rsquo;<span>s a twist! This is a </span><em><span>finite</span></em><span> PRNG. So, if you ask it to flip a coin it can give you</span>
<span>heads. And next time it might give you tails. But if you continue asking it for more, at some point</span>
<span>it</span>&rsquo;<span>ll give you </span><span class="display"><code>Err(OutOfEntropy)</code><span>.</span></span></p>
<p><span>That</span>&rsquo;<span>s why all these </span><code>?</code><span> and the outer loop of</span>
<span class="display"><code>while !rng.is_empty() {</code><span>.</span></span></p>
<p><span>In other words, as soon as the test runs out of entropy, it short-circuits and completes. And that</span>
<span>means that by reducing the amount of entropy available the test becomes shorter, and this works</span>
<span>irrespective of how complex is the logic inside the test!</span></p>
<p><span>And </span>&ldquo;<span>entropy</span>&rdquo;<span> is a big scary word here, what actually happens is that the PRNG is just an </span><code>&amp;mut
&amp;[u8]</code><span> inside. That is, a slice of random bytes, which is shortened every time you ask for a random</span>
<span>number. And the shorter the initial slice, the simpler the test gets. Minimization can be this</span>
<span>simple!</span></p>
<p><span>You can find source code for this article at</span>
<a href="https://github.com/matklad/properly-concurrent" class="display url">https://github.com/matklad/properly-concurrent</a></p>
</section>
]]></content>
</entry>

<entry>
<title type="text">Regular, Recursive, Restricted</title>
<link href="https://matklad.github.io/2024/06/04/regular-recursive-restricted.html" rel="alternate" type="text/html" title="Regular, Recursive, Restricted" />
<published>2024-06-04T00:00:00+00:00</published>
<updated>2024-06-04T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/06/04/regular-recursive-restricted</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[A post/question about formal grammars, wherein I search for a good formalism for describing infix
expressions.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/06/04/regular-recursive-restricted.html"><![CDATA[
<h1><span>Regular, Recursive, Restricted</span> <time class="meta" datetime="2024-06-04">Jun 4, 2024</time></h1>
<p><span>A post/question about formal grammars, wherein I search for a good formalism for describing infix</span>
<span>expressions.</span></p>
<p><span>Problem statement: it</span>&rsquo;<span>s hard to describe arithmetic expressions in a way that:</span></p>
<ul>
<li>
<span>declaratively captures the overall shape of expression, and</span>
</li>
<li>
<span>has a clear precedence semantics</span>
</li>
</ul>
<p><span>Let</span>&rsquo;<span>s start with the following grammar for arithmetic expressions:</span></p>

<figure class="code-block">


<pre><code><span class="line">Expr =</span>
<span class="line">    'number'</span>
<span class="line">  | '(' Expr ')'</span>
<span class="line">  | Expr '+' Expr</span>
<span class="line">  | Expr '*' Expr</span></code></pre>

</figure>
<p><span>It is definitely declarative and obvious. But it is ambiguous </span>&mdash;<span> it doesn</span>&rsquo;<span>t tell whether </span><code>*</code><span> or </span><code>+</code>
<span>binds tighter, and their associativity. You </span><em><span>can</span></em><span> express those properties directly in the grammar:</span></p>

<figure class="code-block">


<pre><code><span class="line">Expr =</span>
<span class="line">    Factor</span>
<span class="line">  | Expr '+' Factor</span>
<span class="line"></span>
<span class="line">Factor =</span>
<span class="line">    Atom</span>
<span class="line">  | Factor '*' Atom</span>
<span class="line"></span>
<span class="line">Atom = 'number' | '(' Expr ')'</span></code></pre>

</figure>
<p><span>But at this point we lose decorativeness. The way my brain parses the above grammar is by pattern</span>
<span>matching it as a grammar for infix expressions and folding it back to the initial compressed form,</span>
<em><span>not</span></em><span> by reading the grammar rules as written.</span></p>
<p><span>To go in another direction, you can define ambiguity away and get parsing expression grammars:</span></p>

<figure class="code-block">


<pre><code><span class="line">Exp =</span>
<span class="line">    Sum</span>
<span class="line">  / Product</span>
<span class="line">  / Atom</span>
<span class="line"></span>
<span class="line">Sum     = Expr (('+' / '-') Expr)+</span>
<span class="line">Product = Expr (('*' / '/') Expr)+</span>
<span class="line"></span>
<span class="line">Atom = 'number' | '(' Expr ')'</span></code></pre>

</figure>
<p><span>This captures precedence </span><em><span>mostly</span></em><span> declaratively: we first match </span><code>Sum</code><span>, and, failing that, match</span>
<code>Product</code><span>. But the clarity of semantics is lost </span>&mdash;<span> PEGs are never ambiguous by virtue of always</span>
<span>picking the first alternative, so it</span>&rsquo;<span>s too easy to introduce an unintended ambiguity.</span></p>
<p><span>Can we have both? Clarity with respect to tree shape and clarity with respect to ambiguity?</span></p>
<p><span>Let me present a formalism that, I think, ticks both boxes for the toy example and pose a question</span>
<span>of whether it generalizes.</span></p>
<hr>
<p><span>Running example:</span></p>

<figure class="code-block">


<pre><code><span class="line">E =</span>
<span class="line">    'number'</span>
<span class="line">  | '(' E ')'</span>
<span class="line">  | E '+' E</span></code></pre>

</figure>
<p><span>As a grammar for strings, it is ambiguous. There are two parse trees for </span><code>1 + 2 + 3</code><span> </span>&mdash;<span> the</span>
&ldquo;<span>correct</span>&rdquo;<span> one </span><code>(1 + 2) + 3</code><span>, and the alternative: </span><code>1 + (2 + 3)</code><span>.</span></p>
<p><span>Instead, lets see it as a grammar for trees instead. Specifically, trees where:</span></p>
<ul>
<li>
<span>Leaves are labeled with </span><code>'number'</code><span>, </span><code>'+'</code><span>, or </span><code>'*'</code><span>.</span>
</li>
<li>
<span>Interior nodes are labeled with </span><code>E</code><span>.</span>
</li>
<li>
<span>For each interior node, the string formed by labels of its </span><em><span>direct</span></em><span> children conforms to the</span>
<span>specified regular expression.</span>
</li>
</ul>
<p><span>For trees, this is a perfectly fine grammar! Given a labeled tree, its trivial to check whether it</span>
<span>matches the grammar: for each node, you can directly match the regular expression. There</span>&rsquo;<span>s also no</span>
<span>meaningful ambiguity </span>&mdash;<span> while arbitrary regular expressions can be ambiguous (</span><code>aa | a*</code><span>), this</span>
<span>doesn</span>&rsquo;<span>t really come up as harmful in practice all that often, and, in any case, it</span>&rsquo;<span>s easy to check</span>
<span>that any two regular alternatives are disjoint (intersect the two automata, minimize the result,</span>
<span>check if it is empty).</span></p>
<p><span>As a grammar for trees, it has the following property: there are two distinct trees which</span>
<span>nevertheless share the same sequence of leaves:</span></p>

<figure class="code-block">


<pre><code><span class="line">        E                  E</span>
<span class="line">        o                  o</span>
<span class="line">      / | \              / | \</span>
<span class="line">     E '+' E            E '+' E</span>
<span class="line">     o     |            |     o</span>
<span class="line">   / | \  '3'          '1'  / | \</span>
<span class="line">  E '+' E                  E '+' E</span>
<span class="line">  |     |                  |     |</span>
<span class="line"> '1'   '2'                '2'   '3'</span></code></pre>

</figure>
<p><span>So let</span>&rsquo;<span>s restrict the set of trees, in the most straightforward manner, by adding some inequalities:</span></p>

<figure class="code-block">


<pre><code><span class="line">E =</span>
<span class="line">    'number'</span>
<span class="line">  | '(' E ')'</span>
<span class="line">  | E '+' E</span>
<span class="line"></span>
<span class="line">E !=</span>
<span class="line">    E '+' [E '+' E]</span></code></pre>

</figure>
<p><span>Here, square brackets denote a child. </span><code>E '+' [E '+' E]</code><span> is a plus node whose right child is also a</span>
<span>plus node. Checking whether a tree conform to this modified set of rules is easy as negative rules</span>
<span>are also just regular expressions. Well, I think you need some fiddling here, as, as written, a</span>
<span>negative rule matches two different levels of the tree, but you can flatten both the rule and the</span>
<span>actual tree to the grandchildren level by enclosing children in parenthesis. Let me show an example:</span></p>
<p><span>We want to match this node:</span></p>

<figure class="code-block">


<pre><code><span class="line">    E</span>
<span class="line">    o</span>
<span class="line">  / | \</span>
<span class="line"> E '+' E</span>
<span class="line"> |     o</span>
<span class="line">'1'  / | \</span>
<span class="line">    E '+' E</span></code></pre>

</figure>
<p><span>against this rule concerning children and grand children:</span></p>

<figure class="code-block">


<pre><code><span class="line">E '+' [E '+' E]</span></code></pre>

</figure>
<p><span>We write the list of children and grandchidren of the node, while adding extra </span><code>[]</code><span>, to get this</span>
<span>string:</span></p>

<figure class="code-block">


<pre><code><span class="line">['1'] '+' [E '+' E]</span></code></pre>

</figure>
<p><span>And in the rule we replace top-level non-terminals with </span><code>[.*]</code><span>, to get this regular expression:</span></p>

<figure class="code-block">


<pre><code><span class="line">[.*] '+' [E '+' E]</span></code></pre>

</figure>
<p><span>Now we can match the string against a regex, get a mach, and rule out the tree (remember, this is</span>
<code>!=</code><span>).</span></p>
<p><span>So here it is, a perfectly functional mathematical animal: recursive restricted regular expression:</span></p>
<ul>
<li>
<span>A set of non-terminals </span><code>N</code><span> (denoted with </span><code>TitleCase</code><span> names)</span>
</li>
<li>
<span>A set of terminals </span><code>T</code><span> (denoted with </span><code>'quoted'</code><span> names)</span>
</li>
<li>
<span>A generating mapping from non-terminals </span><code>N</code><span> to regular expressions over </span><code>N ∪ T</code><span> alphabet</span>
</li>
<li>
<span>A restricting mapping from non-terminals </span><code>N</code><span> to regular expressions over </span><code>N ∪ T ∪ {], [}</code><span> (that is</span>
<span>regular expressions with square brackets to denote children)</span>
</li>
</ul>
<p><span>This construction denotes a set of labeled trees, where interior nodes are labeled with </span><code>N</code><span>, leaves</span>
<span>are labeled with </span><code>T</code><span> and for each interior node</span></p>
<ul>
<li>
<span>its children match the corresponding generating regular expression</span>
</li>
<li>
<span>its grandchildren do not match the corresponding restricting regular expression</span>
</li>
</ul>
<p><span>And the main question one would have, if confronted with a specimen, is </span>&ldquo;<span>is it ambiguous?</span>&rdquo;<span> That is,</span>
<span>are there two trees in the set which have the same sequence of leaves?</span></p>
<p><span>Let</span>&rsquo;<span>s look at an example:</span></p>

<figure class="code-block">


<pre><code><span class="line">Expr =</span>
<span class="line">    'number'</span>
<span class="line">  | '(' Expr ')'</span>
<span class="line">  | Expr '+' Expr</span>
<span class="line">  | Expr '*' Expr</span>
<span class="line"></span>
<span class="line">Expr !=</span>
<span class="line">             Expr '+' [Expr '+' Expr]</span>
<span class="line">|            Expr '*' [Expr '*' Expr]</span>
<span class="line">|            Expr '*' [Expr '+' Expr]</span>
<span class="line">| [Expr '+' Expr] '*' Expr</span></code></pre>

</figure>
<p><span>It looks unambiguous to me! And I am pretty sure that I can prove, by hand, that it is in fact</span>
<span>unambiguous (well, I might discover that I miss a couple of restrictions in process, but it feels</span>
<span>like it should work in principle). The question is, can a computer take an arbitrary recursive</span>
<span>restricted regular expression and tell me that its unambiguous, or, failing that, provide a</span>
<span>counter-example?</span></p>
<p><span>In the general case, the answer is no </span>&mdash;<span> this is at least as expressive as CFG, and ambiguity of</span>
<span>arbitrary CFG is undecidable. But perhaps there</span>&rsquo;<span>s some reasonable set of restrictions under which it</span>
<span>is in fact possible to prove the absence of ambiguity?</span></p>
]]></content>
</entry>

<entry>
<title type="text">Basic Things</title>
<link href="https://matklad.github.io/2024/03/22/basic-things.html" rel="alternate" type="text/html" title="Basic Things" />
<published>2024-03-22T00:00:00+00:00</published>
<updated>2024-03-22T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/03/22/basic-things</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[After working on the initial stages of several largish projects, I accumulated a list of things that
share the following three properties:]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/03/22/basic-things.html"><![CDATA[
<h1><span>Basic Things</span> <time class="meta" datetime="2024-03-22">Mar 22, 2024</time></h1>
<p><span>After working on the initial stages of several largish projects, I accumulated a list of things that</span>
<span>share the following three properties:</span></p>
<ul>
<li>
<span>they are irrelevant while the project is small,</span>
</li>
<li>
<span>they are a productivity multiplier when the project is large,</span>
</li>
<li>
<span>they are much harder to introduce down the line.</span>
</li>
</ul>
<p><span>Here</span>&rsquo;<span>s the list:</span></p>
<section id="READMEs">

    <h2>
    <a href="#READMEs"><span>READMEs</span> </a>
    </h2>
<p><span>A project should have a </span><em><span>short</span></em><span> one-page readme that is mostly links to more topical documentation.</span>
<span>The two most important links are the user docs and the dev docs.</span></p>
<p><span>A common failure is a readme growing haphazardly by accretion, such that it is neither a good</span>
<span>landing page, nor a source of comprehensive docs on any particular topic. It is hard to refactor</span>
<span>such an unstructured readme later. The information is valuable, if disorganized, but there</span>
<span>isn</span>&rsquo;<span>t any better place to move it to.</span></p>
</section>
<section id="Developer-Docs">

    <h2>
    <a href="#Developer-Docs"><span>Developer Docs</span> </a>
    </h2>
<p><span>For developers, you generally want to have a docs folder in the repository. The docs folder should</span>
<em><span>also</span></em><span> contain a short landing page describing the structure of the documentation. This structure</span>
<span>should allow for both a small number of high quality curated documents, and a large number of ad-hoc</span>
<span>append-only notes on any particular topic. For example, </span><code>docs/README.md</code><span> could point to carefully</span>
<span>crafted</span>
<a href="https://matklad.github.io/2021/02/06/ARCHITECTURE.md.html"><code>ARCHITECTURE.md</code></a>
<span>and </span><code>CONTRIBUTING.md</code><span>, which describe high level code and social</span>
<span>architectures, and explicitly say that everything else in the </span><code>docs/</code><span> folder is a set of unorganized</span>
<span>topical guides.</span></p>
<p><span>Common failure modes here:</span></p>
<ol type="a">
<li>
<p><span>There</span>&rsquo;<span>s no place where to put new developer documentation at all. As a result, no docs are</span>
<span>getting written, and, by the time you do need docs, the knowledge is lost.</span></p>
</li>
<li>
<p><span>There</span>&rsquo;<span>s only highly  structured, carefully reviewed developer documentation. Contributing docs</span>
<span>requires a lot of efforts, and many small things go undocumented.</span></p>
</li>
<li>
<p><span>There</span>&rsquo;<span>s only unstructured append-only pile of isolated documents. Things are </span><em><span>mostly</span></em><span> documented,</span>
<span>often two or three times, but any new team member has to do the wheat from the chaff thing anew.</span></p>
</li>
</ol>
</section>
<section id="Users-Website">

    <h2>
    <a href="#Users-Website"><span>Users Website</span> </a>
    </h2>
<p><span>Most project can benefit from a dedicated website targeted at the users. You want to have website</span>
<span>ready when there are few-to-no users: usage compounds over time, so, if you find yourself with a</span>
<span>significant number of users and no web </span>&ldquo;<span>face</span>&rdquo;<span>, you</span>&rsquo;<span>ve lost quite a bit of value already!</span></p>
<p><span>Some other failure modes here:</span></p>
<ol type="a">
<li>
<p><span>A different team manages the website. This prevents project developers from directly contributing</span>
<span>improvements, and may lead to divergence between the docs and the shipped product.</span></p>
</li>
<li>
<p><span>Today</span>&rsquo;<span>s web stacks gravitate towards infinite complexity. It</span>&rsquo;<span>s all too natural to pick an </span>&ldquo;<span>easy</span>&rdquo;
<span>heavy framework at the start, and then get yourself into npm</span>&rsquo;<span>s bog. Website is about content, and</span>
<span>content has gravity. Whatever markup language dialect you choose at the beginning is going to</span>
<span>stay with for some time. Do carefully consider the choice of your web stack.</span></p>
</li>
<li>
<p><span>Saying that which isn</span>&rsquo;<span>t quite done yet. Don</span>&rsquo;<span>t overpromise, it</span>&rsquo;<span>s much easier to say more later</span>
<span>than to take back your words, and humbleness might be a good marketing. Consider if you are in a</span>
<span>domain where engineering credibility travel faster than buzz words. But this is situational. More</span>
<span>general advice would be that marketing also compounds over time, so it pays off to be deliberate</span>
<span>about your image from the start.</span></p>
</li>
</ol>
</section>
<section id="Internal-Website">

    <h2>
    <a href="#Internal-Website"><span>Internal Website</span> </a>
    </h2>
<p><span>This is more situational, but consider if, in addition to public-facing website, you also need an</span>
<span>internal, engineering-facing one. At some point you</span>&rsquo;<span>ll probably need a bit more interactivity than</span>
<span>what</span>&rsquo;<span>s available in a </span><code>README.md</code><span> </span>&mdash;<span> perhaps you need a place to display code-related metrics like</span>
<span>coverage or some javascript to compute release rotation. Having a place on the web where a</span>
<span>contributor can place something they need right now without much red tape is nice!</span></p>
<p><span>This is a recurring theme </span>&mdash;<span> you should be organized, you should not be organized. </span><em><span>Some</span></em><span> things</span>
<span>have large fan-out and should be guarded with careful review. </span><em><span>Other</span></em><span> things benefit from just being</span>
<span>there and a lightweight process. You need to create places for both kinds of things, and a clear</span>
<span>decision rule about what goes where.</span></p>
<p><span>For internal website, you</span>&rsquo;<span>ll probably need some kind of data store as well. If you want to track</span>
<span>binary size across commits, </span><em><span>something</span></em><span> needs to map commit hashes to (lets be optimistic)</span>
<span>kilobytes! I don</span>&rsquo;<span>t know a good solution here. I use a JSON file in a github repository for similar</span>
<span>purposes.</span></p>
</section>
<section id="Process-Docs">

    <h2>
    <a href="#Process-Docs"><span>Process Docs</span> </a>
    </h2>
<p><span>There are many possible ways to get some code into the main branch. Pick one, and spell it out in</span>
<span>an </span><code>.md</code><span> file explicitly:</span></p>
<ul>
<li>
<p><span>Are feature branches pushed to the central repository, or is anyone works off their fork? I find</span>
<span>forks work better in general as they automatically namespace everyone</span>&rsquo;<span>s branches, and put team</span>
<span>members and external contributors on equal footing.</span></p>
</li>
<li>
<p><span>If the repository is shared, what is the naming convention for branches? I prefix mine with</span>
<code>matklad/</code><span>.</span></p>
</li>
<li>
<p><span>You use </span><a href="https://graydon2.dreamwidth.org/1597.html"><span>not rocket-science rule</span></a><span> (more on this later :).</span></p>
</li>
<li>
<p><span>Who should do code review of a particular PR? A single person, to avoid bystander effect and to</span>
<span>reduce notification fatigue. The reviewer is picked by the author of PR, as that</span>&rsquo;<span>s a stable</span>
<span>equilibrium in a high-trust team and cuts red tape.</span></p>
</li>
<li>
<p><span>How the reviewer knows that they need to review code? On GitHub, you want to </span><em><span>assign</span></em><span> rather than</span>
<em><span>request</span></em><span> a review. Assign is level-triggered </span>&mdash;<span> it won</span>&rsquo;<span>t go away until the PR is merged, and it</span>
<span>becomes the responsibility of the reviewer to help the PR along until it is merged (</span><em><span>request</span>
<span>review</span></em><span> is still useful to poke the assignee after a round of feedback&amp;changes). More generally,</span>
<span>code review is the highest priority task </span>&mdash;<span> there</span>&rsquo;<span>s no reason to work on new code</span>
<span>if there</span>&rsquo;<span>s already some finished code which is just blocked on your review.</span></p>
</li>
<li>
<p><span>What is the purpose of review? Reviewing for correctness, for single voice, for idioms, for</span>
<span>knowledge sharing, for high-level architecture are choices! Explicitly spell out what makes most</span>
<span>sense in the context of your project.</span></p>
</li>
<li>
<p><span>Meta process docs: positively encourage contributing process documentation itself.</span></p>
</li>
</ul>
</section>
<section id="Style">

    <h2>
    <a href="#Style"><span>Style</span> </a>
    </h2>
<p><span>Speaking about meta process, style guide is where it is most practically valuable. Make sure that</span>
<span>most stylistic comments during code reviews are immediately codified in the project-specific style</span>
<span>document. New contributors should learn project</span>&rsquo;<span>s voice not through a hundred repetitive comments on</span>
<span>PRs, but through a dozen links to specific items of the style guide.</span></p>
<p><span>Do you even need a project-specific style guide? I think you do </span>&mdash;<span> cutting down mental energy for</span>
<span>trivial decisions is helpful. If you need a result variable, and half of the functions call it </span><code>res</code>
<span>and another half of the functions call it </span><code>result</code><span>, making this choice is just distracting.</span></p>
<p><span>Project-specific naming conventions is one of the more useful thing to place in the style guide.</span></p>
<p><span>Optimize style guide for extensibility. Uplifting a comment from a code review to the style guide</span>
<span>should not require much work.</span></p>
<p><span>Ensure that there</span>&rsquo;<span>s a style tzar </span>&mdash;<span> building consensus around </span><em><span>specific</span></em><span> style choices is very</span>
<span>hard, better to delegate the entire responsibility to one person who can make good enough choices.</span>
<span>Style usually is not about what</span>&rsquo;<span>s better, it</span>&rsquo;<span>s about removing needless options in a semi-arbitrary</span>
<span>ways.</span></p>
</section>
<section id="Git">

    <h2>
    <a href="#Git"><span>Git</span> </a>
    </h2>
<p><span>Document stylistic details pertaining to git. If project uses </span><code>area:</code><span> prefixes for commits, spell</span>
<span>out an explicit list of such prefixes.</span></p>
<p><span>Consider documenting acceptable line length for the summary line. Git man page boldly declares that</span>
<span>a summary should be under 50 characters, but that is just plain false. Even in the kernel, most</span>
<span>summaries are somewhere between 50 and 80 characters.</span></p>
<p><span>Definitely explicitly forbid adding large files to git. Repository size increases monotonically,</span>
<code>git clone</code><span> time is important.</span></p>
<p><span>Document merge-vs-rebase thing. My preferred answer is:</span></p>
<ul>
<li>
<span>A unit of change is a pull request, which might contain several commits</span>
</li>
<li>
<span>Merge commit for the pull request is what is being tested</span>
</li>
<li>
<span>The main branch contains only merge commits</span>
</li>
<li>
<span>Conversely, </span><em><span>only</span></em><span> the main branch contains merge commits, pull requests themselves are always</span>
<span>rebased.</span>
</li>
</ul>
<p><span>Forbidding large files in the repo is a good policy, but it</span>&rsquo;<span>s hard to follow. Over the lifetime of</span>
<span>the project, someone somewhere will sneakily add and revert a megabyte of generated protobufs, and</span>
<span>that will fly under code review radar.</span></p>
<p><span>This brings us to the most basic thing of them all:</span></p>
</section>
<section id="Not-Rocket-Science-Rule">

    <h2>
    <a href="#Not-Rocket-Science-Rule"><span>Not Rocket Science Rule</span> </a>
    </h2>
<p><span>Maintain a well-defined set of automated checks that pass on the main branch at all times. If you</span>
<span>don</span>&rsquo;<span>t want large blobs in git repository, write a test rejecting large git objects and run that</span>
<span>right before updating the main branch. No merge commits on feature branches? Write a test which</span>
<span>fails with a pageful of Git self-help if one is detected. Want to wrap </span><code>.md</code><span> at 80 columns? Write a</span>
<span>test :)</span></p>
<p><span>It is perhaps worth you while to re-read the original post:</span>
<a href="https://graydon2.dreamwidth.org/1597.html" class="display url">https://graydon2.dreamwidth.org/1597.html</a></p>
<p><a href="https://matklad.github.io/2024/01/03/of-rats-and-ratchets.html"><span>This mindset of monotonically growing set of properties</span></a>
<span>that are true about the codebase is </span><em><span>incredibly</span></em><span> powerful. You start seeing code as temporary, fluid</span>
<span>thing that can always be changed relatively cheaply, and the accumulated set of automated tests as</span>
<span>the real value of the project.</span></p>
<p><span>Another second order effect is that NRSR puts a pressure to optimize your build and test</span>
<span>infrastructure. If you don</span>&rsquo;<span>t have an option to merge the code when an unrelated flaky test fails,</span>
<span>you won</span>&rsquo;<span>t have flaky tests.</span></p>
<p><span>A common anti-pattern here is that a project grows a set of semi-checks </span>&mdash;<span> tests that exists, but</span>
<span>are not 100% reliable, and thus are not exercised by the CI routinely. And that creates ambiguity</span>
&mdash;<span> are tests failing due to a regression which should be fixed, or were they never reliable, and</span>
<span>just test a property that isn</span>&rsquo;<span>t actually essential for functioning of the project? This fuzziness</span>
<span>compounds over time. If a check isn</span>&rsquo;<span>t reliable enough to be part of NRSR CI gate, it isn</span>&rsquo;<span>t actually</span>
<span>a check you care about, and should be removed.</span></p>
<p><span>But to do NRSR, you need to build &amp; CI your code first:</span></p>
</section>
<section id="Build-CI">

    <h2>
    <a href="#Build-CI"><span>Build &amp; CI</span> </a>
    </h2>
<p><span>This is a complex topic. Let</span>&rsquo;<span>s start with the basics: what is a build system? I would love to</span>
<span>highlight a couple of slightly unconventional answers here.</span></p>
<p><em><span>First</span></em><span>, a build system is a bootstrap process: it is how you get from </span><code>git clone</code><span> to a working</span>
<span>binary. The two aspects of this boostrapping process are important:</span></p>
<ul>
<li>
<span>It should be simple. No</span>
<span class="display"><code>sudo apt-get install bazzilion packages</code><span>,</span></span>
<span>the single binary of your build system should be able to bring everything else that</span>&rsquo;<span>s needed,</span>
<span>automatically.</span>
</li>
<li>
<span>It should be repeatable. Your laptop and your CI should end up with exactly identical set of</span>
<span>dependencies. The end result should be a function of commit hash, and not your local shell</span>
<span>history, otherwise NRSR doesn</span>&rsquo;<span>t work.</span>
</li>
</ul>
<p><em><span>Second</span></em><span>, a build system is developer UI. To do almost anything, you need to type some sort of build</span>
<span>system invocation into your shell. There should be a single, clearly documented command for building</span>
<span>and testing the project. If it is not a single </span><code>makebelieve test</code><span>, something</span>&rsquo;<span>s wrong.</span></p>
<p><span>One anti-pattern here is when the build system spills over to CI. When, to figure out what the set</span>
<span>of checks even is, you need to read </span><code>.github/workflows/*.yml</code><span> to compile a list of commands. That</span>&rsquo;<span>s</span>
<span>accidental complexity! Sprawling yamls are a bad entry point. Put all the logic into the build</span>
<span>system and let the CI drive that, and not vice verse.</span></p>
<p><a href="https://matklad.github.io/2023/12/31/O(1)-build-file.html"><span>There is a stronger version of the</span>
<span>advice</span></a><span>. No matter the size of the</span>
<span>project, there</span>&rsquo;<span>s probably only a handful of workflows that make sense for it: testing, running,</span>
<span>releasing, etc. This small set of workflows should be nailed from the start, and specific commands</span>
<span>should be documented. When the project subsequently grows in volumes, this set of build-system entry</span>
<span>points should </span><em><span>not</span></em><span> grow.</span></p>
<p><span>If you add a Frobnicator, </span><code>makebelieve test</code><span> invocation </span><em><span>should</span></em><span> test that Frobnicator works. If</span>
<span>instead you need a dedicated </span><code>makebelieve test-frobnicator</code><span> and the corresponding line in some CI</span>
<span>yaml, you are on a perilous path.</span></p>
<p><em><span>Finally</span></em><span>, a build system is a collection of commands to make stuff happen. In larger projects,</span>
<span>you</span>&rsquo;<span>ll inevitably need some non-trivial amount of glue automation. Even if the entry point is just</span>
<code>makebelive release</code><span>, internally that might require any number of different tools to build, sign,</span>
<span>tag, upload, validate, and generate a changelog for a new release.</span></p>
<p><span>A common anti-pattern is to write these sorts of automations in bash and Python, but that</span>&rsquo;<span>s almost</span>
<span>pure technical debt. These ecosystems are extremely finnicky in and of themselves, and, crucially</span>
<span>(unless your project itself is written in bash or Python), they are a second ecosystem to what you</span>
<span>already have in your project for </span>&ldquo;<span>normal</span>&rdquo;<span> code.</span></p>
<p><span>But releasing software is also just code, which you can write in your primarly language.</span>
<a href="https://twitter.com/id_aa_carmack/status/989951283900514304"><span>The right tool for the job is often the tool you are already using</span></a><span>.</span>
<span>It pays off to explicitly attack the problem of glue from the start, and to pick/write a library</span>
<span>that makes writing subprocess wrangling logic easy.</span></p>
<p><span>Summing the build and CI story up:</span></p>
<p><span>Build system is self-contained, reproducible and takes on the task of downloading all external</span>
<span>dependencies. Irrespective of size of the project, it contains O(1) different entry points. One of</span>
<span>those entry points is triggered by the not rocket science rule CI infra to run the set of canonical</span>
<span>checks. There</span>&rsquo;<span>s an explicit support for free-form automation, which is implemented in the same</span>
<span>language as the bulk of the project.</span></p>
<p><span>Integration with NRSR is the most important aspect of the build process, as it determines how the</span>
<span>project evolves over time. Let</span>&rsquo;<span>s zoom in.</span></p>
</section>
<section id="Testing">

    <h2>
    <a href="#Testing"><span>Testing</span> </a>
    </h2>
<p><span>Testing is a primary architectural concern. When the first line of code is written, you already</span>
<span>should understand the big picture testing story. It is empathically </span><em><span>not</span></em><span> </span>&ldquo;<span>every class and module</span>
<span>has unit-test</span>&rdquo;<span>. Testing should be data oriented </span>&mdash;<span> the job of a particular software is to take some</span>
<span>data in, transform it, and spit different data out. Overall testing strategy requires:</span></p>
<ul>
<li>
<span>some way to specify/generate input data,</span>
</li>
<li>
<span>some way to assert desired properties of output data, and</span>
</li>
<li>
<span>a way to run many individual checks very fast.</span>
</li>
</ul>
<p><span>If time is a meaningful part of the input data, it should be modeled explicitly. Not getting the</span>
<span>testing architecture right usually results in:</span></p>
<ul>
<li>
<span>Software that is hard to change because thousands of test nail existing internal APIs.</span>
</li>
<li>
<span>Software that is hard to change because there are no test to confidently verify absence of</span>
<span>unintended breakages.</span>
</li>
<li>
<span>Software that is hard to change because each change requires hours of testing time to verify.</span>
</li>
</ul>
<p><span>How to architect a test suite goes beyond the scope of this article, but please read</span>
<a href="https://matklad.github.io/2022/07/04/unit-and-integration-tests.html"><span>Unit and Integration Tests</span></a>
<span>and</span>
<a href="https://matklad.github.io/2021/05/31/how-to-test.html"><span>How To Test</span></a><span>.</span></p>
<p><span>Some specific things that are in scope for this article:</span></p>
<p><span>Zero tolerance for flaky tests. Strict not rocket science rules gives this by construction </span>&mdash;<span> if</span>
<span>you can</span>&rsquo;<span>t merge </span><em><span>your</span></em><span> pull request because someone elses test is flaky, that flaky test immediately</span>
<span>becomes your problem.</span></p>
<p><span>Fast tests. Again, NRSR already provides a natural pressure for this, but it also helps to make</span>
<span>testing time more salient otherwise. Just by default printing the total test time and five slowest</span>
<span>tests in a run goes a long way.</span></p>
<p><span>Not all tests could be fast. Continuing the ying-yang theme of embracing order and chaos</span>
<span>simultaneously, it helps to introduce the concept of slow tests early on. CI always runs the full</span>
<span>suite of tests, fast and slow. But the local </span><code>makebelive test</code><span> by default runs only fast test, with</span>
<span>an opt-in for slow tests. Opt in can be as simple as an </span><code>SLOW_TESTS=1</code><span> environmental variable.</span></p>
<p><span>Introduce a </span><a href="https://ianthehenry.com/posts/my-kind-of-repl/"><span>snapshot testing</span></a><span> library early.</span>
<span>Although the bulk of tests should probably use project-specific testing harness, for everything else</span>
<span>inline repl-driven snapshot testing is a good default approach, and is something costly to introduce</span>
<span>once you</span>&rsquo;<span>ve accumulated a body of non-snapshot-based tests.</span></p>
<p><span>Alongside the tests, come the benchmarks.</span></p>
</section>
<section id="Benchmarking">

    <h2>
    <a href="#Benchmarking"><span>Benchmarking</span> </a>
    </h2>
<p><span>I don</span>&rsquo;<span>t have a grand vison about how to make benchmark work in a large, living project, it always</span>
<span>feels like a struggle to me. I do have a couple of tactical tips though.</span></p>
<p><em><span>Firstly</span></em><span>, any code that is </span><em><span>not</span></em><span> running during NRSR is effectively dead. It is exceedingly common</span>
<span>for benchmarks to be added alongside a performance improvement, and then </span><em><span>not</span></em><span> getting hooked up</span>
<span>with CI. So, two month down the line, the benchmark either stops compiling outright, or maybe just</span>
<span>panics at a startup due to some unrelated change.</span></p>
<p><span>This fix here is to make sure that every benchmark is </span><em><span>also</span></em><span> a test. Parametrize every benchmark by</span>
<span>input size, such that with a small input it finishes in milliseconds. Then write a test that</span>
<span>literally just calls the benchmarking code with this small input. And remember that your build</span>
<span>system should have O(1) entry points. Plug this into a </span><span class="display"><code>makebelieve test</code><span>,</span></span><span> not into a</span>
<span>dedicated </span><span class="display"><code>makebelieve benchmark --small-size</code><span>.</span></span></p>
<p><em><span>Secondly</span></em><span>, any large project has a certain amount of very important macro metrics.</span></p>
<ul>
<li>
<span>How long does it take to build?</span>
</li>
<li>
<span>How long does it take to test?</span>
</li>
<li>
<span>How large is the resulting artifact shipping to users?</span>
</li>
</ul>
<p><span>These are some of the questions that always matter. You need infrastructure to track these numbers,</span>
<span>and to see them regularly. This where the internal website and its data store come in. During CI,</span>
<span>note those number. After CI run, upload a record with commit hash, metric name, metric value</span>
<em><span>somewhere</span></em><span>. Don</span>&rsquo;<span>t worry if the results are noisy </span>&mdash;<span> you target the baseline here, ability to</span>
<span>notice large changes over time.</span></p>
<p><span>Two options for the </span>&ldquo;<span>upload</span>&rdquo;<span> part:</span></p>
<ul>
<li>
<p><span>Just put them into some </span><code>.json</code><span> file in a git repo, and LLM a bit of javascript to display a nice</span>
<span>graph from these data.</span></p>
</li>
<li>
<p><a href="https://nyrkio.com" class="url">https://nyrkio.com</a><span> is a surprisingly good SaaS offering that I can recommend.</span></p>
</li>
</ul>
</section>
<section id="Fuzz-Testing">

    <h2>
    <a href="#Fuzz-Testing"><span>Fuzz Testing</span> </a>
    </h2>
<p><span>Serious fuzz testing curiously shares characteristics of tests and benchmarks. Like a normal test, a</span>
<span>fuzz test informs you about a correctness issue in your application, and is reproducible. Like a</span>
<span>benchmark, it is (infinitely) long running and infeasible to do as a part of NRSR.</span></p>
<p><span>I don</span>&rsquo;<span>t yet have a good hang on how to most effectively integrate continuous fuzzing into</span>
<span>development process. I don</span>&rsquo;<span>t know what is the not rocket science rule of fuzzing. But two things</span>
<span>help:</span></p>
<p><em><span>First</span></em><span>, even if you can</span>&rsquo;<span>t run fuzzing loop during CI, you can run isolated seeds. To help ensure</span>
<span>that the fuzing code doesn</span>&rsquo;<span>t get broken, do the same thing as with benchmark </span>&mdash;<span> add a test that</span>
<span>runs fuzzing logic with a fixed seed and small, fast parameters. One variation here is that you can</span>
<span>use commit sha as random a seed </span>&mdash;<span> that way the code is still reproducible, but there is enough</span>
<span>variation to avoid dynamically dead code.</span></p>
<p><em><span>Second</span></em><span>, it is helpful to think about fuzzing in terms of level triggering. With tests, when you</span>
<span>make an erroneous commit, you immediately know that it breaks stuff. With fuzzing, you generally</span>
<span>discover this later, and a broken seed generally persists for several commits. So, as an output of</span>
<span>the fuzzer, I think what you want is </span><em><span>not</span></em><span> a set of GitHub issues, but rather a dashboard of sorts</span>
<span>which shows a table of recent commits and failing seeds for those commits.</span></p>
<p><span>With not rocket science rule firmly in place, it makes sense to think about releases.</span></p>
</section>
<section id="Releases">

    <h2>
    <a href="#Releases"><span>Releases</span> </a>
    </h2>
<p><span>Two core insights here:</span></p>
<p><em><span>First</span></em><span> release </span><em><span>process</span></em><span> is orthogonal from software being </span><em><span>production ready</span></em><span>. You can release</span>
<span>stuff before it is ready (provided that you add a short disclaimer to the readme). So, it pays off</span>
<span>to add proper release process early on, such that, when the time comes to actually release</span>
<span>software, it comes down to removing disclaimers and writing the announcement post, as all technical</span>
<span>work has been done ages ago.</span></p>
<p><em><span>Second</span></em><span>, software engineering in general observes reverse triangle inequality: to get from A to C,</span>
<span>it is faster to go from A to B and then from B to C, then moving from A to C atomically. If you make</span>
<span>a pull request, it helps to split it up into smaller parts. If you refactor something, it is faster</span>
<span>to first introduce a new working copy and then separately retire the old code, rather than changing</span>
<span>the thing in place.</span></p>
<p><span>Releases are no different: faster, more frequent releases are easier and less risky. Weekly cadence</span>
<span>works great, provided that you have a solid set of checks in your NRSR.</span></p>
<p><span>It is much easier to start with a state where almost nothing works, but there</span>&rsquo;<span>s a solid release</span>
<span>(with an empty set of features), and ramp up from there, than to hack with reckless abandon</span>
<em><span>without</span></em><span> thinking much about eventual release, and then scramble to decide which is ready and</span>
<span>releasable, a what should be cut.</span></p>
</section>
<section id="Summary">

    <h2>
    <a href="#Summary"><span>Summary</span> </a>
    </h2>
<p><span>I think that</span>&rsquo;<span>s it for today? That</span>&rsquo;<span>s a lot of small points! Here</span>&rsquo;<span>s a bullet list for convenient</span>
<span>reference:</span></p>
<ul>
<li>
<span>README as a landing page.</span>
</li>
<li>
<span>Dev docs.</span>
</li>
<li>
<span>User docs.</span>
</li>
<li>
<span>Structured dev docs (architecture and processes).</span>
</li>
<li>
<span>Unstructured ingest-optimized dev docs (code style, topical guides).</span>
</li>
<li>
<span>User website, beware of content gravity.</span>
</li>
<li>
<span>Ingest-optimized internal web site.</span>
</li>
<li>
<span>Meta documentation process </span>&mdash;<span> its everyone job to append to code style and process docs.</span>
</li>
<li>
<span>Clear code review protocol (in whose court is the ball currently?).</span>
</li>
<li>
<span>Automated check for no large blobs in a git repo.</span>
</li>
<li>
<span>Not rocket science rule.</span>
</li>
<li>
<span>Let</span>&rsquo;<span>s repeat: at </span><strong><span>all</span></strong><span> times, the main branch points at a commit hash which is known to pass a</span>
<span>set of well-defined checks.</span>
</li>
<li>
<span>No semi tests: if the code is not good enough to add to NRSR, it is deleted.</span>
</li>
<li>
<span>No flaky tests (mostly by construction from NRSR).</span>
</li>
<li>
<span>Single command build.</span>
</li>
<li>
<span>Reproducible build.</span>
</li>
<li>
<span>Fixed number of build system entry points. No separate lint step, a lint is a kind of a test.</span>
</li>
<li>
<span>CI delegates to the build system.</span>
</li>
<li>
<span>Space for ad-hoc automation in the main language.</span>
</li>
<li>
<span>Overarching testing infrastructure, grand unified theory of project</span>&rsquo;<span>s testing.</span>
</li>
<li>
<span>Fast/Slow test split (fast=seconds per test suite, slow=low digit minutes per test suite).</span>
</li>
<li>
<span>Snapshot testing.</span>
</li>
<li>
<span>Benchmarks are tests.</span>
</li>
<li>
<span>Macro metrics tracking (time to build, time to test).</span>
</li>
<li>
<span>Fuzz tests are tests.</span>
</li>
<li>
<span>Level-triggered display of continuous fuzzing results.</span>
</li>
<li>
<span>Inverse triangle inequality.</span>
</li>
<li>
<span>Weekly releases.</span>
</li>
</ul>
</section>
]]></content>
</entry>

</feed>
